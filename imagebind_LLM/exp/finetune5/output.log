/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
Start
GPU:: 0
| distributed init (rank 0): env://, gpu 0
[1692798462.678689] [mozgsxctr1692792176642-sllj8:11058:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device
[21:47:42.719615] job dir: /home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM
[21:47:42.719740] Namespace(batch_size=8,
epochs=5,
accum_iter=4,
llama_type='7B',
llama_path='/home/u8915687/lab/big-superb/Macaw-LLM2/weights/llama/',
pretrained_path='/home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM/ckpts/7B.pth',
max_words=512,
weight_decay=0.02,
lr=None,
blr=0.0005,
min_lr=0.0,
warmup_epochs=1,
data_config='/home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exps/config.yaml',
num_workers=10,
pin_mem=True,
output_dir='exp/finetune5',
log_dir='./output',
device='cuda',
seed=0,
start_epoch=0,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=0,
gpu=0,
distributed=True,
dist_backend='nccl')
[21:48:11.716675] model args: ModelArgs(dim=4096, n_layers=32, n_heads=32, vocab_size=-1, multiple_of=256, norm_eps=1e-06, max_batch_size=1, max_seq_len=512, w_bias=True, w_lora=True, lora_rank=16)
[21:48:32.535026] Model = LLaMA_adapter(
  (image_bind): ImageBindModel(
    (modality_preprocessors): ModuleDict(
      (vision): RGBDTPreprocessor(
        (cls_token): tensor((1, 1, 1280), requires_grad=False)
        
        (rgbt_stem): PatchEmbedGeneric(
          (proj): Sequential(
            (0): PadIm2Video()
            (1): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
          )
        )
        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(
          (pos_embed): tensor((1, 257, 1280), requires_grad=False)
          
        )
      )
      (text): TextPreprocessor(
        (pos_embed): tensor((1, 77, 1024), requires_grad=False)
        (mask): tensor((77, 77), requires_grad=False)
        
        (token_embedding): Embedding(49408, 1024)
      )
      (audio): AudioPreprocessor(
        (cls_token): tensor((1, 1, 768), requires_grad=False)
        
        (rgbt_stem): PatchEmbedGeneric(
          (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10), bias=False)
          (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(
          (pos_embed): tensor((1, 229, 768), requires_grad=False)
          
        )
      )
      (depth): RGBDTPreprocessor(
        (cls_token): tensor((1, 1, 384), requires_grad=False)
        
        (depth_stem): PatchEmbedGeneric(
          (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16), bias=False)
          (norm_layer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(
          (pos_embed): tensor((1, 197, 384), requires_grad=False)
          
        )
      )
      (thermal): ThermalPreprocessor(
        (cls_token): tensor((1, 1, 768), requires_grad=False)
        
        (rgbt_stem): PatchEmbedGeneric(
          (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
          (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(
          (pos_embed): tensor((1, 197, 768), requires_grad=False)
          
        )
      )
      (imu): IMUPreprocessor(
        (pos_embed): tensor((1, 251, 512), requires_grad=False)
        (cls_token): tensor((1, 1, 512), requires_grad=False)
        
        (imu_stem): PatchEmbedGeneric(
          (proj): Linear(in_features=48, out_features=512, bias=False)
          (norm_layer): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (modality_trunks): ModuleDict(
      (vision): SimpleTransformer(
        (pre_transformer_layer): Sequential(
          (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (1): EinOpsRearrange()
        )
        (blocks): Sequential(
          (0): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (1): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (2): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (3): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (4): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (5): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (6): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (7): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (8): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (9): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (10): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (11): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (12): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (13): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (14): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (15): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (16): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (17): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (18): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (19): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (20): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (21): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (22): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (23): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (24): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (25): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (26): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (27): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (28): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (29): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (30): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
          (31): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          )
        )
        (post_transformer_layer): EinOpsRearrange()
      )
      (text): SimpleTransformer(
        (pre_transformer_layer): Sequential(
          (0): Identity()
          (1): EinOpsRearrange()
        )
        (blocks): Sequential(
          (0): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (1): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (2): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (3): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (4): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (5): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (6): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (7): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (8): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (9): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (10): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (11): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (12): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (13): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (14): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (15): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (16): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (17): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (18): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (19): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (20): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (21): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (22): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
          (23): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          )
        )
        (post_transformer_layer): EinOpsRearrange()
      )
      (audio): SimpleTransformer(
        (pre_transformer_layer): Sequential(
          (0): Identity()
          (1): EinOpsRearrange()
        )
        (blocks): Sequential(
          (0): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (1): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (2): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.018)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (3): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.027)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (4): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.036)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (5): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.045)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (6): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.055)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (7): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.064)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (8): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.073)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (9): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.082)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (10): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (11): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
        )
        (post_transformer_layer): EinOpsRearrange()
      )
      (depth): SimpleTransformer(
        (pre_transformer_layer): Sequential(
          (0): Identity()
          (1): EinOpsRearrange()
        )
        (blocks): Sequential(
          (0): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (1): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (2): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (3): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (4): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (5): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (6): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (7): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (8): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (9): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (10): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
          (11): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
          )
        )
        (post_transformer_layer): EinOpsRearrange()
      )
      (thermal): SimpleTransformer(
        (pre_transformer_layer): Sequential(
          (0): Identity()
          (1): EinOpsRearrange()
        )
        (blocks): Sequential(
          (0): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (1): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (2): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (3): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (4): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (5): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (6): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (7): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (8): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (9): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (10): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
          (11): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          )
        )
        (post_transformer_layer): EinOpsRearrange()
      )
      (imu): SimpleTransformer(
        (pre_transformer_layer): Sequential(
          (0): Identity()
          (1): EinOpsRearrange()
        )
        (blocks): Sequential(
          (0): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (drop_path): Identity()
            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          )
          (1): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.140)
            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          )
          (2): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.280)
            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          )
          (3): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.420)
            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          )
          (4): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.560)
            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          )
          (5): BlockWithMasking(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
            )
            (drop_path): DropPath(drop_prob=0.700)
            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
          )
        )
        (post_transformer_layer): EinOpsRearrange()
      )
    )
    (modality_heads): ModuleDict(
      (vision): Sequential(
        (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (1): SelectElement()
        (2): Linear(in_features=1280, out_features=1024, bias=False)
      )
      (text): SelectEOSAndProject(
        (proj): Sequential(
          (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (1): Linear(in_features=1024, out_features=1024, bias=False)
        )
      )
      (audio): Sequential(
        (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (1): SelectElement()
        (2): Linear(in_features=768, out_features=1024, bias=False)
      )
      (depth): Sequential(
        (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (1): SelectElement()
        (2): Linear(in_features=384, out_features=1024, bias=False)
      )
      (thermal): Sequential(
        (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (1): SelectElement()
        (2): Linear(in_features=768, out_features=1024, bias=False)
      )
      (imu): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): SelectElement()
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=512, out_features=1024, bias=False)
      )
      (point): Sequential(
        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (1): Dropout(p=0.5, inplace=False)
        (2): Linear(in_features=512, out_features=1024, bias=False)
      )
    )
    (modality_postprocessors): ModuleDict(
      (vision): Normalize()
      (text): Sequential(
        (0): Normalize()
        (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)
      )
      (audio): Sequential(
        (0): Normalize()
        (1): LearnableLogitScaling(logit_scale_init=20.0,learnable=False, max_logit_scale=100)
      )
      (depth): Sequential(
        (0): Normalize()
        (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)
      )
      (thermal): Sequential(
        (0): Normalize()
        (1): LearnableLogitScaling(logit_scale_init=10.0,learnable=False, max_logit_scale=100)
      )
      (imu): Sequential(
        (0): Normalize()
        (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)
      )
      (point): Sequential(
        (0): Normalize()
        (1): LearnableLogitScaling(logit_scale_init=1.0,learnable=False, max_logit_scale=100)
      )
    )
    (point_trunk): PointTransformerBind(
      (point_encoder): PointTransformer(
        (group_divider): Group()
        (encoder): Encoder(
          (first_conv): Sequential(
            (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
          )
          (second_conv): Sequential(
            (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
          )
        )
        (reduce_dim): Linear(in_features=256, out_features=384, bias=True)
        (pos_embed): Sequential(
          (0): Linear(in_features=3, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=128, out_features=384, bias=True)
        )
        (blocks): TransformerEncoder(
          (blocks): ModuleList(
            (0): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): Identity()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.009)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.018)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.027)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.036)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.045)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (6): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.055)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (7): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.064)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (8): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.073)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (9): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.082)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (10): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.091)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
            (11): Block(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (drop_path): DropPath(drop_prob=0.100)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
              (attn): Attention(
                (qkv): Linear(in_features=384, out_features=1152, bias=False)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (image_bind_proj): Linear(in_features=1024, out_features=4096, bias=True)
  (image_bind_norm_1): RMSNorm()
  (image_bind_f1_1): Linear(in_features=4096, out_features=16384, bias=False)
  (image_bind_f2_1): Linear(in_features=16384, out_features=4096, bias=False)
  (image_bind_f3_1): Linear(in_features=4096, out_features=16384, bias=False)
  (image_bind_norm_2): RMSNorm()
  (image_bind_f1_2): Linear(in_features=4096, out_features=16384, bias=False)
  (image_bind_f2_2): Linear(in_features=16384, out_features=4096, bias=False)
  (image_bind_f3_2): Linear(in_features=4096, out_features=16384, bias=False)
  (image_bind_norm_3): RMSNorm()
  (image_bind_f1_3): Linear(in_features=4096, out_features=16384, bias=False)
  (image_bind_f2_3): Linear(in_features=16384, out_features=4096, bias=False)
  (image_bind_f3_3): Linear(in_features=4096, out_features=16384, bias=False)
  (llama): Transformer(
    (tok_embeddings): Embedding(32000, 4096)
    (layers): ModuleList(
      (0): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (1): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (2): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (3): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (4): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (5): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (6): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (7): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (8): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (9): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (10): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (11): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (12): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (13): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (14): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (15): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (16): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (17): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (18): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (19): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (20): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (21): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (22): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (23): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (24): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (25): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (26): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (27): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (28): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (29): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (30): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
      (31): TransformerBlock(
        (attention): Attention(
          (wq): Linear(in_features=4096, out_features=4096, bias=True)
          (wk): Linear(in_features=4096, out_features=4096, bias=False)
          (wv): Linear(in_features=4096, out_features=4096, bias=False)
          (wo): Linear(in_features=4096, out_features=4096, bias=True)
          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)
        )
        (feed_forward): FeedForward(
          (w1): Linear(in_features=4096, out_features=11008, bias=True)
          (w2): Linear(in_features=11008, out_features=4096, bias=True)
          (w3): Linear(in_features=4096, out_features=11008, bias=True)
          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)
          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)
          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)
          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)
          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)
        )
        (attention_norm): RMSNorm()
        (ffn_norm): RMSNorm()
      )
    )
    (norm): RMSNorm()
    (output): Linear(in_features=4096, out_features=32000, bias=False)
  )
  (prefix_query): Embedding(32, 4096)
  (criterion): CrossEntropyLoss()
)
[21:48:32.549927] Total Params: 8212.44 M
[21:48:32.554841] Trainable Params:: 39.43 M
[21:48:32.554888] image_bind.modality_preprocessors.vision.cls_token torch.Size([1, 1, 1280]) False
[21:48:32.554939] image_bind.modality_preprocessors.vision.rgbt_stem.proj.1.weight torch.Size([1280, 3, 2, 14, 14]) False
[21:48:32.554977] image_bind.modality_preprocessors.vision.pos_embedding_helper.pos_embed torch.Size([1, 257, 1280]) False
[21:48:32.555014] image_bind.modality_preprocessors.text.pos_embed torch.Size([1, 77, 1024]) False
[21:48:32.555050] image_bind.modality_preprocessors.text.token_embedding.weight torch.Size([49408, 1024]) False
[21:48:32.555086] image_bind.modality_preprocessors.audio.cls_token torch.Size([1, 1, 768]) False
[21:48:32.555122] image_bind.modality_preprocessors.audio.rgbt_stem.proj.weight torch.Size([768, 1, 16, 16]) False
[21:48:32.555160] image_bind.modality_preprocessors.audio.rgbt_stem.norm_layer.weight torch.Size([768]) False
[21:48:32.555197] image_bind.modality_preprocessors.audio.rgbt_stem.norm_layer.bias torch.Size([768]) False
[21:48:32.555237] image_bind.modality_preprocessors.audio.pos_embedding_helper.pos_embed torch.Size([1, 229, 768]) False
[21:48:32.555280] image_bind.modality_preprocessors.depth.cls_token torch.Size([1, 1, 384]) False
[21:48:32.555323] image_bind.modality_preprocessors.depth.depth_stem.proj.weight torch.Size([384, 1, 16, 16]) False
[21:48:32.555358] image_bind.modality_preprocessors.depth.depth_stem.norm_layer.weight torch.Size([384]) False
[21:48:32.555391] image_bind.modality_preprocessors.depth.depth_stem.norm_layer.bias torch.Size([384]) False
[21:48:32.555426] image_bind.modality_preprocessors.depth.pos_embedding_helper.pos_embed torch.Size([1, 197, 384]) False
[21:48:32.555462] image_bind.modality_preprocessors.thermal.cls_token torch.Size([1, 1, 768]) False
[21:48:32.555505] image_bind.modality_preprocessors.thermal.rgbt_stem.proj.weight torch.Size([768, 1, 16, 16]) False
[21:48:32.555541] image_bind.modality_preprocessors.thermal.rgbt_stem.norm_layer.weight torch.Size([768]) False
[21:48:32.555574] image_bind.modality_preprocessors.thermal.rgbt_stem.norm_layer.bias torch.Size([768]) False
[21:48:32.555608] image_bind.modality_preprocessors.thermal.pos_embedding_helper.pos_embed torch.Size([1, 197, 768]) False
[21:48:32.555643] image_bind.modality_preprocessors.imu.pos_embed torch.Size([1, 251, 512]) False
[21:48:32.555677] image_bind.modality_preprocessors.imu.cls_token torch.Size([1, 1, 512]) False
[21:48:32.555713] image_bind.modality_preprocessors.imu.imu_stem.proj.weight torch.Size([512, 48]) False
[21:48:32.555748] image_bind.modality_preprocessors.imu.imu_stem.norm_layer.weight torch.Size([512]) False
[21:48:32.555780] image_bind.modality_preprocessors.imu.imu_stem.norm_layer.bias torch.Size([512]) False
[21:48:32.555819] image_bind.modality_trunks.vision.pre_transformer_layer.0.weight torch.Size([1280]) False
[21:48:32.555858] image_bind.modality_trunks.vision.pre_transformer_layer.0.bias torch.Size([1280]) False
[21:48:32.555896] image_bind.modality_trunks.vision.blocks.0.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.555929] image_bind.modality_trunks.vision.blocks.0.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.555963] image_bind.modality_trunks.vision.blocks.0.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.555996] image_bind.modality_trunks.vision.blocks.0.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.556032] image_bind.modality_trunks.vision.blocks.0.norm_1.weight torch.Size([1280]) False
[21:48:32.556064] image_bind.modality_trunks.vision.blocks.0.norm_1.bias torch.Size([1280]) False
[21:48:32.556099] image_bind.modality_trunks.vision.blocks.0.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.556132] image_bind.modality_trunks.vision.blocks.0.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.556167] image_bind.modality_trunks.vision.blocks.0.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.556200] image_bind.modality_trunks.vision.blocks.0.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.556235] image_bind.modality_trunks.vision.blocks.0.norm_2.weight torch.Size([1280]) False
[21:48:32.556267] image_bind.modality_trunks.vision.blocks.0.norm_2.bias torch.Size([1280]) False
[21:48:32.556302] image_bind.modality_trunks.vision.blocks.1.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.556335] image_bind.modality_trunks.vision.blocks.1.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.556369] image_bind.modality_trunks.vision.blocks.1.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.556403] image_bind.modality_trunks.vision.blocks.1.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.556445] image_bind.modality_trunks.vision.blocks.1.norm_1.weight torch.Size([1280]) False
[21:48:32.556488] image_bind.modality_trunks.vision.blocks.1.norm_1.bias torch.Size([1280]) False
[21:48:32.556531] image_bind.modality_trunks.vision.blocks.1.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.556571] image_bind.modality_trunks.vision.blocks.1.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.556613] image_bind.modality_trunks.vision.blocks.1.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.556653] image_bind.modality_trunks.vision.blocks.1.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.556696] image_bind.modality_trunks.vision.blocks.1.norm_2.weight torch.Size([1280]) False
[21:48:32.556736] image_bind.modality_trunks.vision.blocks.1.norm_2.bias torch.Size([1280]) False
[21:48:32.556779] image_bind.modality_trunks.vision.blocks.2.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.556820] image_bind.modality_trunks.vision.blocks.2.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.556854] image_bind.modality_trunks.vision.blocks.2.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.556887] image_bind.modality_trunks.vision.blocks.2.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.556922] image_bind.modality_trunks.vision.blocks.2.norm_1.weight torch.Size([1280]) False
[21:48:32.556955] image_bind.modality_trunks.vision.blocks.2.norm_1.bias torch.Size([1280]) False
[21:48:32.556989] image_bind.modality_trunks.vision.blocks.2.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.557022] image_bind.modality_trunks.vision.blocks.2.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.557056] image_bind.modality_trunks.vision.blocks.2.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.557089] image_bind.modality_trunks.vision.blocks.2.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.557124] image_bind.modality_trunks.vision.blocks.2.norm_2.weight torch.Size([1280]) False
[21:48:32.557157] image_bind.modality_trunks.vision.blocks.2.norm_2.bias torch.Size([1280]) False
[21:48:32.557191] image_bind.modality_trunks.vision.blocks.3.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.557224] image_bind.modality_trunks.vision.blocks.3.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.557258] image_bind.modality_trunks.vision.blocks.3.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.557297] image_bind.modality_trunks.vision.blocks.3.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.557339] image_bind.modality_trunks.vision.blocks.3.norm_1.weight torch.Size([1280]) False
[21:48:32.557379] image_bind.modality_trunks.vision.blocks.3.norm_1.bias torch.Size([1280]) False
[21:48:32.557422] image_bind.modality_trunks.vision.blocks.3.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.557461] image_bind.modality_trunks.vision.blocks.3.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.557509] image_bind.modality_trunks.vision.blocks.3.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.557550] image_bind.modality_trunks.vision.blocks.3.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.557592] image_bind.modality_trunks.vision.blocks.3.norm_2.weight torch.Size([1280]) False
[21:48:32.557632] image_bind.modality_trunks.vision.blocks.3.norm_2.bias torch.Size([1280]) False
[21:48:32.557673] image_bind.modality_trunks.vision.blocks.4.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.557715] image_bind.modality_trunks.vision.blocks.4.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.557757] image_bind.modality_trunks.vision.blocks.4.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.557796] image_bind.modality_trunks.vision.blocks.4.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.557839] image_bind.modality_trunks.vision.blocks.4.norm_1.weight torch.Size([1280]) False
[21:48:32.557879] image_bind.modality_trunks.vision.blocks.4.norm_1.bias torch.Size([1280]) False
[21:48:32.557921] image_bind.modality_trunks.vision.blocks.4.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.557962] image_bind.modality_trunks.vision.blocks.4.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.558003] image_bind.modality_trunks.vision.blocks.4.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.558042] image_bind.modality_trunks.vision.blocks.4.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.558084] image_bind.modality_trunks.vision.blocks.4.norm_2.weight torch.Size([1280]) False
[21:48:32.558117] image_bind.modality_trunks.vision.blocks.4.norm_2.bias torch.Size([1280]) False
[21:48:32.558152] image_bind.modality_trunks.vision.blocks.5.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.558185] image_bind.modality_trunks.vision.blocks.5.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.558219] image_bind.modality_trunks.vision.blocks.5.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.558251] image_bind.modality_trunks.vision.blocks.5.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.558286] image_bind.modality_trunks.vision.blocks.5.norm_1.weight torch.Size([1280]) False
[21:48:32.558318] image_bind.modality_trunks.vision.blocks.5.norm_1.bias torch.Size([1280]) False
[21:48:32.558353] image_bind.modality_trunks.vision.blocks.5.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.558385] image_bind.modality_trunks.vision.blocks.5.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.558420] image_bind.modality_trunks.vision.blocks.5.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.558453] image_bind.modality_trunks.vision.blocks.5.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.558492] image_bind.modality_trunks.vision.blocks.5.norm_2.weight torch.Size([1280]) False
[21:48:32.558530] image_bind.modality_trunks.vision.blocks.5.norm_2.bias torch.Size([1280]) False
[21:48:32.558571] image_bind.modality_trunks.vision.blocks.6.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.558611] image_bind.modality_trunks.vision.blocks.6.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.558653] image_bind.modality_trunks.vision.blocks.6.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.558692] image_bind.modality_trunks.vision.blocks.6.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.558735] image_bind.modality_trunks.vision.blocks.6.norm_1.weight torch.Size([1280]) False
[21:48:32.558774] image_bind.modality_trunks.vision.blocks.6.norm_1.bias torch.Size([1280]) False
[21:48:32.558815] image_bind.modality_trunks.vision.blocks.6.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.558856] image_bind.modality_trunks.vision.blocks.6.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.558897] image_bind.modality_trunks.vision.blocks.6.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.558937] image_bind.modality_trunks.vision.blocks.6.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.558979] image_bind.modality_trunks.vision.blocks.6.norm_2.weight torch.Size([1280]) False
[21:48:32.559019] image_bind.modality_trunks.vision.blocks.6.norm_2.bias torch.Size([1280]) False
[21:48:32.559062] image_bind.modality_trunks.vision.blocks.7.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.559104] image_bind.modality_trunks.vision.blocks.7.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.559145] image_bind.modality_trunks.vision.blocks.7.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.559186] image_bind.modality_trunks.vision.blocks.7.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.559229] image_bind.modality_trunks.vision.blocks.7.norm_1.weight torch.Size([1280]) False
[21:48:32.559268] image_bind.modality_trunks.vision.blocks.7.norm_1.bias torch.Size([1280]) False
[21:48:32.559310] image_bind.modality_trunks.vision.blocks.7.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.559350] image_bind.modality_trunks.vision.blocks.7.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.559392] image_bind.modality_trunks.vision.blocks.7.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.559434] image_bind.modality_trunks.vision.blocks.7.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.559477] image_bind.modality_trunks.vision.blocks.7.norm_2.weight torch.Size([1280]) False
[21:48:32.559524] image_bind.modality_trunks.vision.blocks.7.norm_2.bias torch.Size([1280]) False
[21:48:32.559566] image_bind.modality_trunks.vision.blocks.8.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.559607] image_bind.modality_trunks.vision.blocks.8.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.559648] image_bind.modality_trunks.vision.blocks.8.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.559680] image_bind.modality_trunks.vision.blocks.8.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.559715] image_bind.modality_trunks.vision.blocks.8.norm_1.weight torch.Size([1280]) False
[21:48:32.559747] image_bind.modality_trunks.vision.blocks.8.norm_1.bias torch.Size([1280]) False
[21:48:32.559781] image_bind.modality_trunks.vision.blocks.8.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.559815] image_bind.modality_trunks.vision.blocks.8.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.559849] image_bind.modality_trunks.vision.blocks.8.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.559882] image_bind.modality_trunks.vision.blocks.8.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.559917] image_bind.modality_trunks.vision.blocks.8.norm_2.weight torch.Size([1280]) False
[21:48:32.559949] image_bind.modality_trunks.vision.blocks.8.norm_2.bias torch.Size([1280]) False
[21:48:32.559984] image_bind.modality_trunks.vision.blocks.9.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.560017] image_bind.modality_trunks.vision.blocks.9.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.560050] image_bind.modality_trunks.vision.blocks.9.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.560083] image_bind.modality_trunks.vision.blocks.9.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.560117] image_bind.modality_trunks.vision.blocks.9.norm_1.weight torch.Size([1280]) False
[21:48:32.560150] image_bind.modality_trunks.vision.blocks.9.norm_1.bias torch.Size([1280]) False
[21:48:32.560184] image_bind.modality_trunks.vision.blocks.9.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.560217] image_bind.modality_trunks.vision.blocks.9.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.560251] image_bind.modality_trunks.vision.blocks.9.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.560284] image_bind.modality_trunks.vision.blocks.9.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.560319] image_bind.modality_trunks.vision.blocks.9.norm_2.weight torch.Size([1280]) False
[21:48:32.560352] image_bind.modality_trunks.vision.blocks.9.norm_2.bias torch.Size([1280]) False
[21:48:32.560387] image_bind.modality_trunks.vision.blocks.10.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.560427] image_bind.modality_trunks.vision.blocks.10.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.560468] image_bind.modality_trunks.vision.blocks.10.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.560513] image_bind.modality_trunks.vision.blocks.10.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.560556] image_bind.modality_trunks.vision.blocks.10.norm_1.weight torch.Size([1280]) False
[21:48:32.560596] image_bind.modality_trunks.vision.blocks.10.norm_1.bias torch.Size([1280]) False
[21:48:32.560637] image_bind.modality_trunks.vision.blocks.10.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.560677] image_bind.modality_trunks.vision.blocks.10.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.560719] image_bind.modality_trunks.vision.blocks.10.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.560759] image_bind.modality_trunks.vision.blocks.10.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.560801] image_bind.modality_trunks.vision.blocks.10.norm_2.weight torch.Size([1280]) False
[21:48:32.560841] image_bind.modality_trunks.vision.blocks.10.norm_2.bias torch.Size([1280]) False
[21:48:32.560882] image_bind.modality_trunks.vision.blocks.11.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.560915] image_bind.modality_trunks.vision.blocks.11.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.560949] image_bind.modality_trunks.vision.blocks.11.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.560981] image_bind.modality_trunks.vision.blocks.11.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.561016] image_bind.modality_trunks.vision.blocks.11.norm_1.weight torch.Size([1280]) False
[21:48:32.561048] image_bind.modality_trunks.vision.blocks.11.norm_1.bias torch.Size([1280]) False
[21:48:32.561083] image_bind.modality_trunks.vision.blocks.11.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.561116] image_bind.modality_trunks.vision.blocks.11.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.561150] image_bind.modality_trunks.vision.blocks.11.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.561183] image_bind.modality_trunks.vision.blocks.11.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.561217] image_bind.modality_trunks.vision.blocks.11.norm_2.weight torch.Size([1280]) False
[21:48:32.561250] image_bind.modality_trunks.vision.blocks.11.norm_2.bias torch.Size([1280]) False
[21:48:32.561285] image_bind.modality_trunks.vision.blocks.12.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.561318] image_bind.modality_trunks.vision.blocks.12.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.561351] image_bind.modality_trunks.vision.blocks.12.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.561384] image_bind.modality_trunks.vision.blocks.12.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.561424] image_bind.modality_trunks.vision.blocks.12.norm_1.weight torch.Size([1280]) False
[21:48:32.561464] image_bind.modality_trunks.vision.blocks.12.norm_1.bias torch.Size([1280]) False
[21:48:32.561510] image_bind.modality_trunks.vision.blocks.12.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.561552] image_bind.modality_trunks.vision.blocks.12.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.561594] image_bind.modality_trunks.vision.blocks.12.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.561634] image_bind.modality_trunks.vision.blocks.12.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.561676] image_bind.modality_trunks.vision.blocks.12.norm_2.weight torch.Size([1280]) False
[21:48:32.561715] image_bind.modality_trunks.vision.blocks.12.norm_2.bias torch.Size([1280]) False
[21:48:32.561750] image_bind.modality_trunks.vision.blocks.13.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.561783] image_bind.modality_trunks.vision.blocks.13.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.561817] image_bind.modality_trunks.vision.blocks.13.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.561849] image_bind.modality_trunks.vision.blocks.13.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.561884] image_bind.modality_trunks.vision.blocks.13.norm_1.weight torch.Size([1280]) False
[21:48:32.561916] image_bind.modality_trunks.vision.blocks.13.norm_1.bias torch.Size([1280]) False
[21:48:32.561951] image_bind.modality_trunks.vision.blocks.13.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.561984] image_bind.modality_trunks.vision.blocks.13.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.562019] image_bind.modality_trunks.vision.blocks.13.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.562051] image_bind.modality_trunks.vision.blocks.13.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.562086] image_bind.modality_trunks.vision.blocks.13.norm_2.weight torch.Size([1280]) False
[21:48:32.562122] image_bind.modality_trunks.vision.blocks.13.norm_2.bias torch.Size([1280]) False
[21:48:32.562164] image_bind.modality_trunks.vision.blocks.14.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.562205] image_bind.modality_trunks.vision.blocks.14.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.562245] image_bind.modality_trunks.vision.blocks.14.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.562285] image_bind.modality_trunks.vision.blocks.14.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.562328] image_bind.modality_trunks.vision.blocks.14.norm_1.weight torch.Size([1280]) False
[21:48:32.562368] image_bind.modality_trunks.vision.blocks.14.norm_1.bias torch.Size([1280]) False
[21:48:32.562408] image_bind.modality_trunks.vision.blocks.14.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.562441] image_bind.modality_trunks.vision.blocks.14.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.562476] image_bind.modality_trunks.vision.blocks.14.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.562514] image_bind.modality_trunks.vision.blocks.14.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.562549] image_bind.modality_trunks.vision.blocks.14.norm_2.weight torch.Size([1280]) False
[21:48:32.562581] image_bind.modality_trunks.vision.blocks.14.norm_2.bias torch.Size([1280]) False
[21:48:32.562616] image_bind.modality_trunks.vision.blocks.15.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.562649] image_bind.modality_trunks.vision.blocks.15.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.562682] image_bind.modality_trunks.vision.blocks.15.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.562715] image_bind.modality_trunks.vision.blocks.15.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.562749] image_bind.modality_trunks.vision.blocks.15.norm_1.weight torch.Size([1280]) False
[21:48:32.562782] image_bind.modality_trunks.vision.blocks.15.norm_1.bias torch.Size([1280]) False
[21:48:32.562816] image_bind.modality_trunks.vision.blocks.15.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.562855] image_bind.modality_trunks.vision.blocks.15.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.562896] image_bind.modality_trunks.vision.blocks.15.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.562936] image_bind.modality_trunks.vision.blocks.15.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.562978] image_bind.modality_trunks.vision.blocks.15.norm_2.weight torch.Size([1280]) False
[21:48:32.563017] image_bind.modality_trunks.vision.blocks.15.norm_2.bias torch.Size([1280]) False
[21:48:32.563052] image_bind.modality_trunks.vision.blocks.16.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.563085] image_bind.modality_trunks.vision.blocks.16.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.563118] image_bind.modality_trunks.vision.blocks.16.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.563151] image_bind.modality_trunks.vision.blocks.16.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.563185] image_bind.modality_trunks.vision.blocks.16.norm_1.weight torch.Size([1280]) False
[21:48:32.563218] image_bind.modality_trunks.vision.blocks.16.norm_1.bias torch.Size([1280]) False
[21:48:32.563252] image_bind.modality_trunks.vision.blocks.16.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.563285] image_bind.modality_trunks.vision.blocks.16.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.563319] image_bind.modality_trunks.vision.blocks.16.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.563352] image_bind.modality_trunks.vision.blocks.16.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.563387] image_bind.modality_trunks.vision.blocks.16.norm_2.weight torch.Size([1280]) False
[21:48:32.563419] image_bind.modality_trunks.vision.blocks.16.norm_2.bias torch.Size([1280]) False
[21:48:32.563455] image_bind.modality_trunks.vision.blocks.17.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.563499] image_bind.modality_trunks.vision.blocks.17.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.563541] image_bind.modality_trunks.vision.blocks.17.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.563581] image_bind.modality_trunks.vision.blocks.17.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.563622] image_bind.modality_trunks.vision.blocks.17.norm_1.weight torch.Size([1280]) False
[21:48:32.563658] image_bind.modality_trunks.vision.blocks.17.norm_1.bias torch.Size([1280]) False
[21:48:32.563692] image_bind.modality_trunks.vision.blocks.17.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.563726] image_bind.modality_trunks.vision.blocks.17.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.563760] image_bind.modality_trunks.vision.blocks.17.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.563793] image_bind.modality_trunks.vision.blocks.17.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.563828] image_bind.modality_trunks.vision.blocks.17.norm_2.weight torch.Size([1280]) False
[21:48:32.563860] image_bind.modality_trunks.vision.blocks.17.norm_2.bias torch.Size([1280]) False
[21:48:32.563895] image_bind.modality_trunks.vision.blocks.18.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.563928] image_bind.modality_trunks.vision.blocks.18.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.563962] image_bind.modality_trunks.vision.blocks.18.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.563994] image_bind.modality_trunks.vision.blocks.18.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.564029] image_bind.modality_trunks.vision.blocks.18.norm_1.weight torch.Size([1280]) False
[21:48:32.564064] image_bind.modality_trunks.vision.blocks.18.norm_1.bias torch.Size([1280]) False
[21:48:32.564107] image_bind.modality_trunks.vision.blocks.18.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.564146] image_bind.modality_trunks.vision.blocks.18.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.564187] image_bind.modality_trunks.vision.blocks.18.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.564228] image_bind.modality_trunks.vision.blocks.18.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.564269] image_bind.modality_trunks.vision.blocks.18.norm_2.weight torch.Size([1280]) False
[21:48:32.564309] image_bind.modality_trunks.vision.blocks.18.norm_2.bias torch.Size([1280]) False
[21:48:32.564351] image_bind.modality_trunks.vision.blocks.19.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.564392] image_bind.modality_trunks.vision.blocks.19.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.564434] image_bind.modality_trunks.vision.blocks.19.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.564474] image_bind.modality_trunks.vision.blocks.19.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.564520] image_bind.modality_trunks.vision.blocks.19.norm_1.weight torch.Size([1280]) False
[21:48:32.564559] image_bind.modality_trunks.vision.blocks.19.norm_1.bias torch.Size([1280]) False
[21:48:32.564594] image_bind.modality_trunks.vision.blocks.19.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.564626] image_bind.modality_trunks.vision.blocks.19.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.564661] image_bind.modality_trunks.vision.blocks.19.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.564693] image_bind.modality_trunks.vision.blocks.19.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.564728] image_bind.modality_trunks.vision.blocks.19.norm_2.weight torch.Size([1280]) False
[21:48:32.564761] image_bind.modality_trunks.vision.blocks.19.norm_2.bias torch.Size([1280]) False
[21:48:32.564795] image_bind.modality_trunks.vision.blocks.20.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.564828] image_bind.modality_trunks.vision.blocks.20.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.564862] image_bind.modality_trunks.vision.blocks.20.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.564894] image_bind.modality_trunks.vision.blocks.20.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.564929] image_bind.modality_trunks.vision.blocks.20.norm_1.weight torch.Size([1280]) False
[21:48:32.564961] image_bind.modality_trunks.vision.blocks.20.norm_1.bias torch.Size([1280]) False
[21:48:32.564996] image_bind.modality_trunks.vision.blocks.20.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.565028] image_bind.modality_trunks.vision.blocks.20.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.565063] image_bind.modality_trunks.vision.blocks.20.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.565096] image_bind.modality_trunks.vision.blocks.20.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.565131] image_bind.modality_trunks.vision.blocks.20.norm_2.weight torch.Size([1280]) False
[21:48:32.565163] image_bind.modality_trunks.vision.blocks.20.norm_2.bias torch.Size([1280]) False
[21:48:32.565198] image_bind.modality_trunks.vision.blocks.21.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.565236] image_bind.modality_trunks.vision.blocks.21.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.565269] image_bind.modality_trunks.vision.blocks.21.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.565302] image_bind.modality_trunks.vision.blocks.21.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.565336] image_bind.modality_trunks.vision.blocks.21.norm_1.weight torch.Size([1280]) False
[21:48:32.565369] image_bind.modality_trunks.vision.blocks.21.norm_1.bias torch.Size([1280]) False
[21:48:32.565403] image_bind.modality_trunks.vision.blocks.21.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.565436] image_bind.modality_trunks.vision.blocks.21.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.565471] image_bind.modality_trunks.vision.blocks.21.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.565508] image_bind.modality_trunks.vision.blocks.21.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.565543] image_bind.modality_trunks.vision.blocks.21.norm_2.weight torch.Size([1280]) False
[21:48:32.565575] image_bind.modality_trunks.vision.blocks.21.norm_2.bias torch.Size([1280]) False
[21:48:32.565610] image_bind.modality_trunks.vision.blocks.22.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.565642] image_bind.modality_trunks.vision.blocks.22.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.565676] image_bind.modality_trunks.vision.blocks.22.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.565708] image_bind.modality_trunks.vision.blocks.22.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.565743] image_bind.modality_trunks.vision.blocks.22.norm_1.weight torch.Size([1280]) False
[21:48:32.565775] image_bind.modality_trunks.vision.blocks.22.norm_1.bias torch.Size([1280]) False
[21:48:32.565810] image_bind.modality_trunks.vision.blocks.22.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.565842] image_bind.modality_trunks.vision.blocks.22.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.565877] image_bind.modality_trunks.vision.blocks.22.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.565909] image_bind.modality_trunks.vision.blocks.22.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.565944] image_bind.modality_trunks.vision.blocks.22.norm_2.weight torch.Size([1280]) False
[21:48:32.565976] image_bind.modality_trunks.vision.blocks.22.norm_2.bias torch.Size([1280]) False
[21:48:32.566011] image_bind.modality_trunks.vision.blocks.23.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.566043] image_bind.modality_trunks.vision.blocks.23.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.566077] image_bind.modality_trunks.vision.blocks.23.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.566114] image_bind.modality_trunks.vision.blocks.23.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.566149] image_bind.modality_trunks.vision.blocks.23.norm_1.weight torch.Size([1280]) False
[21:48:32.566181] image_bind.modality_trunks.vision.blocks.23.norm_1.bias torch.Size([1280]) False
[21:48:32.566216] image_bind.modality_trunks.vision.blocks.23.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.566249] image_bind.modality_trunks.vision.blocks.23.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.566283] image_bind.modality_trunks.vision.blocks.23.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.566316] image_bind.modality_trunks.vision.blocks.23.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.566350] image_bind.modality_trunks.vision.blocks.23.norm_2.weight torch.Size([1280]) False
[21:48:32.566383] image_bind.modality_trunks.vision.blocks.23.norm_2.bias torch.Size([1280]) False
[21:48:32.566417] image_bind.modality_trunks.vision.blocks.24.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.566452] image_bind.modality_trunks.vision.blocks.24.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.566498] image_bind.modality_trunks.vision.blocks.24.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.566539] image_bind.modality_trunks.vision.blocks.24.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.566582] image_bind.modality_trunks.vision.blocks.24.norm_1.weight torch.Size([1280]) False
[21:48:32.566623] image_bind.modality_trunks.vision.blocks.24.norm_1.bias torch.Size([1280]) False
[21:48:32.566668] image_bind.modality_trunks.vision.blocks.24.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.566708] image_bind.modality_trunks.vision.blocks.24.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.566749] image_bind.modality_trunks.vision.blocks.24.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.566788] image_bind.modality_trunks.vision.blocks.24.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.566831] image_bind.modality_trunks.vision.blocks.24.norm_2.weight torch.Size([1280]) False
[21:48:32.566870] image_bind.modality_trunks.vision.blocks.24.norm_2.bias torch.Size([1280]) False
[21:48:32.566914] image_bind.modality_trunks.vision.blocks.25.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.566957] image_bind.modality_trunks.vision.blocks.25.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.566997] image_bind.modality_trunks.vision.blocks.25.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.567037] image_bind.modality_trunks.vision.blocks.25.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.567079] image_bind.modality_trunks.vision.blocks.25.norm_1.weight torch.Size([1280]) False
[21:48:32.567119] image_bind.modality_trunks.vision.blocks.25.norm_1.bias torch.Size([1280]) False
[21:48:32.567163] image_bind.modality_trunks.vision.blocks.25.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.567206] image_bind.modality_trunks.vision.blocks.25.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.567249] image_bind.modality_trunks.vision.blocks.25.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.567291] image_bind.modality_trunks.vision.blocks.25.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.567333] image_bind.modality_trunks.vision.blocks.25.norm_2.weight torch.Size([1280]) False
[21:48:32.567372] image_bind.modality_trunks.vision.blocks.25.norm_2.bias torch.Size([1280]) False
[21:48:32.567414] image_bind.modality_trunks.vision.blocks.26.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.567457] image_bind.modality_trunks.vision.blocks.26.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.567504] image_bind.modality_trunks.vision.blocks.26.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.567547] image_bind.modality_trunks.vision.blocks.26.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.567586] image_bind.modality_trunks.vision.blocks.26.norm_1.weight torch.Size([1280]) False
[21:48:32.567619] image_bind.modality_trunks.vision.blocks.26.norm_1.bias torch.Size([1280]) False
[21:48:32.567653] image_bind.modality_trunks.vision.blocks.26.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.567686] image_bind.modality_trunks.vision.blocks.26.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.567721] image_bind.modality_trunks.vision.blocks.26.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.567754] image_bind.modality_trunks.vision.blocks.26.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.567789] image_bind.modality_trunks.vision.blocks.26.norm_2.weight torch.Size([1280]) False
[21:48:32.567822] image_bind.modality_trunks.vision.blocks.26.norm_2.bias torch.Size([1280]) False
[21:48:32.567857] image_bind.modality_trunks.vision.blocks.27.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.567889] image_bind.modality_trunks.vision.blocks.27.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.567923] image_bind.modality_trunks.vision.blocks.27.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.567956] image_bind.modality_trunks.vision.blocks.27.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.567990] image_bind.modality_trunks.vision.blocks.27.norm_1.weight torch.Size([1280]) False
[21:48:32.568023] image_bind.modality_trunks.vision.blocks.27.norm_1.bias torch.Size([1280]) False
[21:48:32.568057] image_bind.modality_trunks.vision.blocks.27.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.568091] image_bind.modality_trunks.vision.blocks.27.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.568134] image_bind.modality_trunks.vision.blocks.27.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.568176] image_bind.modality_trunks.vision.blocks.27.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.568219] image_bind.modality_trunks.vision.blocks.27.norm_2.weight torch.Size([1280]) False
[21:48:32.568260] image_bind.modality_trunks.vision.blocks.27.norm_2.bias torch.Size([1280]) False
[21:48:32.568301] image_bind.modality_trunks.vision.blocks.28.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.568341] image_bind.modality_trunks.vision.blocks.28.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.568380] image_bind.modality_trunks.vision.blocks.28.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.568420] image_bind.modality_trunks.vision.blocks.28.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.568463] image_bind.modality_trunks.vision.blocks.28.norm_1.weight torch.Size([1280]) False
[21:48:32.568509] image_bind.modality_trunks.vision.blocks.28.norm_1.bias torch.Size([1280]) False
[21:48:32.568548] image_bind.modality_trunks.vision.blocks.28.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.568581] image_bind.modality_trunks.vision.blocks.28.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.568616] image_bind.modality_trunks.vision.blocks.28.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.568648] image_bind.modality_trunks.vision.blocks.28.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.568683] image_bind.modality_trunks.vision.blocks.28.norm_2.weight torch.Size([1280]) False
[21:48:32.568715] image_bind.modality_trunks.vision.blocks.28.norm_2.bias torch.Size([1280]) False
[21:48:32.568750] image_bind.modality_trunks.vision.blocks.29.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.568783] image_bind.modality_trunks.vision.blocks.29.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.568817] image_bind.modality_trunks.vision.blocks.29.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.568855] image_bind.modality_trunks.vision.blocks.29.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.568895] image_bind.modality_trunks.vision.blocks.29.norm_1.weight torch.Size([1280]) False
[21:48:32.568929] image_bind.modality_trunks.vision.blocks.29.norm_1.bias torch.Size([1280]) False
[21:48:32.568970] image_bind.modality_trunks.vision.blocks.29.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.569012] image_bind.modality_trunks.vision.blocks.29.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.569055] image_bind.modality_trunks.vision.blocks.29.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.569094] image_bind.modality_trunks.vision.blocks.29.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.569136] image_bind.modality_trunks.vision.blocks.29.norm_2.weight torch.Size([1280]) False
[21:48:32.569176] image_bind.modality_trunks.vision.blocks.29.norm_2.bias torch.Size([1280]) False
[21:48:32.569220] image_bind.modality_trunks.vision.blocks.30.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.569263] image_bind.modality_trunks.vision.blocks.30.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.569304] image_bind.modality_trunks.vision.blocks.30.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.569348] image_bind.modality_trunks.vision.blocks.30.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.569390] image_bind.modality_trunks.vision.blocks.30.norm_1.weight torch.Size([1280]) False
[21:48:32.569422] image_bind.modality_trunks.vision.blocks.30.norm_1.bias torch.Size([1280]) False
[21:48:32.569456] image_bind.modality_trunks.vision.blocks.30.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.569495] image_bind.modality_trunks.vision.blocks.30.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.569533] image_bind.modality_trunks.vision.blocks.30.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.569566] image_bind.modality_trunks.vision.blocks.30.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.569601] image_bind.modality_trunks.vision.blocks.30.norm_2.weight torch.Size([1280]) False
[21:48:32.569634] image_bind.modality_trunks.vision.blocks.30.norm_2.bias torch.Size([1280]) False
[21:48:32.569669] image_bind.modality_trunks.vision.blocks.31.attn.in_proj_weight torch.Size([3840, 1280]) False
[21:48:32.569702] image_bind.modality_trunks.vision.blocks.31.attn.in_proj_bias torch.Size([3840]) False
[21:48:32.569735] image_bind.modality_trunks.vision.blocks.31.attn.out_proj.weight torch.Size([1280, 1280]) False
[21:48:32.569768] image_bind.modality_trunks.vision.blocks.31.attn.out_proj.bias torch.Size([1280]) False
[21:48:32.569803] image_bind.modality_trunks.vision.blocks.31.norm_1.weight torch.Size([1280]) False
[21:48:32.569835] image_bind.modality_trunks.vision.blocks.31.norm_1.bias torch.Size([1280]) False
[21:48:32.569874] image_bind.modality_trunks.vision.blocks.31.mlp.fc1.weight torch.Size([5120, 1280]) False
[21:48:32.569916] image_bind.modality_trunks.vision.blocks.31.mlp.fc1.bias torch.Size([5120]) False
[21:48:32.569956] image_bind.modality_trunks.vision.blocks.31.mlp.fc2.weight torch.Size([1280, 5120]) False
[21:48:32.569996] image_bind.modality_trunks.vision.blocks.31.mlp.fc2.bias torch.Size([1280]) False
[21:48:32.570040] image_bind.modality_trunks.vision.blocks.31.norm_2.weight torch.Size([1280]) False
[21:48:32.570080] image_bind.modality_trunks.vision.blocks.31.norm_2.bias torch.Size([1280]) False
[21:48:32.570133] image_bind.modality_trunks.text.blocks.0.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.570175] image_bind.modality_trunks.text.blocks.0.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.570209] image_bind.modality_trunks.text.blocks.0.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.570242] image_bind.modality_trunks.text.blocks.0.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.570277] image_bind.modality_trunks.text.blocks.0.norm_1.weight torch.Size([1024]) False
[21:48:32.570310] image_bind.modality_trunks.text.blocks.0.norm_1.bias torch.Size([1024]) False
[21:48:32.570345] image_bind.modality_trunks.text.blocks.0.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.570377] image_bind.modality_trunks.text.blocks.0.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.570412] image_bind.modality_trunks.text.blocks.0.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.570445] image_bind.modality_trunks.text.blocks.0.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.570480] image_bind.modality_trunks.text.blocks.0.norm_2.weight torch.Size([1024]) False
[21:48:32.570518] image_bind.modality_trunks.text.blocks.0.norm_2.bias torch.Size([1024]) False
[21:48:32.570553] image_bind.modality_trunks.text.blocks.1.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.570591] image_bind.modality_trunks.text.blocks.1.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.570632] image_bind.modality_trunks.text.blocks.1.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.570676] image_bind.modality_trunks.text.blocks.1.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.570719] image_bind.modality_trunks.text.blocks.1.norm_1.weight torch.Size([1024]) False
[21:48:32.570760] image_bind.modality_trunks.text.blocks.1.norm_1.bias torch.Size([1024]) False
[21:48:32.570802] image_bind.modality_trunks.text.blocks.1.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.570841] image_bind.modality_trunks.text.blocks.1.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.570883] image_bind.modality_trunks.text.blocks.1.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.570920] image_bind.modality_trunks.text.blocks.1.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.570954] image_bind.modality_trunks.text.blocks.1.norm_2.weight torch.Size([1024]) False
[21:48:32.570987] image_bind.modality_trunks.text.blocks.1.norm_2.bias torch.Size([1024]) False
[21:48:32.571022] image_bind.modality_trunks.text.blocks.2.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.571054] image_bind.modality_trunks.text.blocks.2.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.571088] image_bind.modality_trunks.text.blocks.2.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.571121] image_bind.modality_trunks.text.blocks.2.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.571156] image_bind.modality_trunks.text.blocks.2.norm_1.weight torch.Size([1024]) False
[21:48:32.571188] image_bind.modality_trunks.text.blocks.2.norm_1.bias torch.Size([1024]) False
[21:48:32.571223] image_bind.modality_trunks.text.blocks.2.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.571256] image_bind.modality_trunks.text.blocks.2.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.571290] image_bind.modality_trunks.text.blocks.2.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.571324] image_bind.modality_trunks.text.blocks.2.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.571366] image_bind.modality_trunks.text.blocks.2.norm_2.weight torch.Size([1024]) False
[21:48:32.571399] image_bind.modality_trunks.text.blocks.2.norm_2.bias torch.Size([1024]) False
[21:48:32.571434] image_bind.modality_trunks.text.blocks.3.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.571467] image_bind.modality_trunks.text.blocks.3.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.571505] image_bind.modality_trunks.text.blocks.3.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.571538] image_bind.modality_trunks.text.blocks.3.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.571573] image_bind.modality_trunks.text.blocks.3.norm_1.weight torch.Size([1024]) False
[21:48:32.571606] image_bind.modality_trunks.text.blocks.3.norm_1.bias torch.Size([1024]) False
[21:48:32.571640] image_bind.modality_trunks.text.blocks.3.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.571673] image_bind.modality_trunks.text.blocks.3.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.571708] image_bind.modality_trunks.text.blocks.3.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.571740] image_bind.modality_trunks.text.blocks.3.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.571775] image_bind.modality_trunks.text.blocks.3.norm_2.weight torch.Size([1024]) False
[21:48:32.571808] image_bind.modality_trunks.text.blocks.3.norm_2.bias torch.Size([1024]) False
[21:48:32.571849] image_bind.modality_trunks.text.blocks.4.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.571888] image_bind.modality_trunks.text.blocks.4.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.571931] image_bind.modality_trunks.text.blocks.4.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.571973] image_bind.modality_trunks.text.blocks.4.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.572017] image_bind.modality_trunks.text.blocks.4.norm_1.weight torch.Size([1024]) False
[21:48:32.572058] image_bind.modality_trunks.text.blocks.4.norm_1.bias torch.Size([1024]) False
[21:48:32.572100] image_bind.modality_trunks.text.blocks.4.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.572139] image_bind.modality_trunks.text.blocks.4.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.572182] image_bind.modality_trunks.text.blocks.4.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.572223] image_bind.modality_trunks.text.blocks.4.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.572269] image_bind.modality_trunks.text.blocks.4.norm_2.weight torch.Size([1024]) False
[21:48:32.572311] image_bind.modality_trunks.text.blocks.4.norm_2.bias torch.Size([1024]) False
[21:48:32.572353] image_bind.modality_trunks.text.blocks.5.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.572393] image_bind.modality_trunks.text.blocks.5.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.572433] image_bind.modality_trunks.text.blocks.5.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.572475] image_bind.modality_trunks.text.blocks.5.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.572526] image_bind.modality_trunks.text.blocks.5.norm_1.weight torch.Size([1024]) False
[21:48:32.572566] image_bind.modality_trunks.text.blocks.5.norm_1.bias torch.Size([1024]) False
[21:48:32.572608] image_bind.modality_trunks.text.blocks.5.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.572648] image_bind.modality_trunks.text.blocks.5.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.572690] image_bind.modality_trunks.text.blocks.5.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.572730] image_bind.modality_trunks.text.blocks.5.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.572773] image_bind.modality_trunks.text.blocks.5.norm_2.weight torch.Size([1024]) False
[21:48:32.572814] image_bind.modality_trunks.text.blocks.5.norm_2.bias torch.Size([1024]) False
[21:48:32.572859] image_bind.modality_trunks.text.blocks.6.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.572901] image_bind.modality_trunks.text.blocks.6.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.572935] image_bind.modality_trunks.text.blocks.6.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.572968] image_bind.modality_trunks.text.blocks.6.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.573003] image_bind.modality_trunks.text.blocks.6.norm_1.weight torch.Size([1024]) False
[21:48:32.573036] image_bind.modality_trunks.text.blocks.6.norm_1.bias torch.Size([1024]) False
[21:48:32.573070] image_bind.modality_trunks.text.blocks.6.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.573103] image_bind.modality_trunks.text.blocks.6.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.573138] image_bind.modality_trunks.text.blocks.6.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.573171] image_bind.modality_trunks.text.blocks.6.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.573206] image_bind.modality_trunks.text.blocks.6.norm_2.weight torch.Size([1024]) False
[21:48:32.573238] image_bind.modality_trunks.text.blocks.6.norm_2.bias torch.Size([1024]) False
[21:48:32.573273] image_bind.modality_trunks.text.blocks.7.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.573306] image_bind.modality_trunks.text.blocks.7.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.573344] image_bind.modality_trunks.text.blocks.7.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.573385] image_bind.modality_trunks.text.blocks.7.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.573428] image_bind.modality_trunks.text.blocks.7.norm_1.weight torch.Size([1024]) False
[21:48:32.573468] image_bind.modality_trunks.text.blocks.7.norm_1.bias torch.Size([1024]) False
[21:48:32.573513] image_bind.modality_trunks.text.blocks.7.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.573553] image_bind.modality_trunks.text.blocks.7.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.573596] image_bind.modality_trunks.text.blocks.7.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.573630] image_bind.modality_trunks.text.blocks.7.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.573665] image_bind.modality_trunks.text.blocks.7.norm_2.weight torch.Size([1024]) False
[21:48:32.573699] image_bind.modality_trunks.text.blocks.7.norm_2.bias torch.Size([1024]) False
[21:48:32.573734] image_bind.modality_trunks.text.blocks.8.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.573767] image_bind.modality_trunks.text.blocks.8.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.573800] image_bind.modality_trunks.text.blocks.8.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.573833] image_bind.modality_trunks.text.blocks.8.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.573868] image_bind.modality_trunks.text.blocks.8.norm_1.weight torch.Size([1024]) False
[21:48:32.573901] image_bind.modality_trunks.text.blocks.8.norm_1.bias torch.Size([1024]) False
[21:48:32.573936] image_bind.modality_trunks.text.blocks.8.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.573968] image_bind.modality_trunks.text.blocks.8.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.574003] image_bind.modality_trunks.text.blocks.8.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.574036] image_bind.modality_trunks.text.blocks.8.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.574071] image_bind.modality_trunks.text.blocks.8.norm_2.weight torch.Size([1024]) False
[21:48:32.574105] image_bind.modality_trunks.text.blocks.8.norm_2.bias torch.Size([1024]) False
[21:48:32.574147] image_bind.modality_trunks.text.blocks.9.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.574187] image_bind.modality_trunks.text.blocks.9.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.574225] image_bind.modality_trunks.text.blocks.9.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.574258] image_bind.modality_trunks.text.blocks.9.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.574293] image_bind.modality_trunks.text.blocks.9.norm_1.weight torch.Size([1024]) False
[21:48:32.574325] image_bind.modality_trunks.text.blocks.9.norm_1.bias torch.Size([1024]) False
[21:48:32.574360] image_bind.modality_trunks.text.blocks.9.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.574393] image_bind.modality_trunks.text.blocks.9.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.574427] image_bind.modality_trunks.text.blocks.9.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.574460] image_bind.modality_trunks.text.blocks.9.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.574500] image_bind.modality_trunks.text.blocks.9.norm_2.weight torch.Size([1024]) False
[21:48:32.574533] image_bind.modality_trunks.text.blocks.9.norm_2.bias torch.Size([1024]) False
[21:48:32.574568] image_bind.modality_trunks.text.blocks.10.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.574602] image_bind.modality_trunks.text.blocks.10.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.574641] image_bind.modality_trunks.text.blocks.10.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.574683] image_bind.modality_trunks.text.blocks.10.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.574723] image_bind.modality_trunks.text.blocks.10.norm_1.weight torch.Size([1024]) False
[21:48:32.574756] image_bind.modality_trunks.text.blocks.10.norm_1.bias torch.Size([1024]) False
[21:48:32.574790] image_bind.modality_trunks.text.blocks.10.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.574823] image_bind.modality_trunks.text.blocks.10.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.574858] image_bind.modality_trunks.text.blocks.10.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.574891] image_bind.modality_trunks.text.blocks.10.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.574926] image_bind.modality_trunks.text.blocks.10.norm_2.weight torch.Size([1024]) False
[21:48:32.574959] image_bind.modality_trunks.text.blocks.10.norm_2.bias torch.Size([1024]) False
[21:48:32.574995] image_bind.modality_trunks.text.blocks.11.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.575027] image_bind.modality_trunks.text.blocks.11.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.575061] image_bind.modality_trunks.text.blocks.11.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.575094] image_bind.modality_trunks.text.blocks.11.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.575129] image_bind.modality_trunks.text.blocks.11.norm_1.weight torch.Size([1024]) False
[21:48:32.575163] image_bind.modality_trunks.text.blocks.11.norm_1.bias torch.Size([1024]) False
[21:48:32.575206] image_bind.modality_trunks.text.blocks.11.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.575249] image_bind.modality_trunks.text.blocks.11.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.575293] image_bind.modality_trunks.text.blocks.11.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.575332] image_bind.modality_trunks.text.blocks.11.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.575374] image_bind.modality_trunks.text.blocks.11.norm_2.weight torch.Size([1024]) False
[21:48:32.575413] image_bind.modality_trunks.text.blocks.11.norm_2.bias torch.Size([1024]) False
[21:48:32.575456] image_bind.modality_trunks.text.blocks.12.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.575502] image_bind.modality_trunks.text.blocks.12.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.575541] image_bind.modality_trunks.text.blocks.12.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.575573] image_bind.modality_trunks.text.blocks.12.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.575608] image_bind.modality_trunks.text.blocks.12.norm_1.weight torch.Size([1024]) False
[21:48:32.575641] image_bind.modality_trunks.text.blocks.12.norm_1.bias torch.Size([1024]) False
[21:48:32.575677] image_bind.modality_trunks.text.blocks.12.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.575709] image_bind.modality_trunks.text.blocks.12.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.575744] image_bind.modality_trunks.text.blocks.12.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.575777] image_bind.modality_trunks.text.blocks.12.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.575812] image_bind.modality_trunks.text.blocks.12.norm_2.weight torch.Size([1024]) False
[21:48:32.575845] image_bind.modality_trunks.text.blocks.12.norm_2.bias torch.Size([1024]) False
[21:48:32.575880] image_bind.modality_trunks.text.blocks.13.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.575913] image_bind.modality_trunks.text.blocks.13.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.575947] image_bind.modality_trunks.text.blocks.13.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.575987] image_bind.modality_trunks.text.blocks.13.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.576029] image_bind.modality_trunks.text.blocks.13.norm_1.weight torch.Size([1024]) False
[21:48:32.576069] image_bind.modality_trunks.text.blocks.13.norm_1.bias torch.Size([1024]) False
[21:48:32.576113] image_bind.modality_trunks.text.blocks.13.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.576154] image_bind.modality_trunks.text.blocks.13.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.576199] image_bind.modality_trunks.text.blocks.13.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.576241] image_bind.modality_trunks.text.blocks.13.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.576285] image_bind.modality_trunks.text.blocks.13.norm_2.weight torch.Size([1024]) False
[21:48:32.576324] image_bind.modality_trunks.text.blocks.13.norm_2.bias torch.Size([1024]) False
[21:48:32.576366] image_bind.modality_trunks.text.blocks.14.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.576407] image_bind.modality_trunks.text.blocks.14.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.576450] image_bind.modality_trunks.text.blocks.14.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.576496] image_bind.modality_trunks.text.blocks.14.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.576539] image_bind.modality_trunks.text.blocks.14.norm_1.weight torch.Size([1024]) False
[21:48:32.576579] image_bind.modality_trunks.text.blocks.14.norm_1.bias torch.Size([1024]) False
[21:48:32.576622] image_bind.modality_trunks.text.blocks.14.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.576663] image_bind.modality_trunks.text.blocks.14.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.576706] image_bind.modality_trunks.text.blocks.14.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.576748] image_bind.modality_trunks.text.blocks.14.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.576792] image_bind.modality_trunks.text.blocks.14.norm_2.weight torch.Size([1024]) False
[21:48:32.576834] image_bind.modality_trunks.text.blocks.14.norm_2.bias torch.Size([1024]) False
[21:48:32.576878] image_bind.modality_trunks.text.blocks.15.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.576919] image_bind.modality_trunks.text.blocks.15.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.576960] image_bind.modality_trunks.text.blocks.15.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.576995] image_bind.modality_trunks.text.blocks.15.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.577029] image_bind.modality_trunks.text.blocks.15.norm_1.weight torch.Size([1024]) False
[21:48:32.577062] image_bind.modality_trunks.text.blocks.15.norm_1.bias torch.Size([1024]) False
[21:48:32.577098] image_bind.modality_trunks.text.blocks.15.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.577130] image_bind.modality_trunks.text.blocks.15.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.577165] image_bind.modality_trunks.text.blocks.15.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.577198] image_bind.modality_trunks.text.blocks.15.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.577233] image_bind.modality_trunks.text.blocks.15.norm_2.weight torch.Size([1024]) False
[21:48:32.577265] image_bind.modality_trunks.text.blocks.15.norm_2.bias torch.Size([1024]) False
[21:48:32.577300] image_bind.modality_trunks.text.blocks.16.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.577333] image_bind.modality_trunks.text.blocks.16.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.577367] image_bind.modality_trunks.text.blocks.16.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.577399] image_bind.modality_trunks.text.blocks.16.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.577434] image_bind.modality_trunks.text.blocks.16.norm_1.weight torch.Size([1024]) False
[21:48:32.577466] image_bind.modality_trunks.text.blocks.16.norm_1.bias torch.Size([1024]) False
[21:48:32.577507] image_bind.modality_trunks.text.blocks.16.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.577548] image_bind.modality_trunks.text.blocks.16.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.577583] image_bind.modality_trunks.text.blocks.16.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.577615] image_bind.modality_trunks.text.blocks.16.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.577652] image_bind.modality_trunks.text.blocks.16.norm_2.weight torch.Size([1024]) False
[21:48:32.577684] image_bind.modality_trunks.text.blocks.16.norm_2.bias torch.Size([1024]) False
[21:48:32.577719] image_bind.modality_trunks.text.blocks.17.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.577752] image_bind.modality_trunks.text.blocks.17.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.577786] image_bind.modality_trunks.text.blocks.17.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.577818] image_bind.modality_trunks.text.blocks.17.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.577853] image_bind.modality_trunks.text.blocks.17.norm_1.weight torch.Size([1024]) False
[21:48:32.577885] image_bind.modality_trunks.text.blocks.17.norm_1.bias torch.Size([1024]) False
[21:48:32.577920] image_bind.modality_trunks.text.blocks.17.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.577953] image_bind.modality_trunks.text.blocks.17.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.577993] image_bind.modality_trunks.text.blocks.17.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.578034] image_bind.modality_trunks.text.blocks.17.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.578079] image_bind.modality_trunks.text.blocks.17.norm_2.weight torch.Size([1024]) False
[21:48:32.578120] image_bind.modality_trunks.text.blocks.17.norm_2.bias torch.Size([1024]) False
[21:48:32.578162] image_bind.modality_trunks.text.blocks.18.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.578201] image_bind.modality_trunks.text.blocks.18.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.578241] image_bind.modality_trunks.text.blocks.18.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.578280] image_bind.modality_trunks.text.blocks.18.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.578315] image_bind.modality_trunks.text.blocks.18.norm_1.weight torch.Size([1024]) False
[21:48:32.578348] image_bind.modality_trunks.text.blocks.18.norm_1.bias torch.Size([1024]) False
[21:48:32.578384] image_bind.modality_trunks.text.blocks.18.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.578416] image_bind.modality_trunks.text.blocks.18.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.578451] image_bind.modality_trunks.text.blocks.18.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.578484] image_bind.modality_trunks.text.blocks.18.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.578526] image_bind.modality_trunks.text.blocks.18.norm_2.weight torch.Size([1024]) False
[21:48:32.578559] image_bind.modality_trunks.text.blocks.18.norm_2.bias torch.Size([1024]) False
[21:48:32.578594] image_bind.modality_trunks.text.blocks.19.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.578627] image_bind.modality_trunks.text.blocks.19.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.578661] image_bind.modality_trunks.text.blocks.19.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.578694] image_bind.modality_trunks.text.blocks.19.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.578729] image_bind.modality_trunks.text.blocks.19.norm_1.weight torch.Size([1024]) False
[21:48:32.578771] image_bind.modality_trunks.text.blocks.19.norm_1.bias torch.Size([1024]) False
[21:48:32.578814] image_bind.modality_trunks.text.blocks.19.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.578854] image_bind.modality_trunks.text.blocks.19.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.578897] image_bind.modality_trunks.text.blocks.19.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.578937] image_bind.modality_trunks.text.blocks.19.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.578982] image_bind.modality_trunks.text.blocks.19.norm_2.weight torch.Size([1024]) False
[21:48:32.579023] image_bind.modality_trunks.text.blocks.19.norm_2.bias torch.Size([1024]) False
[21:48:32.579066] image_bind.modality_trunks.text.blocks.20.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.579108] image_bind.modality_trunks.text.blocks.20.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.579148] image_bind.modality_trunks.text.blocks.20.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.579187] image_bind.modality_trunks.text.blocks.20.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.579231] image_bind.modality_trunks.text.blocks.20.norm_1.weight torch.Size([1024]) False
[21:48:32.579271] image_bind.modality_trunks.text.blocks.20.norm_1.bias torch.Size([1024]) False
[21:48:32.579315] image_bind.modality_trunks.text.blocks.20.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.579358] image_bind.modality_trunks.text.blocks.20.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.579400] image_bind.modality_trunks.text.blocks.20.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.579433] image_bind.modality_trunks.text.blocks.20.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.579468] image_bind.modality_trunks.text.blocks.20.norm_2.weight torch.Size([1024]) False
[21:48:32.579507] image_bind.modality_trunks.text.blocks.20.norm_2.bias torch.Size([1024]) False
[21:48:32.579542] image_bind.modality_trunks.text.blocks.21.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.579575] image_bind.modality_trunks.text.blocks.21.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.579608] image_bind.modality_trunks.text.blocks.21.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.579641] image_bind.modality_trunks.text.blocks.21.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.579676] image_bind.modality_trunks.text.blocks.21.norm_1.weight torch.Size([1024]) False
[21:48:32.579709] image_bind.modality_trunks.text.blocks.21.norm_1.bias torch.Size([1024]) False
[21:48:32.579744] image_bind.modality_trunks.text.blocks.21.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.579781] image_bind.modality_trunks.text.blocks.21.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.579822] image_bind.modality_trunks.text.blocks.21.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.579861] image_bind.modality_trunks.text.blocks.21.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.579904] image_bind.modality_trunks.text.blocks.21.norm_2.weight torch.Size([1024]) False
[21:48:32.579946] image_bind.modality_trunks.text.blocks.21.norm_2.bias torch.Size([1024]) False
[21:48:32.579989] image_bind.modality_trunks.text.blocks.22.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.580029] image_bind.modality_trunks.text.blocks.22.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.580069] image_bind.modality_trunks.text.blocks.22.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.580108] image_bind.modality_trunks.text.blocks.22.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.580143] image_bind.modality_trunks.text.blocks.22.norm_1.weight torch.Size([1024]) False
[21:48:32.580176] image_bind.modality_trunks.text.blocks.22.norm_1.bias torch.Size([1024]) False
[21:48:32.580211] image_bind.modality_trunks.text.blocks.22.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.580243] image_bind.modality_trunks.text.blocks.22.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.580278] image_bind.modality_trunks.text.blocks.22.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.580311] image_bind.modality_trunks.text.blocks.22.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.580346] image_bind.modality_trunks.text.blocks.22.norm_2.weight torch.Size([1024]) False
[21:48:32.580379] image_bind.modality_trunks.text.blocks.22.norm_2.bias torch.Size([1024]) False
[21:48:32.580414] image_bind.modality_trunks.text.blocks.23.attn.in_proj_weight torch.Size([3072, 1024]) False
[21:48:32.580447] image_bind.modality_trunks.text.blocks.23.attn.in_proj_bias torch.Size([3072]) False
[21:48:32.580481] image_bind.modality_trunks.text.blocks.23.attn.out_proj.weight torch.Size([1024, 1024]) False
[21:48:32.580520] image_bind.modality_trunks.text.blocks.23.attn.out_proj.bias torch.Size([1024]) False
[21:48:32.580563] image_bind.modality_trunks.text.blocks.23.norm_1.weight torch.Size([1024]) False
[21:48:32.580605] image_bind.modality_trunks.text.blocks.23.norm_1.bias torch.Size([1024]) False
[21:48:32.580648] image_bind.modality_trunks.text.blocks.23.mlp.fc1.weight torch.Size([4096, 1024]) False
[21:48:32.580688] image_bind.modality_trunks.text.blocks.23.mlp.fc1.bias torch.Size([4096]) False
[21:48:32.580731] image_bind.modality_trunks.text.blocks.23.mlp.fc2.weight torch.Size([1024, 4096]) False
[21:48:32.580770] image_bind.modality_trunks.text.blocks.23.mlp.fc2.bias torch.Size([1024]) False
[21:48:32.580812] image_bind.modality_trunks.text.blocks.23.norm_2.weight torch.Size([1024]) False
[21:48:32.580853] image_bind.modality_trunks.text.blocks.23.norm_2.bias torch.Size([1024]) False
[21:48:32.580902] image_bind.modality_trunks.audio.blocks.0.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.580936] image_bind.modality_trunks.audio.blocks.0.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.580969] image_bind.modality_trunks.audio.blocks.0.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.581003] image_bind.modality_trunks.audio.blocks.0.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.581037] image_bind.modality_trunks.audio.blocks.0.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.581070] image_bind.modality_trunks.audio.blocks.0.attn.out_proj.bias torch.Size([768]) False
[21:48:32.581105] image_bind.modality_trunks.audio.blocks.0.norm_1.weight torch.Size([768]) False
[21:48:32.581138] image_bind.modality_trunks.audio.blocks.0.norm_1.bias torch.Size([768]) False
[21:48:32.581174] image_bind.modality_trunks.audio.blocks.0.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.581207] image_bind.modality_trunks.audio.blocks.0.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.581242] image_bind.modality_trunks.audio.blocks.0.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.581275] image_bind.modality_trunks.audio.blocks.0.mlp.fc2.bias torch.Size([768]) False
[21:48:32.581310] image_bind.modality_trunks.audio.blocks.0.norm_2.weight torch.Size([768]) False
[21:48:32.581342] image_bind.modality_trunks.audio.blocks.0.norm_2.bias torch.Size([768]) False
[21:48:32.581377] image_bind.modality_trunks.audio.blocks.1.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.581410] image_bind.modality_trunks.audio.blocks.1.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.581443] image_bind.modality_trunks.audio.blocks.1.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.581476] image_bind.modality_trunks.audio.blocks.1.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.581514] image_bind.modality_trunks.audio.blocks.1.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.581547] image_bind.modality_trunks.audio.blocks.1.attn.out_proj.bias torch.Size([768]) False
[21:48:32.581582] image_bind.modality_trunks.audio.blocks.1.norm_1.weight torch.Size([768]) False
[21:48:32.581615] image_bind.modality_trunks.audio.blocks.1.norm_1.bias torch.Size([768]) False
[21:48:32.581649] image_bind.modality_trunks.audio.blocks.1.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.581682] image_bind.modality_trunks.audio.blocks.1.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.581717] image_bind.modality_trunks.audio.blocks.1.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.581755] image_bind.modality_trunks.audio.blocks.1.mlp.fc2.bias torch.Size([768]) False
[21:48:32.581798] image_bind.modality_trunks.audio.blocks.1.norm_2.weight torch.Size([768]) False
[21:48:32.581837] image_bind.modality_trunks.audio.blocks.1.norm_2.bias torch.Size([768]) False
[21:48:32.581879] image_bind.modality_trunks.audio.blocks.2.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.581920] image_bind.modality_trunks.audio.blocks.2.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.581956] image_bind.modality_trunks.audio.blocks.2.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.581988] image_bind.modality_trunks.audio.blocks.2.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.582023] image_bind.modality_trunks.audio.blocks.2.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.582055] image_bind.modality_trunks.audio.blocks.2.attn.out_proj.bias torch.Size([768]) False
[21:48:32.582090] image_bind.modality_trunks.audio.blocks.2.norm_1.weight torch.Size([768]) False
[21:48:32.582123] image_bind.modality_trunks.audio.blocks.2.norm_1.bias torch.Size([768]) False
[21:48:32.582158] image_bind.modality_trunks.audio.blocks.2.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.582191] image_bind.modality_trunks.audio.blocks.2.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.582225] image_bind.modality_trunks.audio.blocks.2.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.582258] image_bind.modality_trunks.audio.blocks.2.mlp.fc2.bias torch.Size([768]) False
[21:48:32.582293] image_bind.modality_trunks.audio.blocks.2.norm_2.weight torch.Size([768]) False
[21:48:32.582325] image_bind.modality_trunks.audio.blocks.2.norm_2.bias torch.Size([768]) False
[21:48:32.582360] image_bind.modality_trunks.audio.blocks.3.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.582393] image_bind.modality_trunks.audio.blocks.3.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.582426] image_bind.modality_trunks.audio.blocks.3.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.582465] image_bind.modality_trunks.audio.blocks.3.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.582514] image_bind.modality_trunks.audio.blocks.3.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.582557] image_bind.modality_trunks.audio.blocks.3.attn.out_proj.bias torch.Size([768]) False
[21:48:32.582601] image_bind.modality_trunks.audio.blocks.3.norm_1.weight torch.Size([768]) False
[21:48:32.582634] image_bind.modality_trunks.audio.blocks.3.norm_1.bias torch.Size([768]) False
[21:48:32.582669] image_bind.modality_trunks.audio.blocks.3.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.582702] image_bind.modality_trunks.audio.blocks.3.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.582737] image_bind.modality_trunks.audio.blocks.3.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.582770] image_bind.modality_trunks.audio.blocks.3.mlp.fc2.bias torch.Size([768]) False
[21:48:32.582805] image_bind.modality_trunks.audio.blocks.3.norm_2.weight torch.Size([768]) False
[21:48:32.582837] image_bind.modality_trunks.audio.blocks.3.norm_2.bias torch.Size([768]) False
[21:48:32.582872] image_bind.modality_trunks.audio.blocks.4.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.582905] image_bind.modality_trunks.audio.blocks.4.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.582937] image_bind.modality_trunks.audio.blocks.4.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.582970] image_bind.modality_trunks.audio.blocks.4.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.583004] image_bind.modality_trunks.audio.blocks.4.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.583037] image_bind.modality_trunks.audio.blocks.4.attn.out_proj.bias torch.Size([768]) False
[21:48:32.583072] image_bind.modality_trunks.audio.blocks.4.norm_1.weight torch.Size([768]) False
[21:48:32.583104] image_bind.modality_trunks.audio.blocks.4.norm_1.bias torch.Size([768]) False
[21:48:32.583145] image_bind.modality_trunks.audio.blocks.4.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.583186] image_bind.modality_trunks.audio.blocks.4.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.583228] image_bind.modality_trunks.audio.blocks.4.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.583269] image_bind.modality_trunks.audio.blocks.4.mlp.fc2.bias torch.Size([768]) False
[21:48:32.583305] image_bind.modality_trunks.audio.blocks.4.norm_2.weight torch.Size([768]) False
[21:48:32.583337] image_bind.modality_trunks.audio.blocks.4.norm_2.bias torch.Size([768]) False
[21:48:32.583373] image_bind.modality_trunks.audio.blocks.5.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.583406] image_bind.modality_trunks.audio.blocks.5.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.583439] image_bind.modality_trunks.audio.blocks.5.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.583472] image_bind.modality_trunks.audio.blocks.5.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.583510] image_bind.modality_trunks.audio.blocks.5.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.583543] image_bind.modality_trunks.audio.blocks.5.attn.out_proj.bias torch.Size([768]) False
[21:48:32.583578] image_bind.modality_trunks.audio.blocks.5.norm_1.weight torch.Size([768]) False
[21:48:32.583610] image_bind.modality_trunks.audio.blocks.5.norm_1.bias torch.Size([768]) False
[21:48:32.583646] image_bind.modality_trunks.audio.blocks.5.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.583678] image_bind.modality_trunks.audio.blocks.5.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.583713] image_bind.modality_trunks.audio.blocks.5.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.583746] image_bind.modality_trunks.audio.blocks.5.mlp.fc2.bias torch.Size([768]) False
[21:48:32.583781] image_bind.modality_trunks.audio.blocks.5.norm_2.weight torch.Size([768]) False
[21:48:32.583823] image_bind.modality_trunks.audio.blocks.5.norm_2.bias torch.Size([768]) False
[21:48:32.583867] image_bind.modality_trunks.audio.blocks.6.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.583900] image_bind.modality_trunks.audio.blocks.6.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.583933] image_bind.modality_trunks.audio.blocks.6.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.583966] image_bind.modality_trunks.audio.blocks.6.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.584001] image_bind.modality_trunks.audio.blocks.6.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.584033] image_bind.modality_trunks.audio.blocks.6.attn.out_proj.bias torch.Size([768]) False
[21:48:32.584068] image_bind.modality_trunks.audio.blocks.6.norm_1.weight torch.Size([768]) False
[21:48:32.584100] image_bind.modality_trunks.audio.blocks.6.norm_1.bias torch.Size([768]) False
[21:48:32.584135] image_bind.modality_trunks.audio.blocks.6.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.584168] image_bind.modality_trunks.audio.blocks.6.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.584203] image_bind.modality_trunks.audio.blocks.6.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.584236] image_bind.modality_trunks.audio.blocks.6.mlp.fc2.bias torch.Size([768]) False
[21:48:32.584271] image_bind.modality_trunks.audio.blocks.6.norm_2.weight torch.Size([768]) False
[21:48:32.584303] image_bind.modality_trunks.audio.blocks.6.norm_2.bias torch.Size([768]) False
[21:48:32.584339] image_bind.modality_trunks.audio.blocks.7.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.584375] image_bind.modality_trunks.audio.blocks.7.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.584415] image_bind.modality_trunks.audio.blocks.7.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.584455] image_bind.modality_trunks.audio.blocks.7.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.584504] image_bind.modality_trunks.audio.blocks.7.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.584546] image_bind.modality_trunks.audio.blocks.7.attn.out_proj.bias torch.Size([768]) False
[21:48:32.584588] image_bind.modality_trunks.audio.blocks.7.norm_1.weight torch.Size([768]) False
[21:48:32.584627] image_bind.modality_trunks.audio.blocks.7.norm_1.bias torch.Size([768]) False
[21:48:32.584669] image_bind.modality_trunks.audio.blocks.7.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.584709] image_bind.modality_trunks.audio.blocks.7.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.584746] image_bind.modality_trunks.audio.blocks.7.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.584779] image_bind.modality_trunks.audio.blocks.7.mlp.fc2.bias torch.Size([768]) False
[21:48:32.584814] image_bind.modality_trunks.audio.blocks.7.norm_2.weight torch.Size([768]) False
[21:48:32.584847] image_bind.modality_trunks.audio.blocks.7.norm_2.bias torch.Size([768]) False
[21:48:32.584882] image_bind.modality_trunks.audio.blocks.8.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.584915] image_bind.modality_trunks.audio.blocks.8.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.584948] image_bind.modality_trunks.audio.blocks.8.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.584980] image_bind.modality_trunks.audio.blocks.8.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.585014] image_bind.modality_trunks.audio.blocks.8.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.585047] image_bind.modality_trunks.audio.blocks.8.attn.out_proj.bias torch.Size([768]) False
[21:48:32.585082] image_bind.modality_trunks.audio.blocks.8.norm_1.weight torch.Size([768]) False
[21:48:32.585115] image_bind.modality_trunks.audio.blocks.8.norm_1.bias torch.Size([768]) False
[21:48:32.585150] image_bind.modality_trunks.audio.blocks.8.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.585183] image_bind.modality_trunks.audio.blocks.8.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.585224] image_bind.modality_trunks.audio.blocks.8.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.585264] image_bind.modality_trunks.audio.blocks.8.mlp.fc2.bias torch.Size([768]) False
[21:48:32.585307] image_bind.modality_trunks.audio.blocks.8.norm_2.weight torch.Size([768]) False
[21:48:32.585346] image_bind.modality_trunks.audio.blocks.8.norm_2.bias torch.Size([768]) False
[21:48:32.585387] image_bind.modality_trunks.audio.blocks.9.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.585429] image_bind.modality_trunks.audio.blocks.9.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.585470] image_bind.modality_trunks.audio.blocks.9.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.585516] image_bind.modality_trunks.audio.blocks.9.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.585557] image_bind.modality_trunks.audio.blocks.9.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.585595] image_bind.modality_trunks.audio.blocks.9.attn.out_proj.bias torch.Size([768]) False
[21:48:32.585630] image_bind.modality_trunks.audio.blocks.9.norm_1.weight torch.Size([768]) False
[21:48:32.585663] image_bind.modality_trunks.audio.blocks.9.norm_1.bias torch.Size([768]) False
[21:48:32.585697] image_bind.modality_trunks.audio.blocks.9.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.585730] image_bind.modality_trunks.audio.blocks.9.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.585765] image_bind.modality_trunks.audio.blocks.9.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.585797] image_bind.modality_trunks.audio.blocks.9.mlp.fc2.bias torch.Size([768]) False
[21:48:32.585832] image_bind.modality_trunks.audio.blocks.9.norm_2.weight torch.Size([768]) False
[21:48:32.585865] image_bind.modality_trunks.audio.blocks.9.norm_2.bias torch.Size([768]) False
[21:48:32.585899] image_bind.modality_trunks.audio.blocks.10.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.585932] image_bind.modality_trunks.audio.blocks.10.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.585964] image_bind.modality_trunks.audio.blocks.10.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.585997] image_bind.modality_trunks.audio.blocks.10.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.586032] image_bind.modality_trunks.audio.blocks.10.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.586065] image_bind.modality_trunks.audio.blocks.10.attn.out_proj.bias torch.Size([768]) False
[21:48:32.586100] image_bind.modality_trunks.audio.blocks.10.norm_1.weight torch.Size([768]) False
[21:48:32.586138] image_bind.modality_trunks.audio.blocks.10.norm_1.bias torch.Size([768]) False
[21:48:32.586182] image_bind.modality_trunks.audio.blocks.10.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.586223] image_bind.modality_trunks.audio.blocks.10.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.586265] image_bind.modality_trunks.audio.blocks.10.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.586304] image_bind.modality_trunks.audio.blocks.10.mlp.fc2.bias torch.Size([768]) False
[21:48:32.586347] image_bind.modality_trunks.audio.blocks.10.norm_2.weight torch.Size([768]) False
[21:48:32.586389] image_bind.modality_trunks.audio.blocks.10.norm_2.bias torch.Size([768]) False
[21:48:32.586425] image_bind.modality_trunks.audio.blocks.11.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.586457] image_bind.modality_trunks.audio.blocks.11.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.586494] image_bind.modality_trunks.audio.blocks.11.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.586527] image_bind.modality_trunks.audio.blocks.11.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.586561] image_bind.modality_trunks.audio.blocks.11.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.586594] image_bind.modality_trunks.audio.blocks.11.attn.out_proj.bias torch.Size([768]) False
[21:48:32.586629] image_bind.modality_trunks.audio.blocks.11.norm_1.weight torch.Size([768]) False
[21:48:32.586662] image_bind.modality_trunks.audio.blocks.11.norm_1.bias torch.Size([768]) False
[21:48:32.586696] image_bind.modality_trunks.audio.blocks.11.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.586729] image_bind.modality_trunks.audio.blocks.11.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.586764] image_bind.modality_trunks.audio.blocks.11.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.586797] image_bind.modality_trunks.audio.blocks.11.mlp.fc2.bias torch.Size([768]) False
[21:48:32.586832] image_bind.modality_trunks.audio.blocks.11.norm_2.weight torch.Size([768]) False
[21:48:32.586866] image_bind.modality_trunks.audio.blocks.11.norm_2.bias torch.Size([768]) False
[21:48:32.586916] image_bind.modality_trunks.depth.blocks.0.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.586949] image_bind.modality_trunks.depth.blocks.0.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.586982] image_bind.modality_trunks.depth.blocks.0.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.587015] image_bind.modality_trunks.depth.blocks.0.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.587049] image_bind.modality_trunks.depth.blocks.0.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.587082] image_bind.modality_trunks.depth.blocks.0.attn.out_proj.bias torch.Size([384]) False
[21:48:32.587118] image_bind.modality_trunks.depth.blocks.0.norm_1.weight torch.Size([384]) False
[21:48:32.587151] image_bind.modality_trunks.depth.blocks.0.norm_1.bias torch.Size([384]) False
[21:48:32.587186] image_bind.modality_trunks.depth.blocks.0.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.587219] image_bind.modality_trunks.depth.blocks.0.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.587253] image_bind.modality_trunks.depth.blocks.0.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.587286] image_bind.modality_trunks.depth.blocks.0.mlp.fc2.bias torch.Size([384]) False
[21:48:32.587325] image_bind.modality_trunks.depth.blocks.0.norm_2.weight torch.Size([384]) False
[21:48:32.587367] image_bind.modality_trunks.depth.blocks.0.norm_2.bias torch.Size([384]) False
[21:48:32.587411] image_bind.modality_trunks.depth.blocks.1.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.587449] image_bind.modality_trunks.depth.blocks.1.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.587482] image_bind.modality_trunks.depth.blocks.1.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.587520] image_bind.modality_trunks.depth.blocks.1.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.587554] image_bind.modality_trunks.depth.blocks.1.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.587587] image_bind.modality_trunks.depth.blocks.1.attn.out_proj.bias torch.Size([384]) False
[21:48:32.587622] image_bind.modality_trunks.depth.blocks.1.norm_1.weight torch.Size([384]) False
[21:48:32.587654] image_bind.modality_trunks.depth.blocks.1.norm_1.bias torch.Size([384]) False
[21:48:32.587689] image_bind.modality_trunks.depth.blocks.1.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.587722] image_bind.modality_trunks.depth.blocks.1.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.587757] image_bind.modality_trunks.depth.blocks.1.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.587790] image_bind.modality_trunks.depth.blocks.1.mlp.fc2.bias torch.Size([384]) False
[21:48:32.587825] image_bind.modality_trunks.depth.blocks.1.norm_2.weight torch.Size([384]) False
[21:48:32.587858] image_bind.modality_trunks.depth.blocks.1.norm_2.bias torch.Size([384]) False
[21:48:32.587896] image_bind.modality_trunks.depth.blocks.2.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.587930] image_bind.modality_trunks.depth.blocks.2.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.587963] image_bind.modality_trunks.depth.blocks.2.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.587996] image_bind.modality_trunks.depth.blocks.2.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.588031] image_bind.modality_trunks.depth.blocks.2.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.588063] image_bind.modality_trunks.depth.blocks.2.attn.out_proj.bias torch.Size([384]) False
[21:48:32.588099] image_bind.modality_trunks.depth.blocks.2.norm_1.weight torch.Size([384]) False
[21:48:32.588131] image_bind.modality_trunks.depth.blocks.2.norm_1.bias torch.Size([384]) False
[21:48:32.588166] image_bind.modality_trunks.depth.blocks.2.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.588199] image_bind.modality_trunks.depth.blocks.2.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.588235] image_bind.modality_trunks.depth.blocks.2.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.588267] image_bind.modality_trunks.depth.blocks.2.mlp.fc2.bias torch.Size([384]) False
[21:48:32.588303] image_bind.modality_trunks.depth.blocks.2.norm_2.weight torch.Size([384]) False
[21:48:32.588337] image_bind.modality_trunks.depth.blocks.2.norm_2.bias torch.Size([384]) False
[21:48:32.588373] image_bind.modality_trunks.depth.blocks.3.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.588406] image_bind.modality_trunks.depth.blocks.3.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.588439] image_bind.modality_trunks.depth.blocks.3.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.588472] image_bind.modality_trunks.depth.blocks.3.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.588512] image_bind.modality_trunks.depth.blocks.3.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.588544] image_bind.modality_trunks.depth.blocks.3.attn.out_proj.bias torch.Size([384]) False
[21:48:32.588579] image_bind.modality_trunks.depth.blocks.3.norm_1.weight torch.Size([384]) False
[21:48:32.588612] image_bind.modality_trunks.depth.blocks.3.norm_1.bias torch.Size([384]) False
[21:48:32.588647] image_bind.modality_trunks.depth.blocks.3.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.588679] image_bind.modality_trunks.depth.blocks.3.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.588715] image_bind.modality_trunks.depth.blocks.3.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.588748] image_bind.modality_trunks.depth.blocks.3.mlp.fc2.bias torch.Size([384]) False
[21:48:32.588783] image_bind.modality_trunks.depth.blocks.3.norm_2.weight torch.Size([384]) False
[21:48:32.588822] image_bind.modality_trunks.depth.blocks.3.norm_2.bias torch.Size([384]) False
[21:48:32.588865] image_bind.modality_trunks.depth.blocks.4.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.588905] image_bind.modality_trunks.depth.blocks.4.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.588945] image_bind.modality_trunks.depth.blocks.4.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.588987] image_bind.modality_trunks.depth.blocks.4.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.589030] image_bind.modality_trunks.depth.blocks.4.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.589070] image_bind.modality_trunks.depth.blocks.4.attn.out_proj.bias torch.Size([384]) False
[21:48:32.589112] image_bind.modality_trunks.depth.blocks.4.norm_1.weight torch.Size([384]) False
[21:48:32.589144] image_bind.modality_trunks.depth.blocks.4.norm_1.bias torch.Size([384]) False
[21:48:32.589179] image_bind.modality_trunks.depth.blocks.4.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.589212] image_bind.modality_trunks.depth.blocks.4.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.589247] image_bind.modality_trunks.depth.blocks.4.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.589280] image_bind.modality_trunks.depth.blocks.4.mlp.fc2.bias torch.Size([384]) False
[21:48:32.589315] image_bind.modality_trunks.depth.blocks.4.norm_2.weight torch.Size([384]) False
[21:48:32.589348] image_bind.modality_trunks.depth.blocks.4.norm_2.bias torch.Size([384]) False
[21:48:32.589383] image_bind.modality_trunks.depth.blocks.5.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.589417] image_bind.modality_trunks.depth.blocks.5.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.589449] image_bind.modality_trunks.depth.blocks.5.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.589482] image_bind.modality_trunks.depth.blocks.5.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.589521] image_bind.modality_trunks.depth.blocks.5.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.589554] image_bind.modality_trunks.depth.blocks.5.attn.out_proj.bias torch.Size([384]) False
[21:48:32.589589] image_bind.modality_trunks.depth.blocks.5.norm_1.weight torch.Size([384]) False
[21:48:32.589621] image_bind.modality_trunks.depth.blocks.5.norm_1.bias torch.Size([384]) False
[21:48:32.589656] image_bind.modality_trunks.depth.blocks.5.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.589689] image_bind.modality_trunks.depth.blocks.5.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.589733] image_bind.modality_trunks.depth.blocks.5.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.589774] image_bind.modality_trunks.depth.blocks.5.mlp.fc2.bias torch.Size([384]) False
[21:48:32.589818] image_bind.modality_trunks.depth.blocks.5.norm_2.weight torch.Size([384]) False
[21:48:32.589859] image_bind.modality_trunks.depth.blocks.5.norm_2.bias torch.Size([384]) False
[21:48:32.589901] image_bind.modality_trunks.depth.blocks.6.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.589941] image_bind.modality_trunks.depth.blocks.6.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.589980] image_bind.modality_trunks.depth.blocks.6.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.590013] image_bind.modality_trunks.depth.blocks.6.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.590048] image_bind.modality_trunks.depth.blocks.6.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.590081] image_bind.modality_trunks.depth.blocks.6.attn.out_proj.bias torch.Size([384]) False
[21:48:32.590116] image_bind.modality_trunks.depth.blocks.6.norm_1.weight torch.Size([384]) False
[21:48:32.590148] image_bind.modality_trunks.depth.blocks.6.norm_1.bias torch.Size([384]) False
[21:48:32.590183] image_bind.modality_trunks.depth.blocks.6.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.590216] image_bind.modality_trunks.depth.blocks.6.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.590251] image_bind.modality_trunks.depth.blocks.6.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.590284] image_bind.modality_trunks.depth.blocks.6.mlp.fc2.bias torch.Size([384]) False
[21:48:32.590319] image_bind.modality_trunks.depth.blocks.6.norm_2.weight torch.Size([384]) False
[21:48:32.590351] image_bind.modality_trunks.depth.blocks.6.norm_2.bias torch.Size([384]) False
[21:48:32.590387] image_bind.modality_trunks.depth.blocks.7.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.590420] image_bind.modality_trunks.depth.blocks.7.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.590452] image_bind.modality_trunks.depth.blocks.7.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.590490] image_bind.modality_trunks.depth.blocks.7.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.590525] image_bind.modality_trunks.depth.blocks.7.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.590558] image_bind.modality_trunks.depth.blocks.7.attn.out_proj.bias torch.Size([384]) False
[21:48:32.590592] image_bind.modality_trunks.depth.blocks.7.norm_1.weight torch.Size([384]) False
[21:48:32.590625] image_bind.modality_trunks.depth.blocks.7.norm_1.bias torch.Size([384]) False
[21:48:32.590660] image_bind.modality_trunks.depth.blocks.7.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.590693] image_bind.modality_trunks.depth.blocks.7.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.590728] image_bind.modality_trunks.depth.blocks.7.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.590760] image_bind.modality_trunks.depth.blocks.7.mlp.fc2.bias torch.Size([384]) False
[21:48:32.590795] image_bind.modality_trunks.depth.blocks.7.norm_2.weight torch.Size([384]) False
[21:48:32.590828] image_bind.modality_trunks.depth.blocks.7.norm_2.bias torch.Size([384]) False
[21:48:32.590869] image_bind.modality_trunks.depth.blocks.8.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.590909] image_bind.modality_trunks.depth.blocks.8.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.590950] image_bind.modality_trunks.depth.blocks.8.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.590991] image_bind.modality_trunks.depth.blocks.8.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.591033] image_bind.modality_trunks.depth.blocks.8.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.591073] image_bind.modality_trunks.depth.blocks.8.attn.out_proj.bias torch.Size([384]) False
[21:48:32.591115] image_bind.modality_trunks.depth.blocks.8.norm_1.weight torch.Size([384]) False
[21:48:32.591153] image_bind.modality_trunks.depth.blocks.8.norm_1.bias torch.Size([384]) False
[21:48:32.591197] image_bind.modality_trunks.depth.blocks.8.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.591237] image_bind.modality_trunks.depth.blocks.8.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.591280] image_bind.modality_trunks.depth.blocks.8.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.591321] image_bind.modality_trunks.depth.blocks.8.mlp.fc2.bias torch.Size([384]) False
[21:48:32.591365] image_bind.modality_trunks.depth.blocks.8.norm_2.weight torch.Size([384]) False
[21:48:32.591402] image_bind.modality_trunks.depth.blocks.8.norm_2.bias torch.Size([384]) False
[21:48:32.591438] image_bind.modality_trunks.depth.blocks.9.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.591471] image_bind.modality_trunks.depth.blocks.9.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.591510] image_bind.modality_trunks.depth.blocks.9.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.591543] image_bind.modality_trunks.depth.blocks.9.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.591577] image_bind.modality_trunks.depth.blocks.9.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.591610] image_bind.modality_trunks.depth.blocks.9.attn.out_proj.bias torch.Size([384]) False
[21:48:32.591645] image_bind.modality_trunks.depth.blocks.9.norm_1.weight torch.Size([384]) False
[21:48:32.591677] image_bind.modality_trunks.depth.blocks.9.norm_1.bias torch.Size([384]) False
[21:48:32.591712] image_bind.modality_trunks.depth.blocks.9.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.591745] image_bind.modality_trunks.depth.blocks.9.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.591779] image_bind.modality_trunks.depth.blocks.9.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.591812] image_bind.modality_trunks.depth.blocks.9.mlp.fc2.bias torch.Size([384]) False
[21:48:32.591846] image_bind.modality_trunks.depth.blocks.9.norm_2.weight torch.Size([384]) False
[21:48:32.591879] image_bind.modality_trunks.depth.blocks.9.norm_2.bias torch.Size([384]) False
[21:48:32.591914] image_bind.modality_trunks.depth.blocks.10.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.591947] image_bind.modality_trunks.depth.blocks.10.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.591986] image_bind.modality_trunks.depth.blocks.10.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.592025] image_bind.modality_trunks.depth.blocks.10.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.592066] image_bind.modality_trunks.depth.blocks.10.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.592107] image_bind.modality_trunks.depth.blocks.10.attn.out_proj.bias torch.Size([384]) False
[21:48:32.592153] image_bind.modality_trunks.depth.blocks.10.norm_1.weight torch.Size([384]) False
[21:48:32.592196] image_bind.modality_trunks.depth.blocks.10.norm_1.bias torch.Size([384]) False
[21:48:32.592240] image_bind.modality_trunks.depth.blocks.10.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.592282] image_bind.modality_trunks.depth.blocks.10.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.592324] image_bind.modality_trunks.depth.blocks.10.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.592358] image_bind.modality_trunks.depth.blocks.10.mlp.fc2.bias torch.Size([384]) False
[21:48:32.592394] image_bind.modality_trunks.depth.blocks.10.norm_2.weight torch.Size([384]) False
[21:48:32.592427] image_bind.modality_trunks.depth.blocks.10.norm_2.bias torch.Size([384]) False
[21:48:32.592463] image_bind.modality_trunks.depth.blocks.11.attn.in_proj_weight torch.Size([1152, 384]) False
[21:48:32.592499] image_bind.modality_trunks.depth.blocks.11.attn.in_proj_bias torch.Size([1152]) False
[21:48:32.592532] image_bind.modality_trunks.depth.blocks.11.attn.bias_k torch.Size([1, 1, 384]) False
[21:48:32.592564] image_bind.modality_trunks.depth.blocks.11.attn.bias_v torch.Size([1, 1, 384]) False
[21:48:32.592599] image_bind.modality_trunks.depth.blocks.11.attn.out_proj.weight torch.Size([384, 384]) False
[21:48:32.592632] image_bind.modality_trunks.depth.blocks.11.attn.out_proj.bias torch.Size([384]) False
[21:48:32.592667] image_bind.modality_trunks.depth.blocks.11.norm_1.weight torch.Size([384]) False
[21:48:32.592701] image_bind.modality_trunks.depth.blocks.11.norm_1.bias torch.Size([384]) False
[21:48:32.592745] image_bind.modality_trunks.depth.blocks.11.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.592785] image_bind.modality_trunks.depth.blocks.11.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.592829] image_bind.modality_trunks.depth.blocks.11.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.592871] image_bind.modality_trunks.depth.blocks.11.mlp.fc2.bias torch.Size([384]) False
[21:48:32.592915] image_bind.modality_trunks.depth.blocks.11.norm_2.weight torch.Size([384]) False
[21:48:32.592956] image_bind.modality_trunks.depth.blocks.11.norm_2.bias torch.Size([384]) False
[21:48:32.593006] image_bind.modality_trunks.thermal.blocks.0.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.593039] image_bind.modality_trunks.thermal.blocks.0.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.593072] image_bind.modality_trunks.thermal.blocks.0.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.593105] image_bind.modality_trunks.thermal.blocks.0.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.593139] image_bind.modality_trunks.thermal.blocks.0.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.593172] image_bind.modality_trunks.thermal.blocks.0.attn.out_proj.bias torch.Size([768]) False
[21:48:32.593208] image_bind.modality_trunks.thermal.blocks.0.norm_1.weight torch.Size([768]) False
[21:48:32.593240] image_bind.modality_trunks.thermal.blocks.0.norm_1.bias torch.Size([768]) False
[21:48:32.593275] image_bind.modality_trunks.thermal.blocks.0.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.593308] image_bind.modality_trunks.thermal.blocks.0.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.593343] image_bind.modality_trunks.thermal.blocks.0.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.593376] image_bind.modality_trunks.thermal.blocks.0.mlp.fc2.bias torch.Size([768]) False
[21:48:32.593411] image_bind.modality_trunks.thermal.blocks.0.norm_2.weight torch.Size([768]) False
[21:48:32.593444] image_bind.modality_trunks.thermal.blocks.0.norm_2.bias torch.Size([768]) False
[21:48:32.593493] image_bind.modality_trunks.thermal.blocks.1.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.593532] image_bind.modality_trunks.thermal.blocks.1.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.593572] image_bind.modality_trunks.thermal.blocks.1.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.593614] image_bind.modality_trunks.thermal.blocks.1.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.593657] image_bind.modality_trunks.thermal.blocks.1.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.593699] image_bind.modality_trunks.thermal.blocks.1.attn.out_proj.bias torch.Size([768]) False
[21:48:32.593744] image_bind.modality_trunks.thermal.blocks.1.norm_1.weight torch.Size([768]) False
[21:48:32.593785] image_bind.modality_trunks.thermal.blocks.1.norm_1.bias torch.Size([768]) False
[21:48:32.593821] image_bind.modality_trunks.thermal.blocks.1.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.593854] image_bind.modality_trunks.thermal.blocks.1.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.593889] image_bind.modality_trunks.thermal.blocks.1.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.593922] image_bind.modality_trunks.thermal.blocks.1.mlp.fc2.bias torch.Size([768]) False
[21:48:32.593957] image_bind.modality_trunks.thermal.blocks.1.norm_2.weight torch.Size([768]) False
[21:48:32.593989] image_bind.modality_trunks.thermal.blocks.1.norm_2.bias torch.Size([768]) False
[21:48:32.594025] image_bind.modality_trunks.thermal.blocks.2.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.594058] image_bind.modality_trunks.thermal.blocks.2.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.594090] image_bind.modality_trunks.thermal.blocks.2.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.594123] image_bind.modality_trunks.thermal.blocks.2.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.594157] image_bind.modality_trunks.thermal.blocks.2.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.594190] image_bind.modality_trunks.thermal.blocks.2.attn.out_proj.bias torch.Size([768]) False
[21:48:32.594225] image_bind.modality_trunks.thermal.blocks.2.norm_1.weight torch.Size([768]) False
[21:48:32.594257] image_bind.modality_trunks.thermal.blocks.2.norm_1.bias torch.Size([768]) False
[21:48:32.594296] image_bind.modality_trunks.thermal.blocks.2.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.594338] image_bind.modality_trunks.thermal.blocks.2.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.594380] image_bind.modality_trunks.thermal.blocks.2.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.594420] image_bind.modality_trunks.thermal.blocks.2.mlp.fc2.bias torch.Size([768]) False
[21:48:32.594461] image_bind.modality_trunks.thermal.blocks.2.norm_2.weight torch.Size([768]) False
[21:48:32.594505] image_bind.modality_trunks.thermal.blocks.2.norm_2.bias torch.Size([768]) False
[21:48:32.594550] image_bind.modality_trunks.thermal.blocks.3.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.594593] image_bind.modality_trunks.thermal.blocks.3.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.594626] image_bind.modality_trunks.thermal.blocks.3.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.594659] image_bind.modality_trunks.thermal.blocks.3.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.594693] image_bind.modality_trunks.thermal.blocks.3.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.594726] image_bind.modality_trunks.thermal.blocks.3.attn.out_proj.bias torch.Size([768]) False
[21:48:32.594761] image_bind.modality_trunks.thermal.blocks.3.norm_1.weight torch.Size([768]) False
[21:48:32.594793] image_bind.modality_trunks.thermal.blocks.3.norm_1.bias torch.Size([768]) False
[21:48:32.594828] image_bind.modality_trunks.thermal.blocks.3.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.594860] image_bind.modality_trunks.thermal.blocks.3.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.594895] image_bind.modality_trunks.thermal.blocks.3.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.594927] image_bind.modality_trunks.thermal.blocks.3.mlp.fc2.bias torch.Size([768]) False
[21:48:32.594962] image_bind.modality_trunks.thermal.blocks.3.norm_2.weight torch.Size([768]) False
[21:48:32.594995] image_bind.modality_trunks.thermal.blocks.3.norm_2.bias torch.Size([768]) False
[21:48:32.595030] image_bind.modality_trunks.thermal.blocks.4.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.595062] image_bind.modality_trunks.thermal.blocks.4.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.595095] image_bind.modality_trunks.thermal.blocks.4.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.595137] image_bind.modality_trunks.thermal.blocks.4.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.595179] image_bind.modality_trunks.thermal.blocks.4.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.595212] image_bind.modality_trunks.thermal.blocks.4.attn.out_proj.bias torch.Size([768]) False
[21:48:32.595248] image_bind.modality_trunks.thermal.blocks.4.norm_1.weight torch.Size([768]) False
[21:48:32.595280] image_bind.modality_trunks.thermal.blocks.4.norm_1.bias torch.Size([768]) False
[21:48:32.595315] image_bind.modality_trunks.thermal.blocks.4.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.595348] image_bind.modality_trunks.thermal.blocks.4.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.595383] image_bind.modality_trunks.thermal.blocks.4.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.595415] image_bind.modality_trunks.thermal.blocks.4.mlp.fc2.bias torch.Size([768]) False
[21:48:32.595451] image_bind.modality_trunks.thermal.blocks.4.norm_2.weight torch.Size([768]) False
[21:48:32.595483] image_bind.modality_trunks.thermal.blocks.4.norm_2.bias torch.Size([768]) False
[21:48:32.595523] image_bind.modality_trunks.thermal.blocks.5.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.595556] image_bind.modality_trunks.thermal.blocks.5.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.595589] image_bind.modality_trunks.thermal.blocks.5.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.595630] image_bind.modality_trunks.thermal.blocks.5.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.595673] image_bind.modality_trunks.thermal.blocks.5.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.595714] image_bind.modality_trunks.thermal.blocks.5.attn.out_proj.bias torch.Size([768]) False
[21:48:32.595756] image_bind.modality_trunks.thermal.blocks.5.norm_1.weight torch.Size([768]) False
[21:48:32.595796] image_bind.modality_trunks.thermal.blocks.5.norm_1.bias torch.Size([768]) False
[21:48:32.595838] image_bind.modality_trunks.thermal.blocks.5.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.595877] image_bind.modality_trunks.thermal.blocks.5.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.595919] image_bind.modality_trunks.thermal.blocks.5.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.595959] image_bind.modality_trunks.thermal.blocks.5.mlp.fc2.bias torch.Size([768]) False
[21:48:32.596002] image_bind.modality_trunks.thermal.blocks.5.norm_2.weight torch.Size([768]) False
[21:48:32.596043] image_bind.modality_trunks.thermal.blocks.5.norm_2.bias torch.Size([768]) False
[21:48:32.596087] image_bind.modality_trunks.thermal.blocks.6.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.596128] image_bind.modality_trunks.thermal.blocks.6.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.596168] image_bind.modality_trunks.thermal.blocks.6.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.596207] image_bind.modality_trunks.thermal.blocks.6.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.596250] image_bind.modality_trunks.thermal.blocks.6.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.596292] image_bind.modality_trunks.thermal.blocks.6.attn.out_proj.bias torch.Size([768]) False
[21:48:32.596336] image_bind.modality_trunks.thermal.blocks.6.norm_1.weight torch.Size([768]) False
[21:48:32.596376] image_bind.modality_trunks.thermal.blocks.6.norm_1.bias torch.Size([768]) False
[21:48:32.596419] image_bind.modality_trunks.thermal.blocks.6.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.596459] image_bind.modality_trunks.thermal.blocks.6.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.596504] image_bind.modality_trunks.thermal.blocks.6.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.596537] image_bind.modality_trunks.thermal.blocks.6.mlp.fc2.bias torch.Size([768]) False
[21:48:32.596572] image_bind.modality_trunks.thermal.blocks.6.norm_2.weight torch.Size([768]) False
[21:48:32.596606] image_bind.modality_trunks.thermal.blocks.6.norm_2.bias torch.Size([768]) False
[21:48:32.596641] image_bind.modality_trunks.thermal.blocks.7.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.596674] image_bind.modality_trunks.thermal.blocks.7.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.596707] image_bind.modality_trunks.thermal.blocks.7.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.596740] image_bind.modality_trunks.thermal.blocks.7.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.596774] image_bind.modality_trunks.thermal.blocks.7.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.596807] image_bind.modality_trunks.thermal.blocks.7.attn.out_proj.bias torch.Size([768]) False
[21:48:32.596842] image_bind.modality_trunks.thermal.blocks.7.norm_1.weight torch.Size([768]) False
[21:48:32.596874] image_bind.modality_trunks.thermal.blocks.7.norm_1.bias torch.Size([768]) False
[21:48:32.596909] image_bind.modality_trunks.thermal.blocks.7.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.596948] image_bind.modality_trunks.thermal.blocks.7.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.596992] image_bind.modality_trunks.thermal.blocks.7.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.597034] image_bind.modality_trunks.thermal.blocks.7.mlp.fc2.bias torch.Size([768]) False
[21:48:32.597074] image_bind.modality_trunks.thermal.blocks.7.norm_2.weight torch.Size([768]) False
[21:48:32.597107] image_bind.modality_trunks.thermal.blocks.7.norm_2.bias torch.Size([768]) False
[21:48:32.597142] image_bind.modality_trunks.thermal.blocks.8.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.597175] image_bind.modality_trunks.thermal.blocks.8.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.597208] image_bind.modality_trunks.thermal.blocks.8.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.597240] image_bind.modality_trunks.thermal.blocks.8.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.597274] image_bind.modality_trunks.thermal.blocks.8.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.597307] image_bind.modality_trunks.thermal.blocks.8.attn.out_proj.bias torch.Size([768]) False
[21:48:32.597342] image_bind.modality_trunks.thermal.blocks.8.norm_1.weight torch.Size([768]) False
[21:48:32.597374] image_bind.modality_trunks.thermal.blocks.8.norm_1.bias torch.Size([768]) False
[21:48:32.597409] image_bind.modality_trunks.thermal.blocks.8.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.597442] image_bind.modality_trunks.thermal.blocks.8.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.597477] image_bind.modality_trunks.thermal.blocks.8.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.597515] image_bind.modality_trunks.thermal.blocks.8.mlp.fc2.bias torch.Size([768]) False
[21:48:32.597558] image_bind.modality_trunks.thermal.blocks.8.norm_2.weight torch.Size([768]) False
[21:48:32.597599] image_bind.modality_trunks.thermal.blocks.8.norm_2.bias torch.Size([768]) False
[21:48:32.597638] image_bind.modality_trunks.thermal.blocks.9.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.597671] image_bind.modality_trunks.thermal.blocks.9.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.597704] image_bind.modality_trunks.thermal.blocks.9.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.597737] image_bind.modality_trunks.thermal.blocks.9.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.597771] image_bind.modality_trunks.thermal.blocks.9.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.597804] image_bind.modality_trunks.thermal.blocks.9.attn.out_proj.bias torch.Size([768]) False
[21:48:32.597839] image_bind.modality_trunks.thermal.blocks.9.norm_1.weight torch.Size([768]) False
[21:48:32.597872] image_bind.modality_trunks.thermal.blocks.9.norm_1.bias torch.Size([768]) False
[21:48:32.597907] image_bind.modality_trunks.thermal.blocks.9.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.597939] image_bind.modality_trunks.thermal.blocks.9.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.597975] image_bind.modality_trunks.thermal.blocks.9.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.598007] image_bind.modality_trunks.thermal.blocks.9.mlp.fc2.bias torch.Size([768]) False
[21:48:32.598043] image_bind.modality_trunks.thermal.blocks.9.norm_2.weight torch.Size([768]) False
[21:48:32.598083] image_bind.modality_trunks.thermal.blocks.9.norm_2.bias torch.Size([768]) False
[21:48:32.598128] image_bind.modality_trunks.thermal.blocks.10.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.598170] image_bind.modality_trunks.thermal.blocks.10.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.598211] image_bind.modality_trunks.thermal.blocks.10.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.598251] image_bind.modality_trunks.thermal.blocks.10.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.598293] image_bind.modality_trunks.thermal.blocks.10.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.598330] image_bind.modality_trunks.thermal.blocks.10.attn.out_proj.bias torch.Size([768]) False
[21:48:32.598365] image_bind.modality_trunks.thermal.blocks.10.norm_1.weight torch.Size([768]) False
[21:48:32.598397] image_bind.modality_trunks.thermal.blocks.10.norm_1.bias torch.Size([768]) False
[21:48:32.598433] image_bind.modality_trunks.thermal.blocks.10.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.598465] image_bind.modality_trunks.thermal.blocks.10.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.598505] image_bind.modality_trunks.thermal.blocks.10.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.598538] image_bind.modality_trunks.thermal.blocks.10.mlp.fc2.bias torch.Size([768]) False
[21:48:32.598574] image_bind.modality_trunks.thermal.blocks.10.norm_2.weight torch.Size([768]) False
[21:48:32.598606] image_bind.modality_trunks.thermal.blocks.10.norm_2.bias torch.Size([768]) False
[21:48:32.598641] image_bind.modality_trunks.thermal.blocks.11.attn.in_proj_weight torch.Size([2304, 768]) False
[21:48:32.598675] image_bind.modality_trunks.thermal.blocks.11.attn.in_proj_bias torch.Size([2304]) False
[21:48:32.598707] image_bind.modality_trunks.thermal.blocks.11.attn.bias_k torch.Size([1, 1, 768]) False
[21:48:32.598740] image_bind.modality_trunks.thermal.blocks.11.attn.bias_v torch.Size([1, 1, 768]) False
[21:48:32.598782] image_bind.modality_trunks.thermal.blocks.11.attn.out_proj.weight torch.Size([768, 768]) False
[21:48:32.598821] image_bind.modality_trunks.thermal.blocks.11.attn.out_proj.bias torch.Size([768]) False
[21:48:32.598862] image_bind.modality_trunks.thermal.blocks.11.norm_1.weight torch.Size([768]) False
[21:48:32.598897] image_bind.modality_trunks.thermal.blocks.11.norm_1.bias torch.Size([768]) False
[21:48:32.598932] image_bind.modality_trunks.thermal.blocks.11.mlp.fc1.weight torch.Size([3072, 768]) False
[21:48:32.598965] image_bind.modality_trunks.thermal.blocks.11.mlp.fc1.bias torch.Size([3072]) False
[21:48:32.599001] image_bind.modality_trunks.thermal.blocks.11.mlp.fc2.weight torch.Size([768, 3072]) False
[21:48:32.599033] image_bind.modality_trunks.thermal.blocks.11.mlp.fc2.bias torch.Size([768]) False
[21:48:32.599069] image_bind.modality_trunks.thermal.blocks.11.norm_2.weight torch.Size([768]) False
[21:48:32.599101] image_bind.modality_trunks.thermal.blocks.11.norm_2.bias torch.Size([768]) False
[21:48:32.599145] image_bind.modality_trunks.imu.blocks.0.attn.in_proj_weight torch.Size([1536, 512]) False
[21:48:32.599178] image_bind.modality_trunks.imu.blocks.0.attn.in_proj_bias torch.Size([1536]) False
[21:48:32.599211] image_bind.modality_trunks.imu.blocks.0.attn.bias_k torch.Size([1, 1, 512]) False
[21:48:32.599243] image_bind.modality_trunks.imu.blocks.0.attn.bias_v torch.Size([1, 1, 512]) False
[21:48:32.599278] image_bind.modality_trunks.imu.blocks.0.attn.out_proj.weight torch.Size([512, 512]) False
[21:48:32.599311] image_bind.modality_trunks.imu.blocks.0.attn.out_proj.bias torch.Size([512]) False
[21:48:32.599346] image_bind.modality_trunks.imu.blocks.0.norm_1.weight torch.Size([512]) False
[21:48:32.599384] image_bind.modality_trunks.imu.blocks.0.norm_1.bias torch.Size([512]) False
[21:48:32.599420] image_bind.modality_trunks.imu.blocks.0.mlp.fc1.weight torch.Size([2048, 512]) False
[21:48:32.599453] image_bind.modality_trunks.imu.blocks.0.mlp.fc1.bias torch.Size([2048]) False
[21:48:32.599494] image_bind.modality_trunks.imu.blocks.0.mlp.fc2.weight torch.Size([512, 2048]) False
[21:48:32.599527] image_bind.modality_trunks.imu.blocks.0.mlp.fc2.bias torch.Size([512]) False
[21:48:32.599562] image_bind.modality_trunks.imu.blocks.0.norm_2.weight torch.Size([512]) False
[21:48:32.599594] image_bind.modality_trunks.imu.blocks.0.norm_2.bias torch.Size([512]) False
[21:48:32.599629] image_bind.modality_trunks.imu.blocks.1.attn.in_proj_weight torch.Size([1536, 512]) False
[21:48:32.599662] image_bind.modality_trunks.imu.blocks.1.attn.in_proj_bias torch.Size([1536]) False
[21:48:32.599694] image_bind.modality_trunks.imu.blocks.1.attn.bias_k torch.Size([1, 1, 512]) False
[21:48:32.599727] image_bind.modality_trunks.imu.blocks.1.attn.bias_v torch.Size([1, 1, 512]) False
[21:48:32.599761] image_bind.modality_trunks.imu.blocks.1.attn.out_proj.weight torch.Size([512, 512]) False
[21:48:32.599793] image_bind.modality_trunks.imu.blocks.1.attn.out_proj.bias torch.Size([512]) False
[21:48:32.599829] image_bind.modality_trunks.imu.blocks.1.norm_1.weight torch.Size([512]) False
[21:48:32.599861] image_bind.modality_trunks.imu.blocks.1.norm_1.bias torch.Size([512]) False
[21:48:32.599896] image_bind.modality_trunks.imu.blocks.1.mlp.fc1.weight torch.Size([2048, 512]) False
[21:48:32.599929] image_bind.modality_trunks.imu.blocks.1.mlp.fc1.bias torch.Size([2048]) False
[21:48:32.599964] image_bind.modality_trunks.imu.blocks.1.mlp.fc2.weight torch.Size([512, 2048]) False
[21:48:32.600026] image_bind.modality_trunks.imu.blocks.1.mlp.fc2.bias torch.Size([512]) False
[21:48:32.600063] image_bind.modality_trunks.imu.blocks.1.norm_2.weight torch.Size([512]) False
[21:48:32.600096] image_bind.modality_trunks.imu.blocks.1.norm_2.bias torch.Size([512]) False
[21:48:32.600131] image_bind.modality_trunks.imu.blocks.2.attn.in_proj_weight torch.Size([1536, 512]) False
[21:48:32.600164] image_bind.modality_trunks.imu.blocks.2.attn.in_proj_bias torch.Size([1536]) False
[21:48:32.600197] image_bind.modality_trunks.imu.blocks.2.attn.bias_k torch.Size([1, 1, 512]) False
[21:48:32.600230] image_bind.modality_trunks.imu.blocks.2.attn.bias_v torch.Size([1, 1, 512]) False
[21:48:32.600264] image_bind.modality_trunks.imu.blocks.2.attn.out_proj.weight torch.Size([512, 512]) False
[21:48:32.600297] image_bind.modality_trunks.imu.blocks.2.attn.out_proj.bias torch.Size([512]) False
[21:48:32.600332] image_bind.modality_trunks.imu.blocks.2.norm_1.weight torch.Size([512]) False
[21:48:32.600365] image_bind.modality_trunks.imu.blocks.2.norm_1.bias torch.Size([512]) False
[21:48:32.600400] image_bind.modality_trunks.imu.blocks.2.mlp.fc1.weight torch.Size([2048, 512]) False
[21:48:32.600432] image_bind.modality_trunks.imu.blocks.2.mlp.fc1.bias torch.Size([2048]) False
[21:48:32.600467] image_bind.modality_trunks.imu.blocks.2.mlp.fc2.weight torch.Size([512, 2048]) False
[21:48:32.600504] image_bind.modality_trunks.imu.blocks.2.mlp.fc2.bias torch.Size([512]) False
[21:48:32.600540] image_bind.modality_trunks.imu.blocks.2.norm_2.weight torch.Size([512]) False
[21:48:32.600573] image_bind.modality_trunks.imu.blocks.2.norm_2.bias torch.Size([512]) False
[21:48:32.600608] image_bind.modality_trunks.imu.blocks.3.attn.in_proj_weight torch.Size([1536, 512]) False
[21:48:32.600640] image_bind.modality_trunks.imu.blocks.3.attn.in_proj_bias torch.Size([1536]) False
[21:48:32.600673] image_bind.modality_trunks.imu.blocks.3.attn.bias_k torch.Size([1, 1, 512]) False
[21:48:32.600706] image_bind.modality_trunks.imu.blocks.3.attn.bias_v torch.Size([1, 1, 512]) False
[21:48:32.600740] image_bind.modality_trunks.imu.blocks.3.attn.out_proj.weight torch.Size([512, 512]) False
[21:48:32.600773] image_bind.modality_trunks.imu.blocks.3.attn.out_proj.bias torch.Size([512]) False
[21:48:32.600814] image_bind.modality_trunks.imu.blocks.3.norm_1.weight torch.Size([512]) False
[21:48:32.600847] image_bind.modality_trunks.imu.blocks.3.norm_1.bias torch.Size([512]) False
[21:48:32.600882] image_bind.modality_trunks.imu.blocks.3.mlp.fc1.weight torch.Size([2048, 512]) False
[21:48:32.600915] image_bind.modality_trunks.imu.blocks.3.mlp.fc1.bias torch.Size([2048]) False
[21:48:32.600950] image_bind.modality_trunks.imu.blocks.3.mlp.fc2.weight torch.Size([512, 2048]) False
[21:48:32.600982] image_bind.modality_trunks.imu.blocks.3.mlp.fc2.bias torch.Size([512]) False
[21:48:32.601017] image_bind.modality_trunks.imu.blocks.3.norm_2.weight torch.Size([512]) False
[21:48:32.601050] image_bind.modality_trunks.imu.blocks.3.norm_2.bias torch.Size([512]) False
[21:48:32.601085] image_bind.modality_trunks.imu.blocks.4.attn.in_proj_weight torch.Size([1536, 512]) False
[21:48:32.601118] image_bind.modality_trunks.imu.blocks.4.attn.in_proj_bias torch.Size([1536]) False
[21:48:32.601150] image_bind.modality_trunks.imu.blocks.4.attn.bias_k torch.Size([1, 1, 512]) False
[21:48:32.601183] image_bind.modality_trunks.imu.blocks.4.attn.bias_v torch.Size([1, 1, 512]) False
[21:48:32.601217] image_bind.modality_trunks.imu.blocks.4.attn.out_proj.weight torch.Size([512, 512]) False
[21:48:32.601250] image_bind.modality_trunks.imu.blocks.4.attn.out_proj.bias torch.Size([512]) False
[21:48:32.601285] image_bind.modality_trunks.imu.blocks.4.norm_1.weight torch.Size([512]) False
[21:48:32.601317] image_bind.modality_trunks.imu.blocks.4.norm_1.bias torch.Size([512]) False
[21:48:32.601352] image_bind.modality_trunks.imu.blocks.4.mlp.fc1.weight torch.Size([2048, 512]) False
[21:48:32.601385] image_bind.modality_trunks.imu.blocks.4.mlp.fc1.bias torch.Size([2048]) False
[21:48:32.601425] image_bind.modality_trunks.imu.blocks.4.mlp.fc2.weight torch.Size([512, 2048]) False
[21:48:32.601458] image_bind.modality_trunks.imu.blocks.4.mlp.fc2.bias torch.Size([512]) False
[21:48:32.601497] image_bind.modality_trunks.imu.blocks.4.norm_2.weight torch.Size([512]) False
[21:48:32.601530] image_bind.modality_trunks.imu.blocks.4.norm_2.bias torch.Size([512]) False
[21:48:32.601566] image_bind.modality_trunks.imu.blocks.5.attn.in_proj_weight torch.Size([1536, 512]) False
[21:48:32.601599] image_bind.modality_trunks.imu.blocks.5.attn.in_proj_bias torch.Size([1536]) False
[21:48:32.601631] image_bind.modality_trunks.imu.blocks.5.attn.bias_k torch.Size([1, 1, 512]) False
[21:48:32.601664] image_bind.modality_trunks.imu.blocks.5.attn.bias_v torch.Size([1, 1, 512]) False
[21:48:32.601698] image_bind.modality_trunks.imu.blocks.5.attn.out_proj.weight torch.Size([512, 512]) False
[21:48:32.601731] image_bind.modality_trunks.imu.blocks.5.attn.out_proj.bias torch.Size([512]) False
[21:48:32.601766] image_bind.modality_trunks.imu.blocks.5.norm_1.weight torch.Size([512]) False
[21:48:32.601798] image_bind.modality_trunks.imu.blocks.5.norm_1.bias torch.Size([512]) False
[21:48:32.601833] image_bind.modality_trunks.imu.blocks.5.mlp.fc1.weight torch.Size([2048, 512]) False
[21:48:32.601866] image_bind.modality_trunks.imu.blocks.5.mlp.fc1.bias torch.Size([2048]) False
[21:48:32.601901] image_bind.modality_trunks.imu.blocks.5.mlp.fc2.weight torch.Size([512, 2048]) False
[21:48:32.601935] image_bind.modality_trunks.imu.blocks.5.mlp.fc2.bias torch.Size([512]) False
[21:48:32.601975] image_bind.modality_trunks.imu.blocks.5.norm_2.weight torch.Size([512]) False
[21:48:32.602007] image_bind.modality_trunks.imu.blocks.5.norm_2.bias torch.Size([512]) False
[21:48:32.602046] image_bind.modality_heads.vision.0.weight torch.Size([1280]) False
[21:48:32.602080] image_bind.modality_heads.vision.0.bias torch.Size([1280]) False
[21:48:32.602115] image_bind.modality_heads.vision.2.weight torch.Size([1024, 1280]) False
[21:48:32.602154] image_bind.modality_heads.text.proj.0.weight torch.Size([1024]) False
[21:48:32.602187] image_bind.modality_heads.text.proj.0.bias torch.Size([1024]) False
[21:48:32.602221] image_bind.modality_heads.text.proj.1.weight torch.Size([1024, 1024]) False
[21:48:32.602258] image_bind.modality_heads.audio.0.weight torch.Size([768]) False
[21:48:32.602290] image_bind.modality_heads.audio.0.bias torch.Size([768]) False
[21:48:32.602326] image_bind.modality_heads.audio.2.weight torch.Size([1024, 768]) False
[21:48:32.602362] image_bind.modality_heads.depth.0.weight torch.Size([384]) False
[21:48:32.602395] image_bind.modality_heads.depth.0.bias torch.Size([384]) False
[21:48:32.602431] image_bind.modality_heads.depth.2.weight torch.Size([1024, 384]) False
[21:48:32.602468] image_bind.modality_heads.thermal.0.weight torch.Size([768]) False
[21:48:32.602506] image_bind.modality_heads.thermal.0.bias torch.Size([768]) False
[21:48:32.602542] image_bind.modality_heads.thermal.2.weight torch.Size([1024, 768]) False
[21:48:32.602578] image_bind.modality_heads.imu.0.weight torch.Size([512]) False
[21:48:32.602610] image_bind.modality_heads.imu.0.bias torch.Size([512]) False
[21:48:32.602647] image_bind.modality_heads.imu.3.weight torch.Size([1024, 512]) False
[21:48:32.602682] image_bind.modality_heads.point.0.weight torch.Size([512]) False
[21:48:32.602715] image_bind.modality_heads.point.0.bias torch.Size([512]) False
[21:48:32.602750] image_bind.modality_heads.point.2.weight torch.Size([1024, 512]) False
[21:48:32.602790] image_bind.modality_postprocessors.text.1.log_logit_scale torch.Size([]) False
[21:48:32.602842] image_bind.point_trunk.pc_projection torch.Size([768, 512]) False
[21:48:32.602878] image_bind.point_trunk.point_encoder.cls_token torch.Size([1, 1, 384]) False
[21:48:32.602911] image_bind.point_trunk.point_encoder.cls_pos torch.Size([1, 1, 384]) False
[21:48:32.602952] image_bind.point_trunk.point_encoder.encoder.first_conv.0.weight torch.Size([128, 3, 1]) False
[21:48:32.602991] image_bind.point_trunk.point_encoder.encoder.first_conv.0.bias torch.Size([128]) False
[21:48:32.603029] image_bind.point_trunk.point_encoder.encoder.first_conv.1.weight torch.Size([128]) False
[21:48:32.603070] image_bind.point_trunk.point_encoder.encoder.first_conv.1.bias torch.Size([128]) False
[21:48:32.603108] image_bind.point_trunk.point_encoder.encoder.first_conv.3.weight torch.Size([256, 128, 1]) False
[21:48:32.603146] image_bind.point_trunk.point_encoder.encoder.first_conv.3.bias torch.Size([256]) False
[21:48:32.603190] image_bind.point_trunk.point_encoder.encoder.second_conv.0.weight torch.Size([512, 512, 1]) False
[21:48:32.603228] image_bind.point_trunk.point_encoder.encoder.second_conv.0.bias torch.Size([512]) False
[21:48:32.603266] image_bind.point_trunk.point_encoder.encoder.second_conv.1.weight torch.Size([512]) False
[21:48:32.603299] image_bind.point_trunk.point_encoder.encoder.second_conv.1.bias torch.Size([512]) False
[21:48:32.603334] image_bind.point_trunk.point_encoder.encoder.second_conv.3.weight torch.Size([256, 512, 1]) False
[21:48:32.603367] image_bind.point_trunk.point_encoder.encoder.second_conv.3.bias torch.Size([256]) False
[21:48:32.603401] image_bind.point_trunk.point_encoder.reduce_dim.weight torch.Size([384, 256]) False
[21:48:32.603434] image_bind.point_trunk.point_encoder.reduce_dim.bias torch.Size([384]) False
[21:48:32.603469] image_bind.point_trunk.point_encoder.pos_embed.0.weight torch.Size([128, 3]) False
[21:48:32.603507] image_bind.point_trunk.point_encoder.pos_embed.0.bias torch.Size([128]) False
[21:48:32.603544] image_bind.point_trunk.point_encoder.pos_embed.2.weight torch.Size([384, 128]) False
[21:48:32.603583] image_bind.point_trunk.point_encoder.pos_embed.2.bias torch.Size([384]) False
[21:48:32.603623] image_bind.point_trunk.point_encoder.blocks.blocks.0.norm1.weight torch.Size([384]) False
[21:48:32.603655] image_bind.point_trunk.point_encoder.blocks.blocks.0.norm1.bias torch.Size([384]) False
[21:48:32.603691] image_bind.point_trunk.point_encoder.blocks.blocks.0.norm2.weight torch.Size([384]) False
[21:48:32.603725] image_bind.point_trunk.point_encoder.blocks.blocks.0.norm2.bias torch.Size([384]) False
[21:48:32.603761] image_bind.point_trunk.point_encoder.blocks.blocks.0.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.603793] image_bind.point_trunk.point_encoder.blocks.blocks.0.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.603829] image_bind.point_trunk.point_encoder.blocks.blocks.0.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.603862] image_bind.point_trunk.point_encoder.blocks.blocks.0.mlp.fc2.bias torch.Size([384]) False
[21:48:32.603898] image_bind.point_trunk.point_encoder.blocks.blocks.0.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.603953] image_bind.point_trunk.point_encoder.blocks.blocks.0.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.603986] image_bind.point_trunk.point_encoder.blocks.blocks.0.attn.proj.bias torch.Size([384]) False
[21:48:32.604022] image_bind.point_trunk.point_encoder.blocks.blocks.1.norm1.weight torch.Size([384]) False
[21:48:32.604055] image_bind.point_trunk.point_encoder.blocks.blocks.1.norm1.bias torch.Size([384]) False
[21:48:32.604090] image_bind.point_trunk.point_encoder.blocks.blocks.1.norm2.weight torch.Size([384]) False
[21:48:32.604122] image_bind.point_trunk.point_encoder.blocks.blocks.1.norm2.bias torch.Size([384]) False
[21:48:32.604158] image_bind.point_trunk.point_encoder.blocks.blocks.1.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.604191] image_bind.point_trunk.point_encoder.blocks.blocks.1.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.604231] image_bind.point_trunk.point_encoder.blocks.blocks.1.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.604265] image_bind.point_trunk.point_encoder.blocks.blocks.1.mlp.fc2.bias torch.Size([384]) False
[21:48:32.604301] image_bind.point_trunk.point_encoder.blocks.blocks.1.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.604337] image_bind.point_trunk.point_encoder.blocks.blocks.1.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.604370] image_bind.point_trunk.point_encoder.blocks.blocks.1.attn.proj.bias torch.Size([384]) False
[21:48:32.604406] image_bind.point_trunk.point_encoder.blocks.blocks.2.norm1.weight torch.Size([384]) False
[21:48:32.604439] image_bind.point_trunk.point_encoder.blocks.blocks.2.norm1.bias torch.Size([384]) False
[21:48:32.604474] image_bind.point_trunk.point_encoder.blocks.blocks.2.norm2.weight torch.Size([384]) False
[21:48:32.604510] image_bind.point_trunk.point_encoder.blocks.blocks.2.norm2.bias torch.Size([384]) False
[21:48:32.604564] image_bind.point_trunk.point_encoder.blocks.blocks.2.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.604598] image_bind.point_trunk.point_encoder.blocks.blocks.2.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.604634] image_bind.point_trunk.point_encoder.blocks.blocks.2.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.604666] image_bind.point_trunk.point_encoder.blocks.blocks.2.mlp.fc2.bias torch.Size([384]) False
[21:48:32.604703] image_bind.point_trunk.point_encoder.blocks.blocks.2.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.604739] image_bind.point_trunk.point_encoder.blocks.blocks.2.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.604772] image_bind.point_trunk.point_encoder.blocks.blocks.2.attn.proj.bias torch.Size([384]) False
[21:48:32.604809] image_bind.point_trunk.point_encoder.blocks.blocks.3.norm1.weight torch.Size([384]) False
[21:48:32.604841] image_bind.point_trunk.point_encoder.blocks.blocks.3.norm1.bias torch.Size([384]) False
[21:48:32.604877] image_bind.point_trunk.point_encoder.blocks.blocks.3.norm2.weight torch.Size([384]) False
[21:48:32.604917] image_bind.point_trunk.point_encoder.blocks.blocks.3.norm2.bias torch.Size([384]) False
[21:48:32.604956] image_bind.point_trunk.point_encoder.blocks.blocks.3.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.604989] image_bind.point_trunk.point_encoder.blocks.blocks.3.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.605024] image_bind.point_trunk.point_encoder.blocks.blocks.3.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.605057] image_bind.point_trunk.point_encoder.blocks.blocks.3.mlp.fc2.bias torch.Size([384]) False
[21:48:32.605093] image_bind.point_trunk.point_encoder.blocks.blocks.3.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.605129] image_bind.point_trunk.point_encoder.blocks.blocks.3.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.605163] image_bind.point_trunk.point_encoder.blocks.blocks.3.attn.proj.bias torch.Size([384]) False
[21:48:32.605209] image_bind.point_trunk.point_encoder.blocks.blocks.4.norm1.weight torch.Size([384]) False
[21:48:32.605243] image_bind.point_trunk.point_encoder.blocks.blocks.4.norm1.bias torch.Size([384]) False
[21:48:32.605278] image_bind.point_trunk.point_encoder.blocks.blocks.4.norm2.weight torch.Size([384]) False
[21:48:32.605311] image_bind.point_trunk.point_encoder.blocks.blocks.4.norm2.bias torch.Size([384]) False
[21:48:32.605348] image_bind.point_trunk.point_encoder.blocks.blocks.4.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.605383] image_bind.point_trunk.point_encoder.blocks.blocks.4.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.605420] image_bind.point_trunk.point_encoder.blocks.blocks.4.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.605469] image_bind.point_trunk.point_encoder.blocks.blocks.4.mlp.fc2.bias torch.Size([384]) False
[21:48:32.605513] image_bind.point_trunk.point_encoder.blocks.blocks.4.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.605558] image_bind.point_trunk.point_encoder.blocks.blocks.4.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.605591] image_bind.point_trunk.point_encoder.blocks.blocks.4.attn.proj.bias torch.Size([384]) False
[21:48:32.605628] image_bind.point_trunk.point_encoder.blocks.blocks.5.norm1.weight torch.Size([384]) False
[21:48:32.605660] image_bind.point_trunk.point_encoder.blocks.blocks.5.norm1.bias torch.Size([384]) False
[21:48:32.605707] image_bind.point_trunk.point_encoder.blocks.blocks.5.norm2.weight torch.Size([384]) False
[21:48:32.605740] image_bind.point_trunk.point_encoder.blocks.blocks.5.norm2.bias torch.Size([384]) False
[21:48:32.605775] image_bind.point_trunk.point_encoder.blocks.blocks.5.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.605809] image_bind.point_trunk.point_encoder.blocks.blocks.5.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.605851] image_bind.point_trunk.point_encoder.blocks.blocks.5.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.605887] image_bind.point_trunk.point_encoder.blocks.blocks.5.mlp.fc2.bias torch.Size([384]) False
[21:48:32.605924] image_bind.point_trunk.point_encoder.blocks.blocks.5.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.605965] image_bind.point_trunk.point_encoder.blocks.blocks.5.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.606004] image_bind.point_trunk.point_encoder.blocks.blocks.5.attn.proj.bias torch.Size([384]) False
[21:48:32.606047] image_bind.point_trunk.point_encoder.blocks.blocks.6.norm1.weight torch.Size([384]) False
[21:48:32.606083] image_bind.point_trunk.point_encoder.blocks.blocks.6.norm1.bias torch.Size([384]) False
[21:48:32.606122] image_bind.point_trunk.point_encoder.blocks.blocks.6.norm2.weight torch.Size([384]) False
[21:48:32.606157] image_bind.point_trunk.point_encoder.blocks.blocks.6.norm2.bias torch.Size([384]) False
[21:48:32.606192] image_bind.point_trunk.point_encoder.blocks.blocks.6.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.606225] image_bind.point_trunk.point_encoder.blocks.blocks.6.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.606259] image_bind.point_trunk.point_encoder.blocks.blocks.6.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.606292] image_bind.point_trunk.point_encoder.blocks.blocks.6.mlp.fc2.bias torch.Size([384]) False
[21:48:32.606329] image_bind.point_trunk.point_encoder.blocks.blocks.6.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.606364] image_bind.point_trunk.point_encoder.blocks.blocks.6.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.606396] image_bind.point_trunk.point_encoder.blocks.blocks.6.attn.proj.bias torch.Size([384]) False
[21:48:32.606433] image_bind.point_trunk.point_encoder.blocks.blocks.7.norm1.weight torch.Size([384]) False
[21:48:32.606465] image_bind.point_trunk.point_encoder.blocks.blocks.7.norm1.bias torch.Size([384]) False
[21:48:32.606505] image_bind.point_trunk.point_encoder.blocks.blocks.7.norm2.weight torch.Size([384]) False
[21:48:32.606538] image_bind.point_trunk.point_encoder.blocks.blocks.7.norm2.bias torch.Size([384]) False
[21:48:32.606574] image_bind.point_trunk.point_encoder.blocks.blocks.7.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.606616] image_bind.point_trunk.point_encoder.blocks.blocks.7.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.606651] image_bind.point_trunk.point_encoder.blocks.blocks.7.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.606684] image_bind.point_trunk.point_encoder.blocks.blocks.7.mlp.fc2.bias torch.Size([384]) False
[21:48:32.606721] image_bind.point_trunk.point_encoder.blocks.blocks.7.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.606757] image_bind.point_trunk.point_encoder.blocks.blocks.7.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.606790] image_bind.point_trunk.point_encoder.blocks.blocks.7.attn.proj.bias torch.Size([384]) False
[21:48:32.606827] image_bind.point_trunk.point_encoder.blocks.blocks.8.norm1.weight torch.Size([384]) False
[21:48:32.606860] image_bind.point_trunk.point_encoder.blocks.blocks.8.norm1.bias torch.Size([384]) False
[21:48:32.606895] image_bind.point_trunk.point_encoder.blocks.blocks.8.norm2.weight torch.Size([384]) False
[21:48:32.606928] image_bind.point_trunk.point_encoder.blocks.blocks.8.norm2.bias torch.Size([384]) False
[21:48:32.606963] image_bind.point_trunk.point_encoder.blocks.blocks.8.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.606996] image_bind.point_trunk.point_encoder.blocks.blocks.8.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.607032] image_bind.point_trunk.point_encoder.blocks.blocks.8.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.607064] image_bind.point_trunk.point_encoder.blocks.blocks.8.mlp.fc2.bias torch.Size([384]) False
[21:48:32.607100] image_bind.point_trunk.point_encoder.blocks.blocks.8.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.607136] image_bind.point_trunk.point_encoder.blocks.blocks.8.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.607169] image_bind.point_trunk.point_encoder.blocks.blocks.8.attn.proj.bias torch.Size([384]) False
[21:48:32.607206] image_bind.point_trunk.point_encoder.blocks.blocks.9.norm1.weight torch.Size([384]) False
[21:48:32.607239] image_bind.point_trunk.point_encoder.blocks.blocks.9.norm1.bias torch.Size([384]) False
[21:48:32.607274] image_bind.point_trunk.point_encoder.blocks.blocks.9.norm2.weight torch.Size([384]) False
[21:48:32.607316] image_bind.point_trunk.point_encoder.blocks.blocks.9.norm2.bias torch.Size([384]) False
[21:48:32.607351] image_bind.point_trunk.point_encoder.blocks.blocks.9.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.607384] image_bind.point_trunk.point_encoder.blocks.blocks.9.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.607419] image_bind.point_trunk.point_encoder.blocks.blocks.9.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.607452] image_bind.point_trunk.point_encoder.blocks.blocks.9.mlp.fc2.bias torch.Size([384]) False
[21:48:32.607492] image_bind.point_trunk.point_encoder.blocks.blocks.9.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.607528] image_bind.point_trunk.point_encoder.blocks.blocks.9.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.607560] image_bind.point_trunk.point_encoder.blocks.blocks.9.attn.proj.bias torch.Size([384]) False
[21:48:32.607597] image_bind.point_trunk.point_encoder.blocks.blocks.10.norm1.weight torch.Size([384]) False
[21:48:32.607629] image_bind.point_trunk.point_encoder.blocks.blocks.10.norm1.bias torch.Size([384]) False
[21:48:32.607665] image_bind.point_trunk.point_encoder.blocks.blocks.10.norm2.weight torch.Size([384]) False
[21:48:32.607698] image_bind.point_trunk.point_encoder.blocks.blocks.10.norm2.bias torch.Size([384]) False
[21:48:32.607733] image_bind.point_trunk.point_encoder.blocks.blocks.10.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.607765] image_bind.point_trunk.point_encoder.blocks.blocks.10.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.607801] image_bind.point_trunk.point_encoder.blocks.blocks.10.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.607833] image_bind.point_trunk.point_encoder.blocks.blocks.10.mlp.fc2.bias torch.Size([384]) False
[21:48:32.607870] image_bind.point_trunk.point_encoder.blocks.blocks.10.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.607906] image_bind.point_trunk.point_encoder.blocks.blocks.10.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.607939] image_bind.point_trunk.point_encoder.blocks.blocks.10.attn.proj.bias torch.Size([384]) False
[21:48:32.607976] image_bind.point_trunk.point_encoder.blocks.blocks.11.norm1.weight torch.Size([384]) False
[21:48:32.608008] image_bind.point_trunk.point_encoder.blocks.blocks.11.norm1.bias torch.Size([384]) False
[21:48:32.608044] image_bind.point_trunk.point_encoder.blocks.blocks.11.norm2.weight torch.Size([384]) False
[21:48:32.608076] image_bind.point_trunk.point_encoder.blocks.blocks.11.norm2.bias torch.Size([384]) False
[21:48:32.608111] image_bind.point_trunk.point_encoder.blocks.blocks.11.mlp.fc1.weight torch.Size([1536, 384]) False
[21:48:32.608145] image_bind.point_trunk.point_encoder.blocks.blocks.11.mlp.fc1.bias torch.Size([1536]) False
[21:48:32.608186] image_bind.point_trunk.point_encoder.blocks.blocks.11.mlp.fc2.weight torch.Size([384, 1536]) False
[21:48:32.608220] image_bind.point_trunk.point_encoder.blocks.blocks.11.mlp.fc2.bias torch.Size([384]) False
[21:48:32.608257] image_bind.point_trunk.point_encoder.blocks.blocks.11.attn.qkv.weight torch.Size([1152, 384]) False
[21:48:32.608293] image_bind.point_trunk.point_encoder.blocks.blocks.11.attn.proj.weight torch.Size([384, 384]) False
[21:48:32.608325] image_bind.point_trunk.point_encoder.blocks.blocks.11.attn.proj.bias torch.Size([384]) False
[21:48:32.608361] image_bind.point_trunk.point_encoder.norm.weight torch.Size([384]) False
[21:48:32.608395] image_bind.point_trunk.point_encoder.norm.bias torch.Size([384]) False
[21:48:32.608430] image_bind_proj.weight torch.Size([4096, 1024]) False
[21:48:32.608463] image_bind_proj.bias torch.Size([4096]) False
[21:48:32.608505] image_bind_norm_1.weight torch.Size([4096]) False
[21:48:32.608540] image_bind_f1_1.weight torch.Size([16384, 4096]) False
[21:48:32.608576] image_bind_f2_1.weight torch.Size([4096, 16384]) False
[21:48:32.608610] image_bind_f3_1.weight torch.Size([16384, 4096]) False
[21:48:32.608645] image_bind_norm_2.weight torch.Size([4096]) False
[21:48:32.608679] image_bind_f1_2.weight torch.Size([16384, 4096]) False
[21:48:32.608716] image_bind_f2_2.weight torch.Size([4096, 16384]) False
[21:48:32.608755] image_bind_f3_2.weight torch.Size([16384, 4096]) False
[21:48:32.608791] image_bind_norm_3.weight torch.Size([4096]) False
[21:48:32.608825] image_bind_f1_3.weight torch.Size([16384, 4096]) False
[21:48:32.608859] image_bind_f2_3.weight torch.Size([4096, 16384]) False
[21:48:32.608894] image_bind_f3_3.weight torch.Size([16384, 4096]) False
[21:48:32.608931] llama.tok_embeddings.weight torch.Size([32000, 4096]) False
[21:48:32.608970] llama.layers.0.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.609006] llama.layers.0.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.609039] llama.layers.0.attention.wq.bias torch.Size([4096]) True
[21:48:32.609073] llama.layers.0.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.609108] llama.layers.0.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.609143] llama.layers.0.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.609175] llama.layers.0.attention.wo.bias torch.Size([4096]) True
[21:48:32.609211] llama.layers.0.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.609253] llama.layers.0.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.609287] llama.layers.0.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.609322] llama.layers.0.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.609357] llama.layers.0.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.609391] llama.layers.0.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.609425] llama.layers.0.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.609460] llama.layers.0.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.609502] llama.layers.0.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.609535] llama.layers.0.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.609569] llama.layers.0.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.609602] llama.layers.0.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.609636] llama.layers.0.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.609669] llama.layers.0.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.609703] llama.layers.0.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.609738] llama.layers.0.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.609773] llama.layers.0.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.609807] llama.layers.0.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.609845] llama.layers.0.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.609889] llama.layers.0.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.609924] llama.layers.0.attention_norm.weight torch.Size([4096]) True
[21:48:32.609958] llama.layers.0.ffn_norm.weight torch.Size([4096]) True
[21:48:32.609995] llama.layers.1.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.610030] llama.layers.1.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.610063] llama.layers.1.attention.wq.bias torch.Size([4096]) True
[21:48:32.610097] llama.layers.1.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.610132] llama.layers.1.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.610166] llama.layers.1.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.610199] llama.layers.1.attention.wo.bias torch.Size([4096]) True
[21:48:32.610233] llama.layers.1.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.610267] llama.layers.1.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.610301] llama.layers.1.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.610335] llama.layers.1.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.610370] llama.layers.1.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.610406] llama.layers.1.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.610440] llama.layers.1.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.610474] llama.layers.1.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.610515] llama.layers.1.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.610557] llama.layers.1.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.610592] llama.layers.1.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.610636] llama.layers.1.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.610670] llama.layers.1.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.610703] llama.layers.1.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.610738] llama.layers.1.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.610773] llama.layers.1.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.610808] llama.layers.1.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.610843] llama.layers.1.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.610877] llama.layers.1.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.610912] llama.layers.1.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.610946] llama.layers.1.attention_norm.weight torch.Size([4096]) True
[21:48:32.610981] llama.layers.1.ffn_norm.weight torch.Size([4096]) True
[21:48:32.611018] llama.layers.2.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.611054] llama.layers.2.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.611086] llama.layers.2.attention.wq.bias torch.Size([4096]) True
[21:48:32.611121] llama.layers.2.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.611155] llama.layers.2.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.611190] llama.layers.2.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.611223] llama.layers.2.attention.wo.bias torch.Size([4096]) True
[21:48:32.611267] llama.layers.2.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.611301] llama.layers.2.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.611336] llama.layers.2.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.611371] llama.layers.2.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.611405] llama.layers.2.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.611440] llama.layers.2.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.611474] llama.layers.2.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.611513] llama.layers.2.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.611549] llama.layers.2.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.611581] llama.layers.2.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.611616] llama.layers.2.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.611648] llama.layers.2.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.611683] llama.layers.2.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.611718] llama.layers.2.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.611757] llama.layers.2.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.611792] llama.layers.2.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.611826] llama.layers.2.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.611861] llama.layers.2.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.611895] llama.layers.2.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.611929] llama.layers.2.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.611964] llama.layers.2.attention_norm.weight torch.Size([4096]) True
[21:48:32.611998] llama.layers.2.ffn_norm.weight torch.Size([4096]) True
[21:48:32.612034] llama.layers.3.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.612069] llama.layers.3.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.612102] llama.layers.3.attention.wq.bias torch.Size([4096]) True
[21:48:32.612136] llama.layers.3.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.612170] llama.layers.3.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.612205] llama.layers.3.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.612244] llama.layers.3.attention.wo.bias torch.Size([4096]) True
[21:48:32.612279] llama.layers.3.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.612313] llama.layers.3.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.612348] llama.layers.3.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.612383] llama.layers.3.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.612417] llama.layers.3.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.612452] llama.layers.3.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.612489] llama.layers.3.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.612524] llama.layers.3.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.612560] llama.layers.3.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.612594] llama.layers.3.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.612628] llama.layers.3.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.612669] llama.layers.3.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.612704] llama.layers.3.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.612736] llama.layers.3.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.612771] llama.layers.3.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.612805] llama.layers.3.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.612839] llama.layers.3.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.612874] llama.layers.3.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.612908] llama.layers.3.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.612943] llama.layers.3.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.612977] llama.layers.3.attention_norm.weight torch.Size([4096]) True
[21:48:32.613011] llama.layers.3.ffn_norm.weight torch.Size([4096]) True
[21:48:32.613047] llama.layers.4.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.613083] llama.layers.4.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.613116] llama.layers.4.attention.wq.bias torch.Size([4096]) True
[21:48:32.613162] llama.layers.4.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.613197] llama.layers.4.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.613232] llama.layers.4.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.613265] llama.layers.4.attention.wo.bias torch.Size([4096]) True
[21:48:32.613299] llama.layers.4.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.613334] llama.layers.4.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.613368] llama.layers.4.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.613403] llama.layers.4.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.613437] llama.layers.4.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.613471] llama.layers.4.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.613510] llama.layers.4.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.613545] llama.layers.4.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.613580] llama.layers.4.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.613615] llama.layers.4.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.613649] llama.layers.4.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.613682] llama.layers.4.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.613733] llama.layers.4.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.613766] llama.layers.4.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.613805] llama.layers.4.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.613841] llama.layers.4.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.613876] llama.layers.4.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.613911] llama.layers.4.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.613946] llama.layers.4.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.613981] llama.layers.4.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.614025] llama.layers.4.attention_norm.weight torch.Size([4096]) True
[21:48:32.614059] llama.layers.4.ffn_norm.weight torch.Size([4096]) True
[21:48:32.614096] llama.layers.5.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.614131] llama.layers.5.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.614164] llama.layers.5.attention.wq.bias torch.Size([4096]) True
[21:48:32.614199] llama.layers.5.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.614235] llama.layers.5.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.614270] llama.layers.5.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.614303] llama.layers.5.attention.wo.bias torch.Size([4096]) True
[21:48:32.614344] llama.layers.5.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.614379] llama.layers.5.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.614413] llama.layers.5.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.614448] llama.layers.5.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.614483] llama.layers.5.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.614524] llama.layers.5.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.614558] llama.layers.5.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.614593] llama.layers.5.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.614629] llama.layers.5.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.614662] llama.layers.5.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.614696] llama.layers.5.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.614729] llama.layers.5.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.614763] llama.layers.5.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.614796] llama.layers.5.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.614829] llama.layers.5.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.614870] llama.layers.5.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.614906] llama.layers.5.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.614941] llama.layers.5.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.614975] llama.layers.5.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.615009] llama.layers.5.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.615044] llama.layers.5.attention_norm.weight torch.Size([4096]) True
[21:48:32.615078] llama.layers.5.ffn_norm.weight torch.Size([4096]) True
[21:48:32.615114] llama.layers.6.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.615149] llama.layers.6.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.615182] llama.layers.6.attention.wq.bias torch.Size([4096]) True
[21:48:32.615216] llama.layers.6.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.615250] llama.layers.6.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.615284] llama.layers.6.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.615317] llama.layers.6.attention.wo.bias torch.Size([4096]) True
[21:48:32.615356] llama.layers.6.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.615390] llama.layers.6.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.615424] llama.layers.6.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.615458] llama.layers.6.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.615498] llama.layers.6.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.615533] llama.layers.6.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.615567] llama.layers.6.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.615601] llama.layers.6.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.615637] llama.layers.6.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.615669] llama.layers.6.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.615703] llama.layers.6.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.615735] llama.layers.6.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.615769] llama.layers.6.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.615801] llama.layers.6.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.615836] llama.layers.6.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.615877] llama.layers.6.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.615911] llama.layers.6.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.615946] llama.layers.6.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.615980] llama.layers.6.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.616014] llama.layers.6.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.616049] llama.layers.6.attention_norm.weight torch.Size([4096]) True
[21:48:32.616083] llama.layers.6.ffn_norm.weight torch.Size([4096]) True
[21:48:32.616120] llama.layers.7.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.616155] llama.layers.7.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.616188] llama.layers.7.attention.wq.bias torch.Size([4096]) True
[21:48:32.616222] llama.layers.7.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.616256] llama.layers.7.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.616290] llama.layers.7.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.616323] llama.layers.7.attention.wo.bias torch.Size([4096]) True
[21:48:32.616357] llama.layers.7.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.616396] llama.layers.7.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.616431] llama.layers.7.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.616465] llama.layers.7.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.616506] llama.layers.7.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.616540] llama.layers.7.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.616575] llama.layers.7.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.616609] llama.layers.7.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.616644] llama.layers.7.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.616677] llama.layers.7.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.616711] llama.layers.7.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.616744] llama.layers.7.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.616778] llama.layers.7.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.616810] llama.layers.7.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.616851] llama.layers.7.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.616885] llama.layers.7.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.616919] llama.layers.7.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.616953] llama.layers.7.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.616988] llama.layers.7.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.617022] llama.layers.7.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.617057] llama.layers.7.attention_norm.weight torch.Size([4096]) True
[21:48:32.617091] llama.layers.7.ffn_norm.weight torch.Size([4096]) True
[21:48:32.617127] llama.layers.8.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.617162] llama.layers.8.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.617195] llama.layers.8.attention.wq.bias torch.Size([4096]) True
[21:48:32.617229] llama.layers.8.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.617269] llama.layers.8.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.617303] llama.layers.8.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.617336] llama.layers.8.attention.wo.bias torch.Size([4096]) True
[21:48:32.617370] llama.layers.8.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.617404] llama.layers.8.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.617438] llama.layers.8.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.617473] llama.layers.8.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.617511] llama.layers.8.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.617546] llama.layers.8.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.617580] llama.layers.8.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.617614] llama.layers.8.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.617649] llama.layers.8.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.617682] llama.layers.8.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.617716] llama.layers.8.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.617749] llama.layers.8.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.617789] llama.layers.8.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.617822] llama.layers.8.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.617856] llama.layers.8.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.617890] llama.layers.8.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.617924] llama.layers.8.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.617958] llama.layers.8.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.617992] llama.layers.8.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.618027] llama.layers.8.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.618061] llama.layers.8.attention_norm.weight torch.Size([4096]) True
[21:48:32.618102] llama.layers.8.ffn_norm.weight torch.Size([4096]) True
[21:48:32.618140] llama.layers.9.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.618175] llama.layers.9.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.618207] llama.layers.9.attention.wq.bias torch.Size([4096]) True
[21:48:32.618241] llama.layers.9.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.618276] llama.layers.9.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.618311] llama.layers.9.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.618350] llama.layers.9.attention.wo.bias torch.Size([4096]) True
[21:48:32.618383] llama.layers.9.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.618418] llama.layers.9.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.618452] llama.layers.9.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.618491] llama.layers.9.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.618525] llama.layers.9.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.618560] llama.layers.9.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.618593] llama.layers.9.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.618628] llama.layers.9.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.618663] llama.layers.9.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.618698] llama.layers.9.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.618732] llama.layers.9.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.618765] llama.layers.9.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.618801] llama.layers.9.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.618836] llama.layers.9.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.618871] llama.layers.9.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.618905] llama.layers.9.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.618940] llama.layers.9.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.618975] llama.layers.9.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.619010] llama.layers.9.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.619044] llama.layers.9.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.619079] llama.layers.9.attention_norm.weight torch.Size([4096]) True
[21:48:32.619125] llama.layers.9.ffn_norm.weight torch.Size([4096]) True
[21:48:32.619162] llama.layers.10.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.619197] llama.layers.10.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.619230] llama.layers.10.attention.wq.bias torch.Size([4096]) True
[21:48:32.619270] llama.layers.10.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.619304] llama.layers.10.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.619339] llama.layers.10.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.619372] llama.layers.10.attention.wo.bias torch.Size([4096]) True
[21:48:32.619406] llama.layers.10.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.619440] llama.layers.10.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.619474] llama.layers.10.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.619513] llama.layers.10.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.619547] llama.layers.10.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.619581] llama.layers.10.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.619615] llama.layers.10.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.619649] llama.layers.10.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.619686] llama.layers.10.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.619719] llama.layers.10.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.619759] llama.layers.10.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.619792] llama.layers.10.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.619827] llama.layers.10.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.619860] llama.layers.10.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.619893] llama.layers.10.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.619928] llama.layers.10.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.619962] llama.layers.10.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.619996] llama.layers.10.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.620030] llama.layers.10.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.620064] llama.layers.10.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.620098] llama.layers.10.attention_norm.weight torch.Size([4096]) True
[21:48:32.620132] llama.layers.10.ffn_norm.weight torch.Size([4096]) True
[21:48:32.620168] llama.layers.11.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.620203] llama.layers.11.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.620235] llama.layers.11.attention.wq.bias torch.Size([4096]) True
[21:48:32.620269] llama.layers.11.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.620303] llama.layers.11.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.620343] llama.layers.11.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.620376] llama.layers.11.attention.wo.bias torch.Size([4096]) True
[21:48:32.620410] llama.layers.11.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.620444] llama.layers.11.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.620479] llama.layers.11.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.620518] llama.layers.11.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.620552] llama.layers.11.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.620586] llama.layers.11.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.620620] llama.layers.11.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.620654] llama.layers.11.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.620689] llama.layers.11.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.620722] llama.layers.11.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.620756] llama.layers.11.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.620789] llama.layers.11.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.620823] llama.layers.11.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.620856] llama.layers.11.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.620899] llama.layers.11.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.620937] llama.layers.11.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.620972] llama.layers.11.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.621006] llama.layers.11.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.621040] llama.layers.11.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.621074] llama.layers.11.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.621108] llama.layers.11.attention_norm.weight torch.Size([4096]) True
[21:48:32.621142] llama.layers.11.ffn_norm.weight torch.Size([4096]) True
[21:48:32.621178] llama.layers.12.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.621213] llama.layers.12.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.621245] llama.layers.12.attention.wq.bias torch.Size([4096]) True
[21:48:32.621279] llama.layers.12.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.621313] llama.layers.12.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.621348] llama.layers.12.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.621380] llama.layers.12.attention.wo.bias torch.Size([4096]) True
[21:48:32.621414] llama.layers.12.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.621448] llama.layers.12.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.621484] llama.layers.12.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.621523] llama.layers.12.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.621566] llama.layers.12.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.621612] llama.layers.12.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.621646] llama.layers.12.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.621681] llama.layers.12.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.621717] llama.layers.12.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.621750] llama.layers.12.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.621784] llama.layers.12.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.621817] llama.layers.12.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.621851] llama.layers.12.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.621892] llama.layers.12.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.621926] llama.layers.12.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.621961] llama.layers.12.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.621995] llama.layers.12.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.622030] llama.layers.12.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.622068] llama.layers.12.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.622102] llama.layers.12.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.622139] llama.layers.12.attention_norm.weight torch.Size([4096]) True
[21:48:32.622173] llama.layers.12.ffn_norm.weight torch.Size([4096]) True
[21:48:32.622209] llama.layers.13.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.622244] llama.layers.13.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.622276] llama.layers.13.attention.wq.bias torch.Size([4096]) True
[21:48:32.622310] llama.layers.13.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.622345] llama.layers.13.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.622379] llama.layers.13.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.622412] llama.layers.13.attention.wo.bias torch.Size([4096]) True
[21:48:32.622451] llama.layers.13.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.622490] llama.layers.13.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.622524] llama.layers.13.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.622559] llama.layers.13.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.622593] llama.layers.13.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.622627] llama.layers.13.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.622661] llama.layers.13.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.622695] llama.layers.13.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.622731] llama.layers.13.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.622763] llama.layers.13.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.622797] llama.layers.13.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.622830] llama.layers.13.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.622864] llama.layers.13.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.622896] llama.layers.13.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.622936] llama.layers.13.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.622970] llama.layers.13.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.623004] llama.layers.13.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.623039] llama.layers.13.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.623073] llama.layers.13.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.623107] llama.layers.13.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.623142] llama.layers.13.attention_norm.weight torch.Size([4096]) True
[21:48:32.623176] llama.layers.13.ffn_norm.weight torch.Size([4096]) True
[21:48:32.623212] llama.layers.14.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.623246] llama.layers.14.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.623279] llama.layers.14.attention.wq.bias torch.Size([4096]) True
[21:48:32.623313] llama.layers.14.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.623347] llama.layers.14.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.623382] llama.layers.14.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.623420] llama.layers.14.attention.wo.bias torch.Size([4096]) True
[21:48:32.623454] llama.layers.14.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.623493] llama.layers.14.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.623527] llama.layers.14.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.623561] llama.layers.14.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.623595] llama.layers.14.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.623632] llama.layers.14.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.623672] llama.layers.14.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.623706] llama.layers.14.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.623741] llama.layers.14.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.623774] llama.layers.14.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.623808] llama.layers.14.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.623840] llama.layers.14.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.623874] llama.layers.14.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.623911] llama.layers.14.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.623946] llama.layers.14.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.623980] llama.layers.14.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.624014] llama.layers.14.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.624048] llama.layers.14.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.624082] llama.layers.14.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.624117] llama.layers.14.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.624151] llama.layers.14.attention_norm.weight torch.Size([4096]) True
[21:48:32.624187] llama.layers.14.ffn_norm.weight torch.Size([4096]) True
[21:48:32.624223] llama.layers.15.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.624258] llama.layers.15.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.624292] llama.layers.15.attention.wq.bias torch.Size([4096]) True
[21:48:32.624326] llama.layers.15.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.624360] llama.layers.15.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.624394] llama.layers.15.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.624427] llama.layers.15.attention.wo.bias torch.Size([4096]) True
[21:48:32.624463] llama.layers.15.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.624506] llama.layers.15.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.624541] llama.layers.15.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.624585] llama.layers.15.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.624619] llama.layers.15.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.624653] llama.layers.15.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.624687] llama.layers.15.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.624721] llama.layers.15.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.624758] llama.layers.15.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.624790] llama.layers.15.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.624825] llama.layers.15.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.624858] llama.layers.15.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.624892] llama.layers.15.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.624925] llama.layers.15.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.624959] llama.layers.15.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.624994] llama.layers.15.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.625030] llama.layers.15.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.625064] llama.layers.15.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.625099] llama.layers.15.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.625133] llama.layers.15.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.625167] llama.layers.15.attention_norm.weight torch.Size([4096]) True
[21:48:32.625201] llama.layers.15.ffn_norm.weight torch.Size([4096]) True
[21:48:32.625237] llama.layers.16.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.625273] llama.layers.16.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.625305] llama.layers.16.attention.wq.bias torch.Size([4096]) True
[21:48:32.625339] llama.layers.16.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.625374] llama.layers.16.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.625408] llama.layers.16.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.625441] llama.layers.16.attention.wo.bias torch.Size([4096]) True
[21:48:32.625475] llama.layers.16.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.625515] llama.layers.16.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.625555] llama.layers.16.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.625589] llama.layers.16.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.625623] llama.layers.16.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.625658] llama.layers.16.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.625692] llama.layers.16.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.625726] llama.layers.16.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.625763] llama.layers.16.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.625795] llama.layers.16.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.625829] llama.layers.16.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.625862] llama.layers.16.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.625896] llama.layers.16.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.625929] llama.layers.16.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.625962] llama.layers.16.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.625996] llama.layers.16.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.626030] llama.layers.16.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.626065] llama.layers.16.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.626099] llama.layers.16.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.626133] llama.layers.16.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.626167] llama.layers.16.attention_norm.weight torch.Size([4096]) True
[21:48:32.626201] llama.layers.16.ffn_norm.weight torch.Size([4096]) True
[21:48:32.626236] llama.layers.17.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.626271] llama.layers.17.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.626304] llama.layers.17.attention.wq.bias torch.Size([4096]) True
[21:48:32.626338] llama.layers.17.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.626377] llama.layers.17.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.626415] llama.layers.17.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.626454] llama.layers.17.attention.wo.bias torch.Size([4096]) True
[21:48:32.626498] llama.layers.17.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.626534] llama.layers.17.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.626576] llama.layers.17.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.626615] llama.layers.17.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.626657] llama.layers.17.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.626691] llama.layers.17.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.626734] llama.layers.17.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.626778] llama.layers.17.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.626814] llama.layers.17.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.626846] llama.layers.17.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.626880] llama.layers.17.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.626913] llama.layers.17.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.626948] llama.layers.17.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.626980] llama.layers.17.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.627014] llama.layers.17.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.627048] llama.layers.17.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.627083] llama.layers.17.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.627119] llama.layers.17.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.627153] llama.layers.17.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.627187] llama.layers.17.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.627234] llama.layers.17.attention_norm.weight torch.Size([4096]) True
[21:48:32.627272] llama.layers.17.ffn_norm.weight torch.Size([4096]) True
[21:48:32.627316] llama.layers.18.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.627352] llama.layers.18.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.627385] llama.layers.18.attention.wq.bias torch.Size([4096]) True
[21:48:32.627421] llama.layers.18.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.627456] llama.layers.18.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.627494] llama.layers.18.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.627528] llama.layers.18.attention.wo.bias torch.Size([4096]) True
[21:48:32.627564] llama.layers.18.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.627599] llama.layers.18.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.627637] llama.layers.18.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.627682] llama.layers.18.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.627716] llama.layers.18.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.627750] llama.layers.18.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.627785] llama.layers.18.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.627820] llama.layers.18.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.627856] llama.layers.18.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.627888] llama.layers.18.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.627923] llama.layers.18.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.627955] llama.layers.18.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.627990] llama.layers.18.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.628023] llama.layers.18.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.628056] llama.layers.18.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.628091] llama.layers.18.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.628125] llama.layers.18.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.628160] llama.layers.18.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.628195] llama.layers.18.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.628229] llama.layers.18.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.628263] llama.layers.18.attention_norm.weight torch.Size([4096]) True
[21:48:32.628298] llama.layers.18.ffn_norm.weight torch.Size([4096]) True
[21:48:32.628333] llama.layers.19.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.628368] llama.layers.19.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.628401] llama.layers.19.attention.wq.bias torch.Size([4096]) True
[21:48:32.628435] llama.layers.19.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.628469] llama.layers.19.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.628507] llama.layers.19.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.628540] llama.layers.19.attention.wo.bias torch.Size([4096]) True
[21:48:32.628574] llama.layers.19.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.628608] llama.layers.19.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.628642] llama.layers.19.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.628675] llama.layers.19.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.628709] llama.layers.19.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.628744] llama.layers.19.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.628777] llama.layers.19.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.628812] llama.layers.19.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.628848] llama.layers.19.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.628881] llama.layers.19.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.628915] llama.layers.19.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.628948] llama.layers.19.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.628983] llama.layers.19.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.629015] llama.layers.19.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.629049] llama.layers.19.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.629084] llama.layers.19.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.629118] llama.layers.19.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.629152] llama.layers.19.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.629187] llama.layers.19.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.629221] llama.layers.19.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.629255] llama.layers.19.attention_norm.weight torch.Size([4096]) True
[21:48:32.629289] llama.layers.19.ffn_norm.weight torch.Size([4096]) True
[21:48:32.629325] llama.layers.20.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.629360] llama.layers.20.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.629396] llama.layers.20.attention.wq.bias torch.Size([4096]) True
[21:48:32.629435] llama.layers.20.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.629470] llama.layers.20.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.629510] llama.layers.20.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.629543] llama.layers.20.attention.wo.bias torch.Size([4096]) True
[21:48:32.629577] llama.layers.20.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.629612] llama.layers.20.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.629646] llama.layers.20.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.629680] llama.layers.20.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.629714] llama.layers.20.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.629748] llama.layers.20.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.629782] llama.layers.20.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.629816] llama.layers.20.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.629853] llama.layers.20.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.629885] llama.layers.20.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.629922] llama.layers.20.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.629955] llama.layers.20.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.629989] llama.layers.20.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.630024] llama.layers.20.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.630066] llama.layers.20.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.630101] llama.layers.20.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.630135] llama.layers.20.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.630170] llama.layers.20.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.630205] llama.layers.20.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.630240] llama.layers.20.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.630274] llama.layers.20.attention_norm.weight torch.Size([4096]) True
[21:48:32.630310] llama.layers.20.ffn_norm.weight torch.Size([4096]) True
[21:48:32.630346] llama.layers.21.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.630382] llama.layers.21.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.630418] llama.layers.21.attention.wq.bias torch.Size([4096]) True
[21:48:32.630453] llama.layers.21.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.630492] llama.layers.21.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.630529] llama.layers.21.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.630561] llama.layers.21.attention.wo.bias torch.Size([4096]) True
[21:48:32.630595] llama.layers.21.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.630630] llama.layers.21.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.630665] llama.layers.21.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.630700] llama.layers.21.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.630734] llama.layers.21.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.630768] llama.layers.21.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.630803] llama.layers.21.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.630837] llama.layers.21.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.630873] llama.layers.21.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.630912] llama.layers.21.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.630946] llama.layers.21.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.630978] llama.layers.21.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.631012] llama.layers.21.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.631045] llama.layers.21.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.631079] llama.layers.21.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.631113] llama.layers.21.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.631147] llama.layers.21.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.631181] llama.layers.21.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.631215] llama.layers.21.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.631249] llama.layers.21.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.631284] llama.layers.21.attention_norm.weight torch.Size([4096]) True
[21:48:32.631318] llama.layers.21.ffn_norm.weight torch.Size([4096]) True
[21:48:32.631354] llama.layers.22.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.631389] llama.layers.22.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.631422] llama.layers.22.attention.wq.bias torch.Size([4096]) True
[21:48:32.631456] llama.layers.22.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.631494] llama.layers.22.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.631529] llama.layers.22.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.631562] llama.layers.22.attention.wo.bias torch.Size([4096]) True
[21:48:32.631596] llama.layers.22.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.631631] llama.layers.22.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.631665] llama.layers.22.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.631699] llama.layers.22.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.631734] llama.layers.22.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.631768] llama.layers.22.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.631804] llama.layers.22.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.631840] llama.layers.22.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.631876] llama.layers.22.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.631909] llama.layers.22.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.631943] llama.layers.22.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.631976] llama.layers.22.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.632009] llama.layers.22.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.632042] llama.layers.22.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.632076] llama.layers.22.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.632110] llama.layers.22.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.632151] llama.layers.22.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.632186] llama.layers.22.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.632220] llama.layers.22.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.632254] llama.layers.22.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.632288] llama.layers.22.attention_norm.weight torch.Size([4096]) True
[21:48:32.632322] llama.layers.22.ffn_norm.weight torch.Size([4096]) True
[21:48:32.632358] llama.layers.23.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.632393] llama.layers.23.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.632425] llama.layers.23.attention.wq.bias torch.Size([4096]) True
[21:48:32.632459] llama.layers.23.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.632498] llama.layers.23.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.632533] llama.layers.23.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.632569] llama.layers.23.attention.wo.bias torch.Size([4096]) True
[21:48:32.632604] llama.layers.23.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.632638] llama.layers.23.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.632689] llama.layers.23.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.632724] llama.layers.23.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.632759] llama.layers.23.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.632793] llama.layers.23.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.632828] llama.layers.23.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.632862] llama.layers.23.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.632898] llama.layers.23.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.632939] llama.layers.23.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.632973] llama.layers.23.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.633005] llama.layers.23.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.633040] llama.layers.23.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.633072] llama.layers.23.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.633106] llama.layers.23.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.633140] llama.layers.23.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.633179] llama.layers.23.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.633213] llama.layers.23.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.633248] llama.layers.23.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.633282] llama.layers.23.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.633316] llama.layers.23.attention_norm.weight torch.Size([4096]) True
[21:48:32.633350] llama.layers.23.ffn_norm.weight torch.Size([4096]) True
[21:48:32.633387] llama.layers.24.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.633421] llama.layers.24.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.633454] llama.layers.24.attention.wq.bias torch.Size([4096]) True
[21:48:32.633493] llama.layers.24.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.633528] llama.layers.24.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.633563] llama.layers.24.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.633595] llama.layers.24.attention.wo.bias torch.Size([4096]) True
[21:48:32.633630] llama.layers.24.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.633664] llama.layers.24.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.633703] llama.layers.24.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.633738] llama.layers.24.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.633772] llama.layers.24.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.633806] llama.layers.24.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.633840] llama.layers.24.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.633874] llama.layers.24.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.633910] llama.layers.24.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.633943] llama.layers.24.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.633977] llama.layers.24.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.634010] llama.layers.24.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.634044] llama.layers.24.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.634076] llama.layers.24.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.634110] llama.layers.24.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.634144] llama.layers.24.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.634178] llama.layers.24.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.634213] llama.layers.24.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.634248] llama.layers.24.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.634285] llama.layers.24.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.634319] llama.layers.24.attention_norm.weight torch.Size([4096]) True
[21:48:32.634354] llama.layers.24.ffn_norm.weight torch.Size([4096]) True
[21:48:32.634389] llama.layers.25.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.634424] llama.layers.25.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.634457] llama.layers.25.attention.wq.bias torch.Size([4096]) True
[21:48:32.634495] llama.layers.25.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.634530] llama.layers.25.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.634564] llama.layers.25.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.634596] llama.layers.25.attention.wo.bias torch.Size([4096]) True
[21:48:32.634630] llama.layers.25.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.634665] llama.layers.25.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.634699] llama.layers.25.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.634739] llama.layers.25.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.634773] llama.layers.25.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.634808] llama.layers.25.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.634843] llama.layers.25.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.634877] llama.layers.25.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.634913] llama.layers.25.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.634945] llama.layers.25.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.634983] llama.layers.25.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.635020] llama.layers.25.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.635053] llama.layers.25.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.635086] llama.layers.25.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.635120] llama.layers.25.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.635154] llama.layers.25.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.635189] llama.layers.25.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.635229] llama.layers.25.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.635263] llama.layers.25.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.635297] llama.layers.25.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.635332] llama.layers.25.attention_norm.weight torch.Size([4096]) True
[21:48:32.635365] llama.layers.25.ffn_norm.weight torch.Size([4096]) True
[21:48:32.635401] llama.layers.26.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.635436] llama.layers.26.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.635468] llama.layers.26.attention.wq.bias torch.Size([4096]) True
[21:48:32.635510] llama.layers.26.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.635544] llama.layers.26.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.635578] llama.layers.26.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.635612] llama.layers.26.attention.wo.bias torch.Size([4096]) True
[21:48:32.635646] llama.layers.26.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.635680] llama.layers.26.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.635715] llama.layers.26.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.635755] llama.layers.26.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.635789] llama.layers.26.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.635823] llama.layers.26.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.635868] llama.layers.26.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.635903] llama.layers.26.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.635938] llama.layers.26.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.635971] llama.layers.26.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.636005] llama.layers.26.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.636038] llama.layers.26.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.636072] llama.layers.26.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.636105] llama.layers.26.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.636139] llama.layers.26.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.636173] llama.layers.26.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.636207] llama.layers.26.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.636242] llama.layers.26.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.636283] llama.layers.26.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.636317] llama.layers.26.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.636351] llama.layers.26.attention_norm.weight torch.Size([4096]) True
[21:48:32.636385] llama.layers.26.ffn_norm.weight torch.Size([4096]) True
[21:48:32.636421] llama.layers.27.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.636457] llama.layers.27.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.636495] llama.layers.27.attention.wq.bias torch.Size([4096]) True
[21:48:32.636529] llama.layers.27.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.636564] llama.layers.27.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.636598] llama.layers.27.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.636631] llama.layers.27.attention.wo.bias torch.Size([4096]) True
[21:48:32.636665] llama.layers.27.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.636699] llama.layers.27.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.636734] llama.layers.27.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.636768] llama.layers.27.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.636802] llama.layers.27.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.636838] llama.layers.27.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.636877] llama.layers.27.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.636911] llama.layers.27.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.636948] llama.layers.27.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.636981] llama.layers.27.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.637014] llama.layers.27.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.637047] llama.layers.27.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.637081] llama.layers.27.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.637113] llama.layers.27.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.637147] llama.layers.27.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.637181] llama.layers.27.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.637214] llama.layers.27.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.637250] llama.layers.27.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.637289] llama.layers.27.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.637324] llama.layers.27.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.637359] llama.layers.27.attention_norm.weight torch.Size([4096]) True
[21:48:32.637392] llama.layers.27.ffn_norm.weight torch.Size([4096]) True
[21:48:32.637428] llama.layers.28.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.637463] llama.layers.28.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.637500] llama.layers.28.attention.wq.bias torch.Size([4096]) True
[21:48:32.637534] llama.layers.28.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.637568] llama.layers.28.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.637602] llama.layers.28.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.637635] llama.layers.28.attention.wo.bias torch.Size([4096]) True
[21:48:32.637669] llama.layers.28.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.637703] llama.layers.28.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.637738] llama.layers.28.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.637779] llama.layers.28.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.637813] llama.layers.28.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.637847] llama.layers.28.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.637882] llama.layers.28.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.637917] llama.layers.28.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.637952] llama.layers.28.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.637985] llama.layers.28.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.638019] llama.layers.28.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.638052] llama.layers.28.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.638086] llama.layers.28.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.638119] llama.layers.28.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.638152] llama.layers.28.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.638186] llama.layers.28.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.638220] llama.layers.28.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.638254] llama.layers.28.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.638288] llama.layers.28.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.638322] llama.layers.28.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.638357] llama.layers.28.attention_norm.weight torch.Size([4096]) True
[21:48:32.638397] llama.layers.28.ffn_norm.weight torch.Size([4096]) True
[21:48:32.638433] llama.layers.29.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.638468] llama.layers.29.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.638504] llama.layers.29.attention.wq.bias torch.Size([4096]) True
[21:48:32.638539] llama.layers.29.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.638573] llama.layers.29.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.638608] llama.layers.29.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.638640] llama.layers.29.attention.wo.bias torch.Size([4096]) True
[21:48:32.638674] llama.layers.29.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.638708] llama.layers.29.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.638742] llama.layers.29.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.638777] llama.layers.29.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.638811] llama.layers.29.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.638845] llama.layers.29.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.638880] llama.layers.29.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.638917] llama.layers.29.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.638952] llama.layers.29.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.638986] llama.layers.29.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.639020] llama.layers.29.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.639052] llama.layers.29.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.639086] llama.layers.29.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.639118] llama.layers.29.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.639152] llama.layers.29.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.639186] llama.layers.29.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.639220] llama.layers.29.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.639254] llama.layers.29.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.639288] llama.layers.29.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.639322] llama.layers.29.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.639356] llama.layers.29.attention_norm.weight torch.Size([4096]) True
[21:48:32.639391] llama.layers.29.ffn_norm.weight torch.Size([4096]) True
[21:48:32.639426] llama.layers.30.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.639460] llama.layers.30.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.639498] llama.layers.30.attention.wq.bias torch.Size([4096]) True
[21:48:32.639539] llama.layers.30.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.639574] llama.layers.30.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.639608] llama.layers.30.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.639641] llama.layers.30.attention.wo.bias torch.Size([4096]) True
[21:48:32.639676] llama.layers.30.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.639710] llama.layers.30.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.639744] llama.layers.30.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.639784] llama.layers.30.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.639822] llama.layers.30.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.639856] llama.layers.30.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.639890] llama.layers.30.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.639924] llama.layers.30.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.639960] llama.layers.30.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.639993] llama.layers.30.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.640027] llama.layers.30.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.640066] llama.layers.30.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.640100] llama.layers.30.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.640133] llama.layers.30.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.640166] llama.layers.30.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.640201] llama.layers.30.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.640235] llama.layers.30.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.640271] llama.layers.30.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.640305] llama.layers.30.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.640339] llama.layers.30.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.640375] llama.layers.30.attention_norm.weight torch.Size([4096]) True
[21:48:32.640409] llama.layers.30.ffn_norm.weight torch.Size([4096]) True
[21:48:32.640445] llama.layers.31.attention.gate torch.Size([1, 32, 1, 1]) False
[21:48:32.640493] llama.layers.31.attention.wq.weight torch.Size([4096, 4096]) False
[21:48:32.640528] llama.layers.31.attention.wq.bias torch.Size([4096]) True
[21:48:32.640562] llama.layers.31.attention.wk.weight torch.Size([4096, 4096]) False
[21:48:32.640597] llama.layers.31.attention.wv.weight torch.Size([4096, 4096]) False
[21:48:32.640632] llama.layers.31.attention.wo.weight torch.Size([4096, 4096]) False
[21:48:32.640678] llama.layers.31.attention.wo.bias torch.Size([4096]) True
[21:48:32.640712] llama.layers.31.attention.lora_wq_l1.weight torch.Size([16, 4096]) True
[21:48:32.640747] llama.layers.31.attention.lora_wq_l2.weight torch.Size([4096, 16]) True
[21:48:32.640781] llama.layers.31.attention.lora_wk_l1.weight torch.Size([16, 4096]) True
[21:48:32.640816] llama.layers.31.attention.lora_wk_l2.weight torch.Size([4096, 16]) True
[21:48:32.640850] llama.layers.31.attention.lora_wv_l1.weight torch.Size([16, 4096]) True
[21:48:32.640884] llama.layers.31.attention.lora_wv_l2.weight torch.Size([4096, 16]) True
[21:48:32.640925] llama.layers.31.attention.lora_wo_l1.weight torch.Size([16, 4096]) True
[21:48:32.640960] llama.layers.31.attention.lora_wo_l2.weight torch.Size([4096, 16]) True
[21:48:32.640996] llama.layers.31.feed_forward.w1.weight torch.Size([11008, 4096]) False
[21:48:32.641028] llama.layers.31.feed_forward.w1.bias torch.Size([11008]) True
[21:48:32.641062] llama.layers.31.feed_forward.w2.weight torch.Size([4096, 11008]) False
[21:48:32.641095] llama.layers.31.feed_forward.w2.bias torch.Size([4096]) True
[21:48:32.641129] llama.layers.31.feed_forward.w3.weight torch.Size([11008, 4096]) False
[21:48:32.641161] llama.layers.31.feed_forward.w3.bias torch.Size([11008]) True
[21:48:32.641195] llama.layers.31.feed_forward.lora_w1_l1.weight torch.Size([16, 4096]) True
[21:48:32.641229] llama.layers.31.feed_forward.lora_w1_l2.weight torch.Size([11008, 16]) True
[21:48:32.641263] llama.layers.31.feed_forward.lora_w2_l1.weight torch.Size([16, 11008]) True
[21:48:32.641297] llama.layers.31.feed_forward.lora_w2_l2.weight torch.Size([4096, 16]) True
[21:48:32.641331] llama.layers.31.feed_forward.lora_w3_l1.weight torch.Size([16, 4096]) True
[21:48:32.641365] llama.layers.31.feed_forward.lora_w3_l2.weight torch.Size([11008, 16]) True
[21:48:32.641400] llama.layers.31.attention_norm.weight torch.Size([4096]) True
[21:48:32.641433] llama.layers.31.ffn_norm.weight torch.Size([4096]) True
[21:48:32.641468] llama.norm.weight torch.Size([4096]) True
[21:48:32.641508] llama.output.weight torch.Size([32000, 4096]) False
[21:48:32.641545] prefix_query.weight torch.Size([32, 4096]) False
[21:48:33.144478] base lr: 5.00e-04
[21:48:33.144553] actual lr: 6.25e-05
[21:48:33.144573] accumulate grad iterations: 4
[21:48:33.144590] effective batch size: 32
[21:48:33.150213] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 6.25e-05
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 6.25e-05
    maximize: False
    weight_decay: 0.02
)
[21:48:44.981045] Used datasets 22 ['SpoofDetection_Asvspoof2017', 'SpeechTextMatching_LibrispeechTrainClean360', 'DialogueActClassification_DailyTalk', 'DialogueEmotionClassification_DailyTalk', 'SpokenTermDetection_Tedlium2Train', 'NoiseSNRLevelPredictionGaussian_VoxcelebMusan', 'EnhancementDetection_LibrittsTrainClean360Wham', 'SpeakerCounting_LibrittsTrainClean100', 'SpeakerVerification_Aishell1Train', 'SpoofDetection_ASVspoof2015', 'SpeakerVerification_LibrispeechTrainClean100', 'SpeakerVerification_Voxceleb1Train', 'SpokenTermDetection_LibrispeechTrainClean100', 'SpeechDetection_LibrispeechTrainClean100', 'SpeakerVerification_Tedlium2Train', 'NoiseDetectionGaussian_VoxcelebMusan', 'SpeechTextMatching_LibrispeechTrainClean100', 'SpeechDetection_Aishell1Train', 'SpeechTextMatching_Tedlium2Train', 'SpeechDetection_Tedlium2Train', 'ReverberationDetectionSmallRoom_VoxcelebRirsNoises', 'SpeechDetection_Voxceleb1Train']
[21:48:44.981158] 108014
[21:48:44.981349] All train dataset size: 108014
[21:48:44.981417] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fcc8e7802e0>
/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[21:48:45.015989] Start training for 5 epochs
[21:48:45.367496] log_dir: ./output
[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[21:48:51.589108] Epoch: [0]  [    0/13501]  eta: 23:16:25  lr: 0.000000  closs: 6.6518 (6.6518)  mloss: 6.6518 (6.6518)  time: 6.2059  data: 2.4990  max mem: 29101
[21:48:57.966988] Epoch: [0]  [   10/13501]  eta: 4:17:12  lr: 0.000000  closs: 7.2689 (7.2368)  mloss: 7.2689 (7.2368)  time: 1.1439  data: 0.2274  max mem: 29573
[21:49:04.311686] Epoch: [0]  [   20/13501]  eta: 3:22:30  lr: 0.000000  closs: 7.6279 (7.5847)  mloss: 7.6279 (7.5847)  time: 0.6360  data: 0.0002  max mem: 29573
[21:49:10.597690] Epoch: [0]  [   30/13501]  eta: 3:02:35  lr: 0.000000  closs: 7.8606 (7.5742)  mloss: 7.8606 (7.5742)  time: 0.6315  data: 0.0002  max mem: 29573
[21:49:16.928885] Epoch: [0]  [   40/13501]  eta: 2:52:35  lr: 0.000000  closs: 7.2889 (7.4748)  mloss: 7.2889 (7.4748)  time: 0.6308  data: 0.0002  max mem: 29573
[21:49:23.216860] Epoch: [0]  [   50/13501]  eta: 2:46:17  lr: 0.000000  closs: 7.2889 (7.5298)  mloss: 7.2889 (7.5298)  time: 0.6309  data: 0.0002  max mem: 29573
[21:49:29.548915] Epoch: [0]  [   60/13501]  eta: 2:42:10  lr: 0.000000  closs: 7.6776 (7.5283)  mloss: 7.6776 (7.5283)  time: 0.6309  data: 0.0002  max mem: 29573
[21:49:35.832763] Epoch: [0]  [   70/13501]  eta: 2:39:02  lr: 0.000000  closs: 7.2452 (7.4906)  mloss: 7.2452 (7.4906)  time: 0.6307  data: 0.0002  max mem: 29573
[21:49:42.256774] Epoch: [0]  [   80/13501]  eta: 2:37:02  lr: 0.000000  closs: 7.4742 (7.4629)  mloss: 7.4742 (7.4629)  time: 0.6353  data: 0.0002  max mem: 29573
[21:49:48.542802] Epoch: [0]  [   90/13501]  eta: 2:35:07  lr: 0.000000  closs: 7.6435 (7.4973)  mloss: 7.6435 (7.4973)  time: 0.6354  data: 0.0002  max mem: 29573
[21:49:54.883738] Epoch: [0]  [  100/13501]  eta: 2:33:40  lr: 0.000000  closs: 7.7035 (7.5126)  mloss: 7.7035 (7.5126)  time: 0.6313  data: 0.0003  max mem: 29573
[21:50:01.173940] Epoch: [0]  [  110/13501]  eta: 2:32:22  lr: 0.000000  closs: 7.3891 (7.4912)  mloss: 7.3891 (7.4912)  time: 0.6315  data: 0.0002  max mem: 29573
[21:50:07.521038] Epoch: [0]  [  120/13501]  eta: 2:31:22  lr: 0.000001  closs: 6.9075 (7.4667)  mloss: 6.9075 (7.4667)  time: 0.6318  data: 0.0002  max mem: 29573
[21:50:13.815803] Epoch: [0]  [  130/13501]  eta: 2:30:25  lr: 0.000001  closs: 7.6487 (7.4983)  mloss: 7.6487 (7.4983)  time: 0.6320  data: 0.0003  max mem: 29573
[21:50:20.154354] Epoch: [0]  [  140/13501]  eta: 2:29:39  lr: 0.000001  closs: 7.7383 (7.5172)  mloss: 7.7383 (7.5172)  time: 0.6316  data: 0.0003  max mem: 29573
[21:50:26.438773] Epoch: [0]  [  150/13501]  eta: 2:28:54  lr: 0.000001  closs: 7.5524 (7.5097)  mloss: 7.5524 (7.5097)  time: 0.6311  data: 0.0002  max mem: 29573
[21:50:32.775440] Epoch: [0]  [  160/13501]  eta: 2:28:17  lr: 0.000001  closs: 7.5524 (7.5360)  mloss: 7.5524 (7.5360)  time: 0.6310  data: 0.0002  max mem: 29573
[21:50:39.056842] Epoch: [0]  [  170/13501]  eta: 2:27:40  lr: 0.000001  closs: 7.9112 (7.5409)  mloss: 7.9112 (7.5409)  time: 0.6308  data: 0.0002  max mem: 29573
[21:50:45.400185] Epoch: [0]  [  180/13501]  eta: 2:27:11  lr: 0.000001  closs: 7.6319 (7.5428)  mloss: 7.6319 (7.5428)  time: 0.6312  data: 0.0002  max mem: 29573
[21:50:51.686776] Epoch: [0]  [  190/13501]  eta: 2:26:41  lr: 0.000001  closs: 7.8106 (7.5358)  mloss: 7.8106 (7.5358)  time: 0.6314  data: 0.0002  max mem: 29573
[21:50:58.031937] Epoch: [0]  [  200/13501]  eta: 2:26:16  lr: 0.000001  closs: 7.1404 (7.5081)  mloss: 7.1404 (7.5081)  time: 0.6315  data: 0.0002  max mem: 29573
[21:51:04.316350] Epoch: [0]  [  210/13501]  eta: 2:25:50  lr: 0.000001  closs: 7.0742 (7.4864)  mloss: 7.0742 (7.4864)  time: 0.6314  data: 0.0002  max mem: 29573
[21:51:10.674526] Epoch: [0]  [  220/13501]  eta: 2:25:30  lr: 0.000001  closs: 7.1601 (7.4817)  mloss: 7.1601 (7.4817)  time: 0.6321  data: 0.0002  max mem: 29573
[21:51:16.969113] Epoch: [0]  [  230/13501]  eta: 2:25:07  lr: 0.000001  closs: 6.8288 (7.4616)  mloss: 6.8288 (7.4616)  time: 0.6326  data: 0.0002  max mem: 29573
[21:51:23.317301] Epoch: [0]  [  240/13501]  eta: 2:24:49  lr: 0.000001  closs: 7.0208 (7.4577)  mloss: 7.0208 (7.4577)  time: 0.6321  data: 0.0002  max mem: 29573
[21:51:29.599993] Epoch: [0]  [  250/13501]  eta: 2:24:28  lr: 0.000001  closs: 7.0535 (7.4373)  mloss: 7.0535 (7.4373)  time: 0.6315  data: 0.0002  max mem: 29573
[21:51:35.939566] Epoch: [0]  [  260/13501]  eta: 2:24:11  lr: 0.000001  closs: 6.9860 (7.4166)  mloss: 6.9860 (7.4166)  time: 0.6310  data: 0.0002  max mem: 29573
[21:51:42.225338] Epoch: [0]  [  270/13501]  eta: 2:23:52  lr: 0.000001  closs: 7.3289 (7.4198)  mloss: 7.3289 (7.4198)  time: 0.6312  data: 0.0003  max mem: 29573
[21:51:48.566608] Epoch: [0]  [  280/13501]  eta: 2:23:37  lr: 0.000001  closs: 7.2901 (7.3961)  mloss: 7.2901 (7.3961)  time: 0.6313  data: 0.0003  max mem: 29573
[21:51:54.862620] Epoch: [0]  [  290/13501]  eta: 2:23:21  lr: 0.000001  closs: 6.6657 (7.3739)  mloss: 6.6657 (7.3739)  time: 0.6318  data: 0.0002  max mem: 29573
[21:52:01.197072] Epoch: [0]  [  300/13501]  eta: 2:23:06  lr: 0.000001  closs: 6.6204 (7.3422)  mloss: 6.6204 (7.3422)  time: 0.6314  data: 0.0002  max mem: 29573
[21:52:07.482734] Epoch: [0]  [  310/13501]  eta: 2:22:50  lr: 0.000001  closs: 6.3901 (7.3133)  mloss: 6.3901 (7.3133)  time: 0.6309  data: 0.0002  max mem: 29573
[21:52:13.813373] Epoch: [0]  [  320/13501]  eta: 2:22:37  lr: 0.000001  closs: 6.3254 (7.2898)  mloss: 6.3254 (7.2898)  time: 0.6307  data: 0.0002  max mem: 29573
[21:52:20.112512] Epoch: [0]  [  330/13501]  eta: 2:22:23  lr: 0.000002  closs: 6.8227 (7.2669)  mloss: 6.8227 (7.2669)  time: 0.6314  data: 0.0002  max mem: 29573
[21:52:26.458425] Epoch: [0]  [  340/13501]  eta: 2:22:11  lr: 0.000002  closs: 6.8537 (7.2422)  mloss: 6.8537 (7.2422)  time: 0.6322  data: 0.0002  max mem: 29573
[21:52:32.748766] Epoch: [0]  [  350/13501]  eta: 2:21:57  lr: 0.000002  closs: 6.2609 (7.2076)  mloss: 6.2609 (7.2076)  time: 0.6317  data: 0.0002  max mem: 29573
[21:52:39.085161] Epoch: [0]  [  360/13501]  eta: 2:21:46  lr: 0.000002  closs: 6.0387 (7.1744)  mloss: 6.0387 (7.1744)  time: 0.6313  data: 0.0002  max mem: 29573
[21:52:45.379182] Epoch: [0]  [  370/13501]  eta: 2:21:33  lr: 0.000002  closs: 5.9265 (7.1372)  mloss: 5.9265 (7.1372)  time: 0.6314  data: 0.0002  max mem: 29573
[21:52:51.721278] Epoch: [0]  [  380/13501]  eta: 2:21:22  lr: 0.000002  closs: 5.8122 (7.1026)  mloss: 5.8122 (7.1026)  time: 0.6317  data: 0.0002  max mem: 29573
[21:52:58.000498] Epoch: [0]  [  390/13501]  eta: 2:21:09  lr: 0.000002  closs: 5.6940 (7.0645)  mloss: 5.6940 (7.0645)  time: 0.6310  data: 0.0002  max mem: 29573
[21:53:04.335842] Epoch: [0]  [  400/13501]  eta: 2:20:59  lr: 0.000002  closs: 5.6429 (7.0344)  mloss: 5.6429 (7.0344)  time: 0.6307  data: 0.0002  max mem: 29573
[21:53:10.622665] Epoch: [0]  [  410/13501]  eta: 2:20:47  lr: 0.000002  closs: 5.7430 (7.0038)  mloss: 5.7430 (7.0038)  time: 0.6310  data: 0.0002  max mem: 29573
[21:53:16.963383] Epoch: [0]  [  420/13501]  eta: 2:20:37  lr: 0.000002  closs: 5.7430 (6.9733)  mloss: 5.7430 (6.9733)  time: 0.6313  data: 0.0002  max mem: 29573
[21:53:23.259445] Epoch: [0]  [  430/13501]  eta: 2:20:26  lr: 0.000002  closs: 5.4530 (6.9361)  mloss: 5.4530 (6.9361)  time: 0.6318  data: 0.0002  max mem: 29573
[21:53:29.614444] Epoch: [0]  [  440/13501]  eta: 2:20:17  lr: 0.000002  closs: 5.3274 (6.8975)  mloss: 5.3274 (6.8975)  time: 0.6325  data: 0.0002  max mem: 29573
[21:53:35.906114] Epoch: [0]  [  450/13501]  eta: 2:20:06  lr: 0.000002  closs: 5.1401 (6.8579)  mloss: 5.1401 (6.8579)  time: 0.6323  data: 0.0002  max mem: 29573
[21:53:42.249796] Epoch: [0]  [  460/13501]  eta: 2:19:56  lr: 0.000002  closs: 4.9181 (6.8210)  mloss: 4.9181 (6.8210)  time: 0.6317  data: 0.0003  max mem: 29573
[21:53:48.543515] Epoch: [0]  [  470/13501]  eta: 2:19:46  lr: 0.000002  closs: 4.6466 (6.7748)  mloss: 4.6466 (6.7748)  time: 0.6318  data: 0.0003  max mem: 29573
[21:53:54.888633] Epoch: [0]  [  480/13501]  eta: 2:19:37  lr: 0.000002  closs: 4.8240 (6.7399)  mloss: 4.8240 (6.7399)  time: 0.6319  data: 0.0002  max mem: 29573
[21:54:01.164088] Epoch: [0]  [  490/13501]  eta: 2:19:26  lr: 0.000002  closs: 4.6225 (6.6913)  mloss: 4.6225 (6.6913)  time: 0.6309  data: 0.0002  max mem: 29573
[21:54:07.500677] Epoch: [0]  [  500/13501]  eta: 2:19:17  lr: 0.000002  closs: 4.4524 (6.6468)  mloss: 4.4524 (6.6468)  time: 0.6305  data: 0.0002  max mem: 29573
[21:54:13.790837] Epoch: [0]  [  510/13501]  eta: 2:19:07  lr: 0.000002  closs: 4.2495 (6.5994)  mloss: 4.2495 (6.5994)  time: 0.6313  data: 0.0002  max mem: 29573
[21:54:20.132872] Epoch: [0]  [  520/13501]  eta: 2:18:59  lr: 0.000002  closs: 4.1951 (6.5536)  mloss: 4.1951 (6.5536)  time: 0.6315  data: 0.0002  max mem: 29573
[21:54:26.423953] Epoch: [0]  [  530/13501]  eta: 2:18:49  lr: 0.000002  closs: 4.0569 (6.5075)  mloss: 4.0569 (6.5075)  time: 0.6316  data: 0.0002  max mem: 29573
[21:54:32.788085] Epoch: [0]  [  540/13501]  eta: 2:18:41  lr: 0.000002  closs: 4.0615 (6.4600)  mloss: 4.0615 (6.4600)  time: 0.6327  data: 0.0002  max mem: 29573
[21:54:39.085873] Epoch: [0]  [  550/13501]  eta: 2:18:32  lr: 0.000003  closs: 3.9103 (6.4121)  mloss: 3.9103 (6.4121)  time: 0.6330  data: 0.0003  max mem: 29573
[21:54:45.423657] Epoch: [0]  [  560/13501]  eta: 2:18:24  lr: 0.000003  closs: 3.7169 (6.3654)  mloss: 3.7169 (6.3654)  time: 0.6317  data: 0.0004  max mem: 29573
[21:54:51.708094] Epoch: [0]  [  570/13501]  eta: 2:18:14  lr: 0.000003  closs: 3.6768 (6.3183)  mloss: 3.6768 (6.3183)  time: 0.6310  data: 0.0003  max mem: 29573
[21:54:58.045056] Epoch: [0]  [  580/13501]  eta: 2:18:06  lr: 0.000003  closs: 3.7280 (6.2718)  mloss: 3.7280 (6.2718)  time: 0.6310  data: 0.0002  max mem: 29573
[21:55:04.337807] Epoch: [0]  [  590/13501]  eta: 2:17:57  lr: 0.000003  closs: 3.5904 (6.2211)  mloss: 3.5904 (6.2211)  time: 0.6314  data: 0.0002  max mem: 29573
[21:55:10.682330] Epoch: [0]  [  600/13501]  eta: 2:17:49  lr: 0.000003  closs: 3.2095 (6.1712)  mloss: 3.2095 (6.1712)  time: 0.6318  data: 0.0002  max mem: 29573
[21:55:16.973343] Epoch: [0]  [  610/13501]  eta: 2:17:40  lr: 0.000003  closs: 2.9964 (6.1146)  mloss: 2.9964 (6.1146)  time: 0.6317  data: 0.0002  max mem: 29573
[21:55:23.322030] Epoch: [0]  [  620/13501]  eta: 2:17:33  lr: 0.000003  closs: 2.8752 (6.0628)  mloss: 2.8752 (6.0628)  time: 0.6319  data: 0.0002  max mem: 29573
[21:55:29.613603] Epoch: [0]  [  630/13501]  eta: 2:17:24  lr: 0.000003  closs: 2.7800 (6.0102)  mloss: 2.7800 (6.0102)  time: 0.6319  data: 0.0002  max mem: 29573
[21:55:35.954023] Epoch: [0]  [  640/13501]  eta: 2:17:16  lr: 0.000003  closs: 2.3137 (5.9525)  mloss: 2.3137 (5.9525)  time: 0.6315  data: 0.0002  max mem: 29573
[21:55:42.263990] Epoch: [0]  [  650/13501]  eta: 2:17:08  lr: 0.000003  closs: 2.2239 (5.8949)  mloss: 2.2239 (5.8949)  time: 0.6324  data: 0.0002  max mem: 29573
[21:55:48.619327] Epoch: [0]  [  660/13501]  eta: 2:17:01  lr: 0.000003  closs: 2.1230 (5.8389)  mloss: 2.1230 (5.8389)  time: 0.6332  data: 0.0002  max mem: 29573
[21:55:54.900272] Epoch: [0]  [  670/13501]  eta: 2:16:52  lr: 0.000003  closs: 1.9683 (5.7813)  mloss: 1.9683 (5.7813)  time: 0.6317  data: 0.0002  max mem: 29573
[21:56:01.248450] Epoch: [0]  [  680/13501]  eta: 2:16:44  lr: 0.000003  closs: 1.8660 (5.7235)  mloss: 1.8660 (5.7235)  time: 0.6314  data: 0.0003  max mem: 29573
[21:56:07.537690] Epoch: [0]  [  690/13501]  eta: 2:16:36  lr: 0.000003  closs: 1.6517 (5.6629)  mloss: 1.6517 (5.6629)  time: 0.6318  data: 0.0003  max mem: 29573
[21:56:13.875925] Epoch: [0]  [  700/13501]  eta: 2:16:28  lr: 0.000003  closs: 1.4988 (5.6030)  mloss: 1.4988 (5.6030)  time: 0.6313  data: 0.0002  max mem: 29573
[21:56:20.161911] Epoch: [0]  [  710/13501]  eta: 2:16:20  lr: 0.000003  closs: 1.4436 (5.5438)  mloss: 1.4436 (5.5438)  time: 0.6311  data: 0.0002  max mem: 29573
[21:56:26.507785] Epoch: [0]  [  720/13501]  eta: 2:16:13  lr: 0.000003  closs: 1.2383 (5.4835)  mloss: 1.2383 (5.4835)  time: 0.6315  data: 0.0002  max mem: 29573
[21:56:32.795138] Epoch: [0]  [  730/13501]  eta: 2:16:04  lr: 0.000003  closs: 1.1141 (5.4236)  mloss: 1.1141 (5.4236)  time: 0.6316  data: 0.0002  max mem: 29573
[21:56:39.135275] Epoch: [0]  [  740/13501]  eta: 2:15:57  lr: 0.000003  closs: 1.0819 (5.3658)  mloss: 1.0819 (5.3658)  time: 0.6313  data: 0.0002  max mem: 29573
[21:56:45.428855] Epoch: [0]  [  750/13501]  eta: 2:15:49  lr: 0.000003  closs: 1.0708 (5.3089)  mloss: 1.0708 (5.3089)  time: 0.6316  data: 0.0002  max mem: 29573
[21:56:51.794696] Epoch: [0]  [  760/13501]  eta: 2:15:42  lr: 0.000004  closs: 1.0090 (5.2512)  mloss: 1.0090 (5.2512)  time: 0.6329  data: 0.0002  max mem: 29573
[21:56:58.104426] Epoch: [0]  [  770/13501]  eta: 2:15:34  lr: 0.000004  closs: 0.9128 (5.1955)  mloss: 0.9128 (5.1955)  time: 0.6337  data: 0.0002  max mem: 29573
[21:57:04.444183] Epoch: [0]  [  780/13501]  eta: 2:15:27  lr: 0.000004  closs: 0.8390 (5.1392)  mloss: 0.8390 (5.1392)  time: 0.6324  data: 0.0002  max mem: 29573
[21:57:10.737722] Epoch: [0]  [  790/13501]  eta: 2:15:19  lr: 0.000004  closs: 0.7594 (5.0839)  mloss: 0.7594 (5.0839)  time: 0.6316  data: 0.0002  max mem: 29573
[21:57:17.095302] Epoch: [0]  [  800/13501]  eta: 2:15:12  lr: 0.000004  closs: 0.7509 (5.0299)  mloss: 0.7509 (5.0299)  time: 0.6325  data: 0.0010  max mem: 29573
[21:57:23.376540] Epoch: [0]  [  810/13501]  eta: 2:15:04  lr: 0.000004  closs: 0.7150 (4.9764)  mloss: 0.7150 (4.9764)  time: 0.6319  data: 0.0009  max mem: 29573
[21:57:29.719463] Epoch: [0]  [  820/13501]  eta: 2:14:57  lr: 0.000004  closs: 0.6691 (4.9237)  mloss: 0.6691 (4.9237)  time: 0.6311  data: 0.0002  max mem: 29573
[21:57:35.998298] Epoch: [0]  [  830/13501]  eta: 2:14:49  lr: 0.000004  closs: 0.6181 (4.8719)  mloss: 0.6181 (4.8719)  time: 0.6310  data: 0.0002  max mem: 29573
[21:57:42.329065] Epoch: [0]  [  840/13501]  eta: 2:14:42  lr: 0.000004  closs: 0.6106 (4.8214)  mloss: 0.6106 (4.8214)  time: 0.6304  data: 0.0002  max mem: 29573
[21:57:48.621462] Epoch: [0]  [  850/13501]  eta: 2:14:34  lr: 0.000004  closs: 0.5872 (4.7712)  mloss: 0.5872 (4.7712)  time: 0.6311  data: 0.0002  max mem: 29573
[21:57:54.983432] Epoch: [0]  [  860/13501]  eta: 2:14:28  lr: 0.000004  closs: 0.5372 (4.7216)  mloss: 0.5372 (4.7216)  time: 0.6326  data: 0.0002  max mem: 29573
[21:58:01.287807] Epoch: [0]  [  870/13501]  eta: 2:14:20  lr: 0.000004  closs: 0.4717 (4.6730)  mloss: 0.4717 (4.6730)  time: 0.6332  data: 0.0002  max mem: 29573
[21:58:07.627996] Epoch: [0]  [  880/13501]  eta: 2:14:13  lr: 0.000004  closs: 0.4776 (4.6257)  mloss: 0.4776 (4.6257)  time: 0.6321  data: 0.0002  max mem: 29573
[21:58:13.916866] Epoch: [0]  [  890/13501]  eta: 2:14:05  lr: 0.000004  closs: 0.4679 (4.5784)  mloss: 0.4679 (4.5784)  time: 0.6314  data: 0.0002  max mem: 29573
[21:58:20.256147] Epoch: [0]  [  900/13501]  eta: 2:13:58  lr: 0.000004  closs: 0.4679 (4.5335)  mloss: 0.4679 (4.5335)  time: 0.6313  data: 0.0002  max mem: 29573
[21:58:26.537005] Epoch: [0]  [  910/13501]  eta: 2:13:51  lr: 0.000004  closs: 0.4291 (4.4882)  mloss: 0.4291 (4.4882)  time: 0.6309  data: 0.0002  max mem: 29573
[21:58:32.885083] Epoch: [0]  [  920/13501]  eta: 2:13:44  lr: 0.000004  closs: 0.4291 (4.4444)  mloss: 0.4291 (4.4444)  time: 0.6314  data: 0.0002  max mem: 29573
[21:58:39.168835] Epoch: [0]  [  930/13501]  eta: 2:13:36  lr: 0.000004  closs: 0.4464 (4.4014)  mloss: 0.4464 (4.4014)  time: 0.6315  data: 0.0002  max mem: 29573
[21:58:45.517122] Epoch: [0]  [  940/13501]  eta: 2:13:29  lr: 0.000004  closs: 0.4400 (4.3595)  mloss: 0.4400 (4.3595)  time: 0.6315  data: 0.0002  max mem: 29573
[21:58:51.808775] Epoch: [0]  [  950/13501]  eta: 2:13:22  lr: 0.000004  closs: 0.4400 (4.3181)  mloss: 0.4400 (4.3181)  time: 0.6319  data: 0.0002  max mem: 29573
[21:58:58.150661] Epoch: [0]  [  960/13501]  eta: 2:13:15  lr: 0.000004  closs: 0.4198 (4.2776)  mloss: 0.4198 (4.2776)  time: 0.6316  data: 0.0002  max mem: 29573
[21:59:04.462136] Epoch: [0]  [  970/13501]  eta: 2:13:08  lr: 0.000004  closs: 0.4340 (4.2383)  mloss: 0.4340 (4.2383)  time: 0.6326  data: 0.0002  max mem: 29573
[21:59:10.805072] Epoch: [0]  [  980/13501]  eta: 2:13:01  lr: 0.000005  closs: 0.4408 (4.1993)  mloss: 0.4408 (4.1993)  time: 0.6326  data: 0.0002  max mem: 29573
[21:59:17.102653] Epoch: [0]  [  990/13501]  eta: 2:12:54  lr: 0.000005  closs: 0.4416 (4.1616)  mloss: 0.4416 (4.1616)  time: 0.6319  data: 0.0002  max mem: 29573
[21:59:23.438773] Epoch: [0]  [ 1000/13501]  eta: 2:12:47  lr: 0.000005  closs: 0.4640 (4.1244)  mloss: 0.4640 (4.1244)  time: 0.6316  data: 0.0002  max mem: 29573
[21:59:29.716876] Epoch: [0]  [ 1010/13501]  eta: 2:12:39  lr: 0.000005  closs: 0.3941 (4.0878)  mloss: 0.3941 (4.0878)  time: 0.6306  data: 0.0002  max mem: 29573
[21:59:36.056421] Epoch: [0]  [ 1020/13501]  eta: 2:12:33  lr: 0.000005  closs: 0.3572 (4.0515)  mloss: 0.3572 (4.0515)  time: 0.6308  data: 0.0002  max mem: 29573
[21:59:42.343041] Epoch: [0]  [ 1030/13501]  eta: 2:12:25  lr: 0.000005  closs: 0.3479 (4.0159)  mloss: 0.3479 (4.0159)  time: 0.6312  data: 0.0002  max mem: 29573
[21:59:48.688527] Epoch: [0]  [ 1040/13501]  eta: 2:12:18  lr: 0.000005  closs: 0.3861 (3.9812)  mloss: 0.3861 (3.9812)  time: 0.6315  data: 0.0002  max mem: 29573
[21:59:54.986281] Epoch: [0]  [ 1050/13501]  eta: 2:12:11  lr: 0.000005  closs: 0.4117 (3.9473)  mloss: 0.4117 (3.9473)  time: 0.6321  data: 0.0002  max mem: 29573
[22:00:01.323684] Epoch: [0]  [ 1060/13501]  eta: 2:12:04  lr: 0.000005  closs: 0.3994 (3.9139)  mloss: 0.3994 (3.9139)  time: 0.6317  data: 0.0002  max mem: 29573
[22:00:07.629760] Epoch: [0]  [ 1070/13501]  eta: 2:11:57  lr: 0.000005  closs: 0.3640 (3.8811)  mloss: 0.3640 (3.8811)  time: 0.6321  data: 0.0002  max mem: 29573
[22:00:13.988124] Epoch: [0]  [ 1080/13501]  eta: 2:11:51  lr: 0.000005  closs: 0.3534 (3.8485)  mloss: 0.3534 (3.8485)  time: 0.6331  data: 0.0002  max mem: 29573
[22:00:20.280301] Epoch: [0]  [ 1090/13501]  eta: 2:11:44  lr: 0.000005  closs: 0.3671 (3.8170)  mloss: 0.3671 (3.8170)  time: 0.6325  data: 0.0002  max mem: 29573
[22:00:26.628568] Epoch: [0]  [ 1100/13501]  eta: 2:11:37  lr: 0.000005  closs: 0.4088 (3.7859)  mloss: 0.4088 (3.7859)  time: 0.6319  data: 0.0002  max mem: 29573
[22:00:32.914117] Epoch: [0]  [ 1110/13501]  eta: 2:11:30  lr: 0.000005  closs: 0.3924 (3.7553)  mloss: 0.3924 (3.7553)  time: 0.6316  data: 0.0002  max mem: 29573
[22:00:39.251676] Epoch: [0]  [ 1120/13501]  eta: 2:11:23  lr: 0.000005  closs: 0.3605 (3.7253)  mloss: 0.3605 (3.7253)  time: 0.6311  data: 0.0002  max mem: 29573
[22:00:45.537062] Epoch: [0]  [ 1130/13501]  eta: 2:11:16  lr: 0.000005  closs: 0.3440 (3.6954)  mloss: 0.3440 (3.6954)  time: 0.6311  data: 0.0002  max mem: 29573
[22:00:51.878101] Epoch: [0]  [ 1140/13501]  eta: 2:11:09  lr: 0.000005  closs: 0.3360 (3.6660)  mloss: 0.3360 (3.6660)  time: 0.6312  data: 0.0002  max mem: 29573
[22:00:58.163817] Epoch: [0]  [ 1150/13501]  eta: 2:11:02  lr: 0.000005  closs: 0.3794 (3.6376)  mloss: 0.3794 (3.6376)  time: 0.6313  data: 0.0002  max mem: 29573
[22:01:04.516100] Epoch: [0]  [ 1160/13501]  eta: 2:10:55  lr: 0.000005  closs: 0.3995 (3.6094)  mloss: 0.3995 (3.6094)  time: 0.6318  data: 0.0002  max mem: 29573
[22:01:10.793796] Epoch: [0]  [ 1170/13501]  eta: 2:10:48  lr: 0.000005  closs: 0.3779 (3.5819)  mloss: 0.3779 (3.5819)  time: 0.6314  data: 0.0002  max mem: 29573
[22:01:17.153039] Epoch: [0]  [ 1180/13501]  eta: 2:10:42  lr: 0.000005  closs: 0.3740 (3.5545)  mloss: 0.3740 (3.5545)  time: 0.6318  data: 0.0002  max mem: 29573
[22:01:23.446963] Epoch: [0]  [ 1190/13501]  eta: 2:10:34  lr: 0.000005  closs: 0.3706 (3.5279)  mloss: 0.3706 (3.5279)  time: 0.6326  data: 0.0002  max mem: 29573
[22:01:29.783659] Epoch: [0]  [ 1200/13501]  eta: 2:10:28  lr: 0.000006  closs: 0.3854 (3.5016)  mloss: 0.3854 (3.5016)  time: 0.6315  data: 0.0002  max mem: 29573
[22:01:36.073002] Epoch: [0]  [ 1210/13501]  eta: 2:10:21  lr: 0.000006  closs: 0.3930 (3.4760)  mloss: 0.3930 (3.4760)  time: 0.6312  data: 0.0002  max mem: 29573
[22:01:42.414173] Epoch: [0]  [ 1220/13501]  eta: 2:10:14  lr: 0.000006  closs: 0.3667 (3.4506)  mloss: 0.3667 (3.4506)  time: 0.6314  data: 0.0002  max mem: 29573
[22:01:48.700916] Epoch: [0]  [ 1230/13501]  eta: 2:10:07  lr: 0.000006  closs: 0.3667 (3.4259)  mloss: 0.3667 (3.4259)  time: 0.6313  data: 0.0003  max mem: 29573
[22:01:55.045733] Epoch: [0]  [ 1240/13501]  eta: 2:10:00  lr: 0.000006  closs: 0.3874 (3.4016)  mloss: 0.3874 (3.4016)  time: 0.6315  data: 0.0003  max mem: 29573
[22:02:01.338209] Epoch: [0]  [ 1250/13501]  eta: 2:09:53  lr: 0.000006  closs: 0.3638 (3.3774)  mloss: 0.3638 (3.3774)  time: 0.6318  data: 0.0002  max mem: 29573
[22:02:07.678993] Epoch: [0]  [ 1260/13501]  eta: 2:09:47  lr: 0.000006  closs: 0.3526 (3.3536)  mloss: 0.3526 (3.3536)  time: 0.6316  data: 0.0002  max mem: 29573
[22:02:13.970415] Epoch: [0]  [ 1270/13501]  eta: 2:09:40  lr: 0.000006  closs: 0.3497 (3.3301)  mloss: 0.3497 (3.3301)  time: 0.6315  data: 0.0002  max mem: 29573
[22:02:20.318165] Epoch: [0]  [ 1280/13501]  eta: 2:09:33  lr: 0.000006  closs: 0.3413 (3.3066)  mloss: 0.3413 (3.3066)  time: 0.6319  data: 0.0002  max mem: 29573
[22:02:26.635889] Epoch: [0]  [ 1290/13501]  eta: 2:09:26  lr: 0.000006  closs: 0.3318 (3.2836)  mloss: 0.3318 (3.2836)  time: 0.6332  data: 0.0002  max mem: 29573
[22:02:32.973683] Epoch: [0]  [ 1300/13501]  eta: 2:09:20  lr: 0.000006  closs: 0.3377 (3.2610)  mloss: 0.3377 (3.2610)  time: 0.6327  data: 0.0002  max mem: 29573
[22:02:39.255080] Epoch: [0]  [ 1310/13501]  eta: 2:09:13  lr: 0.000006  closs: 0.3934 (3.2393)  mloss: 0.3934 (3.2393)  time: 0.6309  data: 0.0002  max mem: 29573
[22:02:45.608093] Epoch: [0]  [ 1320/13501]  eta: 2:09:06  lr: 0.000006  closs: 0.3985 (3.2176)  mloss: 0.3985 (3.2176)  time: 0.6316  data: 0.0002  max mem: 29573
[22:02:51.890064] Epoch: [0]  [ 1330/13501]  eta: 2:08:59  lr: 0.000006  closs: 0.3945 (3.1967)  mloss: 0.3945 (3.1967)  time: 0.6317  data: 0.0002  max mem: 29573
[22:02:58.230014] Epoch: [0]  [ 1340/13501]  eta: 2:08:53  lr: 0.000006  closs: 0.3919 (3.1756)  mloss: 0.3919 (3.1756)  time: 0.6310  data: 0.0002  max mem: 29573
[22:03:04.505717] Epoch: [0]  [ 1350/13501]  eta: 2:08:46  lr: 0.000006  closs: 0.3680 (3.1549)  mloss: 0.3680 (3.1549)  time: 0.6307  data: 0.0002  max mem: 29573
[22:03:10.846051] Epoch: [0]  [ 1360/13501]  eta: 2:08:39  lr: 0.000006  closs: 0.3628 (3.1344)  mloss: 0.3628 (3.1344)  time: 0.6307  data: 0.0002  max mem: 29573
[22:03:17.133477] Epoch: [0]  [ 1370/13501]  eta: 2:08:32  lr: 0.000006  closs: 0.3216 (3.1139)  mloss: 0.3216 (3.1139)  time: 0.6313  data: 0.0002  max mem: 29573
[22:03:23.478009] Epoch: [0]  [ 1380/13501]  eta: 2:08:26  lr: 0.000006  closs: 0.3227 (3.0939)  mloss: 0.3227 (3.0939)  time: 0.6315  data: 0.0002  max mem: 29573
[22:03:29.785324] Epoch: [0]  [ 1390/13501]  eta: 2:08:19  lr: 0.000006  closs: 0.3288 (3.0742)  mloss: 0.3288 (3.0742)  time: 0.6325  data: 0.0002  max mem: 29573
[22:03:36.131188] Epoch: [0]  [ 1400/13501]  eta: 2:08:12  lr: 0.000006  closs: 0.3891 (3.0553)  mloss: 0.3891 (3.0553)  time: 0.6326  data: 0.0002  max mem: 29573
[22:03:42.443054] Epoch: [0]  [ 1410/13501]  eta: 2:08:06  lr: 0.000007  closs: 0.3674 (3.0361)  mloss: 0.3674 (3.0361)  time: 0.6328  data: 0.0002  max mem: 29573
[22:03:48.785260] Epoch: [0]  [ 1420/13501]  eta: 2:07:59  lr: 0.000007  closs: 0.3560 (3.0174)  mloss: 0.3560 (3.0174)  time: 0.6326  data: 0.0002  max mem: 29573
[22:03:55.128686] Epoch: [0]  [ 1430/13501]  eta: 2:07:53  lr: 0.000007  closs: 0.3357 (2.9985)  mloss: 0.3357 (2.9985)  time: 0.6342  data: 0.0002  max mem: 29573
[22:04:01.473078] Epoch: [0]  [ 1440/13501]  eta: 2:07:46  lr: 0.000007  closs: 0.3327 (2.9804)  mloss: 0.3327 (2.9804)  time: 0.6343  data: 0.0002  max mem: 29573
[22:04:07.757959] Epoch: [0]  [ 1450/13501]  eta: 2:07:39  lr: 0.000007  closs: 0.3721 (2.9624)  mloss: 0.3721 (2.9624)  time: 0.6314  data: 0.0002  max mem: 29573
[22:04:14.103735] Epoch: [0]  [ 1460/13501]  eta: 2:07:33  lr: 0.000007  closs: 0.3710 (2.9445)  mloss: 0.3710 (2.9445)  time: 0.6315  data: 0.0002  max mem: 29573
[22:04:20.387022] Epoch: [0]  [ 1470/13501]  eta: 2:07:26  lr: 0.000007  closs: 0.3614 (2.9271)  mloss: 0.3614 (2.9271)  time: 0.6314  data: 0.0002  max mem: 29573
[22:04:26.733263] Epoch: [0]  [ 1480/13501]  eta: 2:07:19  lr: 0.000007  closs: 0.3686 (2.9099)  mloss: 0.3686 (2.9099)  time: 0.6314  data: 0.0002  max mem: 29573
[22:04:33.021409] Epoch: [0]  [ 1490/13501]  eta: 2:07:12  lr: 0.000007  closs: 0.3681 (2.8929)  mloss: 0.3681 (2.8929)  time: 0.6316  data: 0.0002  max mem: 29573
[22:04:39.388855] Epoch: [0]  [ 1500/13501]  eta: 2:07:06  lr: 0.000007  closs: 0.3537 (2.8758)  mloss: 0.3537 (2.8758)  time: 0.6327  data: 0.0002  max mem: 29573
[22:04:45.684058] Epoch: [0]  [ 1510/13501]  eta: 2:06:59  lr: 0.000007  closs: 0.3546 (2.8592)  mloss: 0.3546 (2.8592)  time: 0.6331  data: 0.0002  max mem: 29573
[22:04:52.015420] Epoch: [0]  [ 1520/13501]  eta: 2:06:53  lr: 0.000007  closs: 0.3583 (2.8428)  mloss: 0.3583 (2.8428)  time: 0.6312  data: 0.0002  max mem: 29573
[22:04:58.293764] Epoch: [0]  [ 1530/13501]  eta: 2:06:46  lr: 0.000007  closs: 0.3667 (2.8267)  mloss: 0.3667 (2.8267)  time: 0.6304  data: 0.0002  max mem: 29573
[22:05:04.629753] Epoch: [0]  [ 1540/13501]  eta: 2:06:39  lr: 0.000007  closs: 0.3815 (2.8111)  mloss: 0.3815 (2.8111)  time: 0.6306  data: 0.0002  max mem: 29573
[22:05:10.919591] Epoch: [0]  [ 1550/13501]  eta: 2:06:32  lr: 0.000007  closs: 0.3628 (2.7953)  mloss: 0.3628 (2.7953)  time: 0.6312  data: 0.0002  max mem: 29573
[22:05:17.262507] Epoch: [0]  [ 1560/13501]  eta: 2:06:26  lr: 0.000007  closs: 0.3289 (2.7795)  mloss: 0.3289 (2.7795)  time: 0.6316  data: 0.0002  max mem: 29573
[22:05:23.561962] Epoch: [0]  [ 1570/13501]  eta: 2:06:19  lr: 0.000007  closs: 0.3187 (2.7640)  mloss: 0.3187 (2.7640)  time: 0.6320  data: 0.0002  max mem: 29573
[22:05:29.907984] Epoch: [0]  [ 1580/13501]  eta: 2:06:13  lr: 0.000007  closs: 0.3057 (2.7485)  mloss: 0.3057 (2.7485)  time: 0.6322  data: 0.0002  max mem: 29573
[22:05:36.195667] Epoch: [0]  [ 1590/13501]  eta: 2:06:06  lr: 0.000007  closs: 0.3448 (2.7333)  mloss: 0.3448 (2.7333)  time: 0.6316  data: 0.0002  max mem: 29573
[22:05:42.547591] Epoch: [0]  [ 1600/13501]  eta: 2:06:00  lr: 0.000007  closs: 0.3462 (2.7184)  mloss: 0.3462 (2.7184)  time: 0.6319  data: 0.0002  max mem: 29573
[22:05:48.854043] Epoch: [0]  [ 1610/13501]  eta: 2:05:53  lr: 0.000007  closs: 0.3637 (2.7039)  mloss: 0.3637 (2.7039)  time: 0.6328  data: 0.0002  max mem: 29573
[22:05:55.191425] Epoch: [0]  [ 1620/13501]  eta: 2:05:46  lr: 0.000007  closs: 0.3484 (2.6893)  mloss: 0.3484 (2.6893)  time: 0.6321  data: 0.0002  max mem: 29573
[22:06:01.483979] Epoch: [0]  [ 1630/13501]  eta: 2:05:40  lr: 0.000008  closs: 0.3284 (2.6747)  mloss: 0.3284 (2.6747)  time: 0.6314  data: 0.0002  max mem: 29573
[22:06:07.822406] Epoch: [0]  [ 1640/13501]  eta: 2:05:33  lr: 0.000008  closs: 0.3284 (2.6605)  mloss: 0.3284 (2.6605)  time: 0.6315  data: 0.0002  max mem: 29573
[22:06:14.116285] Epoch: [0]  [ 1650/13501]  eta: 2:05:26  lr: 0.000008  closs: 0.3193 (2.6464)  mloss: 0.3193 (2.6464)  time: 0.6315  data: 0.0002  max mem: 29573
[22:06:20.463986] Epoch: [0]  [ 1660/13501]  eta: 2:05:20  lr: 0.000008  closs: 0.3215 (2.6326)  mloss: 0.3215 (2.6326)  time: 0.6320  data: 0.0002  max mem: 29573
[22:06:26.758127] Epoch: [0]  [ 1670/13501]  eta: 2:05:13  lr: 0.000008  closs: 0.3316 (2.6190)  mloss: 0.3316 (2.6190)  time: 0.6320  data: 0.0002  max mem: 29573
[22:06:33.095791] Epoch: [0]  [ 1680/13501]  eta: 2:05:07  lr: 0.000008  closs: 0.3330 (2.6054)  mloss: 0.3330 (2.6054)  time: 0.6315  data: 0.0002  max mem: 29573
[22:06:39.382717] Epoch: [0]  [ 1690/13501]  eta: 2:05:00  lr: 0.000008  closs: 0.3735 (2.5924)  mloss: 0.3735 (2.5924)  time: 0.6311  data: 0.0002  max mem: 29573
[22:06:45.710441] Epoch: [0]  [ 1700/13501]  eta: 2:04:54  lr: 0.000008  closs: 0.3562 (2.5793)  mloss: 0.3562 (2.5793)  time: 0.6307  data: 0.0002  max mem: 29573
[22:06:52.012077] Epoch: [0]  [ 1710/13501]  eta: 2:04:47  lr: 0.000008  closs: 0.3517 (2.5662)  mloss: 0.3517 (2.5662)  time: 0.6314  data: 0.0002  max mem: 29573
[22:06:58.372515] Epoch: [0]  [ 1720/13501]  eta: 2:04:41  lr: 0.000008  closs: 0.3372 (2.5532)  mloss: 0.3372 (2.5532)  time: 0.6330  data: 0.0002  max mem: 29573
[22:07:04.655089] Epoch: [0]  [ 1730/13501]  eta: 2:04:34  lr: 0.000008  closs: 0.3240 (2.5403)  mloss: 0.3240 (2.5403)  time: 0.6321  data: 0.0002  max mem: 29573
[22:07:11.004142] Epoch: [0]  [ 1740/13501]  eta: 2:04:27  lr: 0.000008  closs: 0.3439 (2.5278)  mloss: 0.3439 (2.5278)  time: 0.6315  data: 0.0002  max mem: 29573
[22:07:17.293383] Epoch: [0]  [ 1750/13501]  eta: 2:04:21  lr: 0.000008  closs: 0.3552 (2.5154)  mloss: 0.3552 (2.5154)  time: 0.6318  data: 0.0002  max mem: 29573
[22:07:23.633981] Epoch: [0]  [ 1760/13501]  eta: 2:04:14  lr: 0.000008  closs: 0.3552 (2.5031)  mloss: 0.3552 (2.5031)  time: 0.6314  data: 0.0002  max mem: 29573
[22:07:29.928173] Epoch: [0]  [ 1770/13501]  eta: 2:04:08  lr: 0.000008  closs: 0.3263 (2.4909)  mloss: 0.3263 (2.4909)  time: 0.6317  data: 0.0002  max mem: 29573
[22:07:36.268732] Epoch: [0]  [ 1780/13501]  eta: 2:04:01  lr: 0.000008  closs: 0.3263 (2.4787)  mloss: 0.3263 (2.4787)  time: 0.6317  data: 0.0002  max mem: 29573
[22:07:42.560135] Epoch: [0]  [ 1790/13501]  eta: 2:03:54  lr: 0.000008  closs: 0.3289 (2.4668)  mloss: 0.3289 (2.4668)  time: 0.6315  data: 0.0002  max mem: 29573
[22:07:48.895930] Epoch: [0]  [ 1800/13501]  eta: 2:03:48  lr: 0.000008  closs: 0.3489 (2.4552)  mloss: 0.3489 (2.4552)  time: 0.6313  data: 0.0002  max mem: 29573
[22:07:55.189901] Epoch: [0]  [ 1810/13501]  eta: 2:03:41  lr: 0.000008  closs: 0.3701 (2.4438)  mloss: 0.3701 (2.4438)  time: 0.6314  data: 0.0002  max mem: 29573
[22:08:01.549604] Epoch: [0]  [ 1820/13501]  eta: 2:03:35  lr: 0.000008  closs: 0.3524 (2.4323)  mloss: 0.3524 (2.4323)  time: 0.6326  data: 0.0002  max mem: 29573
[22:08:07.839926] Epoch: [0]  [ 1830/13501]  eta: 2:03:28  lr: 0.000008  closs: 0.3162 (2.4209)  mloss: 0.3162 (2.4209)  time: 0.6324  data: 0.0002  max mem: 29573
[22:08:14.181380] Epoch: [0]  [ 1840/13501]  eta: 2:03:22  lr: 0.000009  closs: 0.3066 (2.4094)  mloss: 0.3066 (2.4094)  time: 0.6315  data: 0.0002  max mem: 29573
[22:08:20.467932] Epoch: [0]  [ 1850/13501]  eta: 2:03:15  lr: 0.000009  closs: 0.3264 (2.3983)  mloss: 0.3264 (2.3983)  time: 0.6313  data: 0.0002  max mem: 29573
[22:08:26.809643] Epoch: [0]  [ 1860/13501]  eta: 2:03:09  lr: 0.000009  closs: 0.3271 (2.3873)  mloss: 0.3271 (2.3873)  time: 0.6313  data: 0.0002  max mem: 29573
[22:08:33.090813] Epoch: [0]  [ 1870/13501]  eta: 2:03:02  lr: 0.000009  closs: 0.3752 (2.3766)  mloss: 0.3752 (2.3766)  time: 0.6311  data: 0.0002  max mem: 29573
[22:08:39.432642] Epoch: [0]  [ 1880/13501]  eta: 2:02:56  lr: 0.000009  closs: 0.3767 (2.3660)  mloss: 0.3767 (2.3660)  time: 0.6311  data: 0.0002  max mem: 29573
[22:08:45.721804] Epoch: [0]  [ 1890/13501]  eta: 2:02:49  lr: 0.000009  closs: 0.3741 (2.3554)  mloss: 0.3741 (2.3554)  time: 0.6315  data: 0.0002  max mem: 29573
[22:08:52.057635] Epoch: [0]  [ 1900/13501]  eta: 2:02:42  lr: 0.000009  closs: 0.3741 (2.3449)  mloss: 0.3741 (2.3449)  time: 0.6312  data: 0.0002  max mem: 29573
[22:08:58.355736] Epoch: [0]  [ 1910/13501]  eta: 2:02:36  lr: 0.000009  closs: 0.3666 (2.3345)  mloss: 0.3666 (2.3345)  time: 0.6316  data: 0.0002  max mem: 29573
[22:09:04.704362] Epoch: [0]  [ 1920/13501]  eta: 2:02:29  lr: 0.000009  closs: 0.3530 (2.3242)  mloss: 0.3530 (2.3242)  time: 0.6323  data: 0.0002  max mem: 29573
[22:09:11.007383] Epoch: [0]  [ 1930/13501]  eta: 2:02:23  lr: 0.000009  closs: 0.3471 (2.3140)  mloss: 0.3471 (2.3140)  time: 0.6325  data: 0.0002  max mem: 29573
[22:09:17.349693] Epoch: [0]  [ 1940/13501]  eta: 2:02:16  lr: 0.000009  closs: 0.3282 (2.3038)  mloss: 0.3282 (2.3038)  time: 0.6322  data: 0.0002  max mem: 29573
[22:09:23.635690] Epoch: [0]  [ 1950/13501]  eta: 2:02:10  lr: 0.000009  closs: 0.3454 (2.2937)  mloss: 0.3454 (2.2937)  time: 0.6313  data: 0.0002  max mem: 29573
[22:09:29.976301] Epoch: [0]  [ 1960/13501]  eta: 2:02:03  lr: 0.000009  closs: 0.3505 (2.2837)  mloss: 0.3505 (2.2837)  time: 0.6313  data: 0.0002  max mem: 29573
[22:09:36.262415] Epoch: [0]  [ 1970/13501]  eta: 2:01:57  lr: 0.000009  closs: 0.3495 (2.2739)  mloss: 0.3495 (2.2739)  time: 0.6313  data: 0.0002  max mem: 29573
[22:09:42.611017] Epoch: [0]  [ 1980/13501]  eta: 2:01:50  lr: 0.000009  closs: 0.3567 (2.2643)  mloss: 0.3567 (2.2643)  time: 0.6317  data: 0.0003  max mem: 29573
[22:09:48.897000] Epoch: [0]  [ 1990/13501]  eta: 2:01:44  lr: 0.000009  closs: 0.3391 (2.2547)  mloss: 0.3391 (2.2547)  time: 0.6317  data: 0.0003  max mem: 29573
[22:09:55.238757] Epoch: [0]  [ 2000/13501]  eta: 2:01:37  lr: 0.000009  closs: 0.3598 (2.2454)  mloss: 0.3598 (2.2454)  time: 0.6313  data: 0.0002  max mem: 29573
[22:10:01.531217] Epoch: [0]  [ 2010/13501]  eta: 2:01:31  lr: 0.000009  closs: 0.3713 (2.2360)  mloss: 0.3713 (2.2360)  time: 0.6316  data: 0.0002  max mem: 29573
[22:10:07.877098] Epoch: [0]  [ 2020/13501]  eta: 2:01:24  lr: 0.000009  closs: 0.3643 (2.2267)  mloss: 0.3643 (2.2267)  time: 0.6318  data: 0.0002  max mem: 29573
[22:10:14.176114] Epoch: [0]  [ 2030/13501]  eta: 2:01:18  lr: 0.000009  closs: 0.3222 (2.2172)  mloss: 0.3222 (2.2172)  time: 0.6322  data: 0.0002  max mem: 29573
[22:10:20.521886] Epoch: [0]  [ 2040/13501]  eta: 2:01:11  lr: 0.000009  closs: 0.3208 (2.2080)  mloss: 0.3208 (2.2080)  time: 0.6322  data: 0.0002  max mem: 29573
[22:10:26.807961] Epoch: [0]  [ 2050/13501]  eta: 2:01:05  lr: 0.000009  closs: 0.3208 (2.1990)  mloss: 0.3208 (2.1990)  time: 0.6315  data: 0.0002  max mem: 29573
[22:10:33.151189] Epoch: [0]  [ 2060/13501]  eta: 2:00:58  lr: 0.000010  closs: 0.3363 (2.1900)  mloss: 0.3363 (2.1900)  time: 0.6314  data: 0.0002  max mem: 29573
[22:10:39.457645] Epoch: [0]  [ 2070/13501]  eta: 2:00:52  lr: 0.000010  closs: 0.3376 (2.1812)  mloss: 0.3376 (2.1812)  time: 0.6324  data: 0.0002  max mem: 29573
[22:10:45.798800] Epoch: [0]  [ 2080/13501]  eta: 2:00:45  lr: 0.000010  closs: 0.3323 (2.1722)  mloss: 0.3323 (2.1722)  time: 0.6323  data: 0.0002  max mem: 29573
[22:10:52.086864] Epoch: [0]  [ 2090/13501]  eta: 2:00:39  lr: 0.000010  closs: 0.3307 (2.1633)  mloss: 0.3307 (2.1633)  time: 0.6314  data: 0.0002  max mem: 29573
[22:10:58.429347] Epoch: [0]  [ 2100/13501]  eta: 2:00:32  lr: 0.000010  closs: 0.3307 (2.1547)  mloss: 0.3307 (2.1547)  time: 0.6315  data: 0.0002  max mem: 29573
[22:11:04.721859] Epoch: [0]  [ 2110/13501]  eta: 2:00:26  lr: 0.000010  closs: 0.3597 (2.1462)  mloss: 0.3597 (2.1462)  time: 0.6317  data: 0.0002  max mem: 29573
[22:11:11.060710] Epoch: [0]  [ 2120/13501]  eta: 2:00:19  lr: 0.000010  closs: 0.3718 (2.1378)  mloss: 0.3718 (2.1378)  time: 0.6315  data: 0.0002  max mem: 29573
[22:11:17.357927] Epoch: [0]  [ 2130/13501]  eta: 2:00:13  lr: 0.000010  closs: 0.3709 (2.1294)  mloss: 0.3709 (2.1294)  time: 0.6317  data: 0.0002  max mem: 29573
[22:11:23.722464] Epoch: [0]  [ 2140/13501]  eta: 2:00:07  lr: 0.000010  closs: 0.3559 (2.1210)  mloss: 0.3559 (2.1210)  time: 0.6330  data: 0.0002  max mem: 29573
[22:11:30.013163] Epoch: [0]  [ 2150/13501]  eta: 2:00:00  lr: 0.000010  closs: 0.3451 (2.1127)  mloss: 0.3451 (2.1127)  time: 0.6327  data: 0.0002  max mem: 29573
[22:11:36.357380] Epoch: [0]  [ 2160/13501]  eta: 1:59:54  lr: 0.000010  closs: 0.3457 (2.1045)  mloss: 0.3457 (2.1045)  time: 0.6317  data: 0.0002  max mem: 29573
[22:11:42.641024] Epoch: [0]  [ 2170/13501]  eta: 1:59:47  lr: 0.000010  closs: 0.3775 (2.0966)  mloss: 0.3775 (2.0966)  time: 0.6313  data: 0.0002  max mem: 29573
[22:11:48.984247] Epoch: [0]  [ 2180/13501]  eta: 1:59:41  lr: 0.000010  closs: 0.3802 (2.0887)  mloss: 0.3802 (2.0887)  time: 0.6313  data: 0.0002  max mem: 29573
[22:11:55.276903] Epoch: [0]  [ 2190/13501]  eta: 1:59:34  lr: 0.000010  closs: 0.3371 (2.0806)  mloss: 0.3371 (2.0806)  time: 0.6317  data: 0.0002  max mem: 29573
[22:12:01.623052] Epoch: [0]  [ 2200/13501]  eta: 1:59:28  lr: 0.000010  closs: 0.3000 (2.0726)  mloss: 0.3000 (2.0726)  time: 0.6319  data: 0.0002  max mem: 29573
[22:12:07.905565] Epoch: [0]  [ 2210/13501]  eta: 1:59:21  lr: 0.000010  closs: 0.3377 (2.0650)  mloss: 0.3377 (2.0650)  time: 0.6314  data: 0.0002  max mem: 29573
[22:12:14.256131] Epoch: [0]  [ 2220/13501]  eta: 1:59:15  lr: 0.000010  closs: 0.3660 (2.0573)  mloss: 0.3660 (2.0573)  time: 0.6316  data: 0.0002  max mem: 29573
[22:12:20.553467] Epoch: [0]  [ 2230/13501]  eta: 1:59:08  lr: 0.000010  closs: 0.3723 (2.0499)  mloss: 0.3723 (2.0499)  time: 0.6323  data: 0.0003  max mem: 29573
[22:12:26.899974] Epoch: [0]  [ 2240/13501]  eta: 1:59:02  lr: 0.000010  closs: 0.3388 (2.0423)  mloss: 0.3388 (2.0423)  time: 0.6321  data: 0.0003  max mem: 29573
[22:12:33.241164] Epoch: [0]  [ 2250/13501]  eta: 1:58:55  lr: 0.000010  closs: 0.3202 (2.0347)  mloss: 0.3202 (2.0347)  time: 0.6343  data: 0.0002  max mem: 29573
[22:12:39.583726] Epoch: [0]  [ 2260/13501]  eta: 1:58:49  lr: 0.000010  closs: 0.3253 (2.0273)  mloss: 0.3253 (2.0273)  time: 0.6341  data: 0.0002  max mem: 29573
[22:12:45.878384] Epoch: [0]  [ 2270/13501]  eta: 1:58:42  lr: 0.000010  closs: 0.3253 (2.0199)  mloss: 0.3253 (2.0199)  time: 0.6318  data: 0.0002  max mem: 29573
[22:12:52.218513] Epoch: [0]  [ 2280/13501]  eta: 1:58:36  lr: 0.000011  closs: 0.3288 (2.0126)  mloss: 0.3288 (2.0126)  time: 0.6317  data: 0.0002  max mem: 29573
[22:12:58.508853] Epoch: [0]  [ 2290/13501]  eta: 1:58:29  lr: 0.000011  closs: 0.3435 (2.0054)  mloss: 0.3435 (2.0054)  time: 0.6314  data: 0.0002  max mem: 29573
[22:13:04.845167] Epoch: [0]  [ 2300/13501]  eta: 1:58:23  lr: 0.000011  closs: 0.3300 (1.9982)  mloss: 0.3300 (1.9982)  time: 0.6313  data: 0.0002  max mem: 29573
[22:13:11.130362] Epoch: [0]  [ 2310/13501]  eta: 1:58:17  lr: 0.000011  closs: 0.4025 (1.9912)  mloss: 0.4025 (1.9912)  time: 0.6310  data: 0.0002  max mem: 29573
[22:13:17.479148] Epoch: [0]  [ 2320/13501]  eta: 1:58:10  lr: 0.000011  closs: 0.3734 (1.9841)  mloss: 0.3734 (1.9841)  time: 0.6316  data: 0.0002  max mem: 29573
[22:13:23.765499] Epoch: [0]  [ 2330/13501]  eta: 1:58:04  lr: 0.000011  closs: 0.3273 (1.9771)  mloss: 0.3273 (1.9771)  time: 0.6317  data: 0.0002  max mem: 29573
[22:13:30.107517] Epoch: [0]  [ 2340/13501]  eta: 1:57:57  lr: 0.000011  closs: 0.3178 (1.9701)  mloss: 0.3178 (1.9701)  time: 0.6313  data: 0.0002  max mem: 29573
[22:13:33.267674] /work/u8915687/big-superb/big-superb-train-data/SpeakerVerification_Aishell1Train/train/BAC009S0423W0203.wav
[22:13:36.418813] Epoch: [0]  [ 2350/13501]  eta: 1:57:51  lr: 0.000011  closs: 0.3178 (1.9633)  mloss: 0.3178 (1.9633)  time: 0.6326  data: 0.0002  max mem: 29573
[22:13:42.769295] Epoch: [0]  [ 2360/13501]  eta: 1:57:44  lr: 0.000011  closs: 0.3274 (1.9563)  mloss: 0.3274 (1.9563)  time: 0.6330  data: 0.0002  max mem: 29573
[22:13:49.046581] Epoch: [0]  [ 2370/13501]  eta: 1:57:38  lr: 0.000011  closs: 0.3522 (1.9498)  mloss: 0.3522 (1.9498)  time: 0.6313  data: 0.0002  max mem: 29573
[22:13:55.386749] Epoch: [0]  [ 2380/13501]  eta: 1:57:31  lr: 0.000011  closs: 0.3807 (1.9431)  mloss: 0.3807 (1.9431)  time: 0.6308  data: 0.0002  max mem: 29573
[22:14:01.669864] Epoch: [0]  [ 2390/13501]  eta: 1:57:25  lr: 0.000011  closs: 0.3344 (1.9363)  mloss: 0.3344 (1.9363)  time: 0.6311  data: 0.0002  max mem: 29573
[22:14:08.011720] Epoch: [0]  [ 2400/13501]  eta: 1:57:18  lr: 0.000011  closs: 0.3288 (1.9298)  mloss: 0.3288 (1.9298)  time: 0.6312  data: 0.0002  max mem: 29573
[22:14:14.299811] Epoch: [0]  [ 2410/13501]  eta: 1:57:12  lr: 0.000011  closs: 0.3288 (1.9232)  mloss: 0.3288 (1.9232)  time: 0.6314  data: 0.0002  max mem: 29573
[22:14:20.643642] Epoch: [0]  [ 2420/13501]  eta: 1:57:06  lr: 0.000011  closs: 0.3241 (1.9168)  mloss: 0.3241 (1.9168)  time: 0.6315  data: 0.0002  max mem: 29573
[22:14:26.931943] Epoch: [0]  [ 2430/13501]  eta: 1:56:59  lr: 0.000011  closs: 0.3180 (1.9102)  mloss: 0.3180 (1.9102)  time: 0.6315  data: 0.0002  max mem: 29573
[22:14:33.265821] Epoch: [0]  [ 2440/13501]  eta: 1:56:53  lr: 0.000011  closs: 0.3094 (1.9038)  mloss: 0.3094 (1.9038)  time: 0.6310  data: 0.0002  max mem: 29573
[22:14:39.561188] Epoch: [0]  [ 2450/13501]  eta: 1:56:46  lr: 0.000011  closs: 0.3341 (1.8975)  mloss: 0.3341 (1.8975)  time: 0.6314  data: 0.0002  max mem: 29573
[22:14:45.916590] Epoch: [0]  [ 2460/13501]  eta: 1:56:40  lr: 0.000011  closs: 0.3654 (1.8915)  mloss: 0.3654 (1.8915)  time: 0.6325  data: 0.0002  max mem: 29573
[22:14:52.207459] Epoch: [0]  [ 2470/13501]  eta: 1:56:33  lr: 0.000011  closs: 0.3747 (1.8853)  mloss: 0.3747 (1.8853)  time: 0.6322  data: 0.0002  max mem: 29573
[22:14:58.553409] Epoch: [0]  [ 2480/13501]  eta: 1:56:27  lr: 0.000011  closs: 0.3257 (1.8790)  mloss: 0.3257 (1.8790)  time: 0.6318  data: 0.0003  max mem: 29573
[22:15:04.834994] Epoch: [0]  [ 2490/13501]  eta: 1:56:20  lr: 0.000012  closs: 0.3360 (1.8729)  mloss: 0.3360 (1.8729)  time: 0.6313  data: 0.0003  max mem: 29573
[22:15:11.173714] Epoch: [0]  [ 2500/13501]  eta: 1:56:14  lr: 0.000012  closs: 0.3724 (1.8670)  mloss: 0.3724 (1.8670)  time: 0.6309  data: 0.0002  max mem: 29573
[22:15:17.465255] Epoch: [0]  [ 2510/13501]  eta: 1:56:07  lr: 0.000012  closs: 0.3372 (1.8609)  mloss: 0.3372 (1.8609)  time: 0.6314  data: 0.0002  max mem: 29573
[22:15:23.806297] Epoch: [0]  [ 2520/13501]  eta: 1:56:01  lr: 0.000012  closs: 0.3372 (1.8549)  mloss: 0.3372 (1.8549)  time: 0.6316  data: 0.0002  max mem: 29573
[22:15:30.102221] Epoch: [0]  [ 2530/13501]  eta: 1:55:55  lr: 0.000012  closs: 0.3470 (1.8491)  mloss: 0.3470 (1.8491)  time: 0.6318  data: 0.0002  max mem: 29573
[22:15:36.452818] Epoch: [0]  [ 2540/13501]  eta: 1:55:48  lr: 0.000012  closs: 0.3284 (1.8431)  mloss: 0.3284 (1.8431)  time: 0.6322  data: 0.0002  max mem: 29573
[22:15:42.742209] Epoch: [0]  [ 2550/13501]  eta: 1:55:42  lr: 0.000012  closs: 0.3153 (1.8373)  mloss: 0.3153 (1.8373)  time: 0.6319  data: 0.0002  max mem: 29573
[22:15:49.103160] Epoch: [0]  [ 2560/13501]  eta: 1:55:35  lr: 0.000012  closs: 0.3255 (1.8314)  mloss: 0.3255 (1.8314)  time: 0.6324  data: 0.0002  max mem: 29573
[22:15:55.402213] Epoch: [0]  [ 2570/13501]  eta: 1:55:29  lr: 0.000012  closs: 0.3251 (1.8256)  mloss: 0.3251 (1.8256)  time: 0.6329  data: 0.0002  max mem: 29573
[22:16:01.746114] Epoch: [0]  [ 2580/13501]  eta: 1:55:23  lr: 0.000012  closs: 0.3215 (1.8198)  mloss: 0.3215 (1.8198)  time: 0.6321  data: 0.0002  max mem: 29573
[22:16:08.030431] Epoch: [0]  [ 2590/13501]  eta: 1:55:16  lr: 0.000012  closs: 0.3374 (1.8142)  mloss: 0.3374 (1.8142)  time: 0.6313  data: 0.0002  max mem: 29573
[22:16:14.370896] Epoch: [0]  [ 2600/13501]  eta: 1:55:10  lr: 0.000012  closs: 0.3364 (1.8086)  mloss: 0.3364 (1.8086)  time: 0.6312  data: 0.0002  max mem: 29573
[22:16:20.659885] Epoch: [0]  [ 2610/13501]  eta: 1:55:03  lr: 0.000012  closs: 0.3400 (1.8031)  mloss: 0.3400 (1.8031)  time: 0.6314  data: 0.0002  max mem: 29573
[22:16:27.043201] Epoch: [0]  [ 2620/13501]  eta: 1:54:57  lr: 0.000012  closs: 0.3400 (1.7975)  mloss: 0.3400 (1.7975)  time: 0.6335  data: 0.0002  max mem: 29573
[22:16:33.329932] Epoch: [0]  [ 2630/13501]  eta: 1:54:50  lr: 0.000012  closs: 0.3280 (1.7920)  mloss: 0.3280 (1.7920)  time: 0.6334  data: 0.0002  max mem: 29573
[22:16:39.677253] Epoch: [0]  [ 2640/13501]  eta: 1:54:44  lr: 0.000012  closs: 0.3441 (1.7866)  mloss: 0.3441 (1.7866)  time: 0.6316  data: 0.0002  max mem: 29573
[22:16:45.965466] Epoch: [0]  [ 2650/13501]  eta: 1:54:38  lr: 0.000012  closs: 0.3240 (1.7810)  mloss: 0.3240 (1.7810)  time: 0.6317  data: 0.0002  max mem: 29573
[22:16:52.319839] Epoch: [0]  [ 2660/13501]  eta: 1:54:31  lr: 0.000012  closs: 0.3043 (1.7755)  mloss: 0.3043 (1.7755)  time: 0.6321  data: 0.0002  max mem: 29573
[22:16:58.623643] Epoch: [0]  [ 2670/13501]  eta: 1:54:25  lr: 0.000012  closs: 0.3205 (1.7703)  mloss: 0.3205 (1.7703)  time: 0.6328  data: 0.0002  max mem: 29573
[22:17:04.978874] Epoch: [0]  [ 2680/13501]  eta: 1:54:19  lr: 0.000012  closs: 0.3797 (1.7651)  mloss: 0.3797 (1.7651)  time: 0.6329  data: 0.0002  max mem: 29573
[22:17:11.272106] Epoch: [0]  [ 2690/13501]  eta: 1:54:12  lr: 0.000012  closs: 0.3485 (1.7598)  mloss: 0.3485 (1.7598)  time: 0.6323  data: 0.0002  max mem: 29573
[22:17:17.607834] Epoch: [0]  [ 2700/13501]  eta: 1:54:06  lr: 0.000012  closs: 0.3485 (1.7547)  mloss: 0.3485 (1.7547)  time: 0.6314  data: 0.0002  max mem: 29573
[22:17:23.889172] Epoch: [0]  [ 2710/13501]  eta: 1:53:59  lr: 0.000013  closs: 0.3583 (1.7496)  mloss: 0.3583 (1.7496)  time: 0.6308  data: 0.0002  max mem: 29573
[22:17:30.227031] Epoch: [0]  [ 2720/13501]  eta: 1:53:53  lr: 0.000013  closs: 0.3535 (1.7444)  mloss: 0.3535 (1.7444)  time: 0.6309  data: 0.0002  max mem: 29573
[22:17:36.510981] Epoch: [0]  [ 2730/13501]  eta: 1:53:46  lr: 0.000013  closs: 0.3700 (1.7394)  mloss: 0.3700 (1.7394)  time: 0.6310  data: 0.0002  max mem: 29573
[22:17:42.848597] Epoch: [0]  [ 2740/13501]  eta: 1:53:40  lr: 0.000013  closs: 0.3338 (1.7343)  mloss: 0.3338 (1.7343)  time: 0.6310  data: 0.0002  max mem: 29573
[22:17:49.141331] Epoch: [0]  [ 2750/13501]  eta: 1:53:33  lr: 0.000013  closs: 0.3237 (1.7291)  mloss: 0.3237 (1.7291)  time: 0.6314  data: 0.0002  max mem: 29573
[22:17:55.480130] Epoch: [0]  [ 2760/13501]  eta: 1:53:27  lr: 0.000013  closs: 0.3191 (1.7241)  mloss: 0.3191 (1.7241)  time: 0.6315  data: 0.0002  max mem: 29573
[22:18:01.789695] Epoch: [0]  [ 2770/13501]  eta: 1:53:21  lr: 0.000013  closs: 0.3185 (1.7190)  mloss: 0.3185 (1.7190)  time: 0.6323  data: 0.0002  max mem: 29573
[22:18:08.145855] Epoch: [0]  [ 2780/13501]  eta: 1:53:14  lr: 0.000013  closs: 0.3383 (1.7140)  mloss: 0.3383 (1.7140)  time: 0.6332  data: 0.0002  max mem: 29573
[22:18:14.502283] Epoch: [0]  [ 2790/13501]  eta: 1:53:08  lr: 0.000013  closs: 0.3462 (1.7092)  mloss: 0.3462 (1.7092)  time: 0.6355  data: 0.0002  max mem: 29573
[22:18:20.845620] Epoch: [0]  [ 2800/13501]  eta: 1:53:02  lr: 0.000013  closs: 0.3462 (1.7044)  mloss: 0.3462 (1.7044)  time: 0.6349  data: 0.0002  max mem: 29573
[22:18:27.133333] Epoch: [0]  [ 2810/13501]  eta: 1:52:55  lr: 0.000013  closs: 0.3440 (1.6996)  mloss: 0.3440 (1.6996)  time: 0.6315  data: 0.0002  max mem: 29573
[22:18:33.472038] Epoch: [0]  [ 2820/13501]  eta: 1:52:49  lr: 0.000013  closs: 0.3207 (1.6947)  mloss: 0.3207 (1.6947)  time: 0.6312  data: 0.0002  max mem: 29573
[22:18:39.758291] Epoch: [0]  [ 2830/13501]  eta: 1:52:42  lr: 0.000013  closs: 0.2974 (1.6898)  mloss: 0.2974 (1.6898)  time: 0.6312  data: 0.0002  max mem: 29573
[22:18:46.107418] Epoch: [0]  [ 2840/13501]  eta: 1:52:36  lr: 0.000013  closs: 0.3095 (1.6851)  mloss: 0.3095 (1.6851)  time: 0.6317  data: 0.0002  max mem: 29573
[22:18:52.390550] Epoch: [0]  [ 2850/13501]  eta: 1:52:29  lr: 0.000013  closs: 0.3462 (1.6805)  mloss: 0.3462 (1.6805)  time: 0.6315  data: 0.0002  max mem: 29573
[22:18:58.732630] Epoch: [0]  [ 2860/13501]  eta: 1:52:23  lr: 0.000013  closs: 0.3841 (1.6760)  mloss: 0.3841 (1.6760)  time: 0.6312  data: 0.0002  max mem: 29573
[22:19:05.030942] Epoch: [0]  [ 2870/13501]  eta: 1:52:17  lr: 0.000013  closs: 0.3492 (1.6715)  mloss: 0.3492 (1.6715)  time: 0.6319  data: 0.0002  max mem: 29573
[22:19:11.398599] Epoch: [0]  [ 2880/13501]  eta: 1:52:10  lr: 0.000013  closs: 0.3449 (1.6669)  mloss: 0.3449 (1.6669)  time: 0.6332  data: 0.0002  max mem: 29573
[22:19:17.688605] Epoch: [0]  [ 2890/13501]  eta: 1:52:04  lr: 0.000013  closs: 0.3449 (1.6623)  mloss: 0.3449 (1.6623)  time: 0.6328  data: 0.0002  max mem: 29573
[22:19:24.035131] Epoch: [0]  [ 2900/13501]  eta: 1:51:58  lr: 0.000013  closs: 0.4020 (1.6581)  mloss: 0.4020 (1.6581)  time: 0.6317  data: 0.0002  max mem: 29573
[22:19:30.328982] Epoch: [0]  [ 2910/13501]  eta: 1:51:51  lr: 0.000013  closs: 0.3864 (1.6535)  mloss: 0.3864 (1.6535)  time: 0.6319  data: 0.0002  max mem: 29573
[22:19:36.667040] Epoch: [0]  [ 2920/13501]  eta: 1:51:45  lr: 0.000014  closs: 0.3353 (1.6491)  mloss: 0.3353 (1.6491)  time: 0.6315  data: 0.0002  max mem: 29573
[22:19:42.960555] Epoch: [0]  [ 2930/13501]  eta: 1:51:38  lr: 0.000014  closs: 0.3612 (1.6447)  mloss: 0.3612 (1.6447)  time: 0.6315  data: 0.0002  max mem: 29573
[22:19:49.309252] Epoch: [0]  [ 2940/13501]  eta: 1:51:32  lr: 0.000014  closs: 0.3421 (1.6402)  mloss: 0.3421 (1.6402)  time: 0.6320  data: 0.0002  max mem: 29573
[22:19:55.607262] Epoch: [0]  [ 2950/13501]  eta: 1:51:25  lr: 0.000014  closs: 0.3385 (1.6358)  mloss: 0.3385 (1.6358)  time: 0.6323  data: 0.0002  max mem: 29573
[22:20:01.947794] Epoch: [0]  [ 2960/13501]  eta: 1:51:19  lr: 0.000014  closs: 0.3469 (1.6315)  mloss: 0.3469 (1.6315)  time: 0.6318  data: 0.0002  max mem: 29573
[22:20:08.242678] Epoch: [0]  [ 2970/13501]  eta: 1:51:13  lr: 0.000014  closs: 0.3490 (1.6272)  mloss: 0.3490 (1.6272)  time: 0.6317  data: 0.0002  max mem: 29573
[22:20:14.593086] Epoch: [0]  [ 2980/13501]  eta: 1:51:06  lr: 0.000014  closs: 0.3572 (1.6229)  mloss: 0.3572 (1.6229)  time: 0.6322  data: 0.0002  max mem: 29573
[22:20:20.901579] Epoch: [0]  [ 2990/13501]  eta: 1:51:00  lr: 0.000014  closs: 0.3572 (1.6187)  mloss: 0.3572 (1.6187)  time: 0.6329  data: 0.0002  max mem: 29573
[22:20:27.244327] Epoch: [0]  [ 3000/13501]  eta: 1:50:54  lr: 0.000014  closs: 0.3600 (1.6146)  mloss: 0.3600 (1.6146)  time: 0.6325  data: 0.0002  max mem: 29573
[22:20:33.535614] Epoch: [0]  [ 3010/13501]  eta: 1:50:47  lr: 0.000014  closs: 0.3604 (1.6103)  mloss: 0.3604 (1.6103)  time: 0.6316  data: 0.0002  max mem: 29573
[22:20:39.887736] Epoch: [0]  [ 3020/13501]  eta: 1:50:41  lr: 0.000014  closs: 0.3503 (1.6061)  mloss: 0.3503 (1.6061)  time: 0.6321  data: 0.0002  max mem: 29573
[22:20:46.188453] Epoch: [0]  [ 3030/13501]  eta: 1:50:34  lr: 0.000014  closs: 0.3178 (1.6020)  mloss: 0.3178 (1.6020)  time: 0.6326  data: 0.0002  max mem: 29573
[22:20:52.527068] Epoch: [0]  [ 3040/13501]  eta: 1:50:28  lr: 0.000014  closs: 0.3282 (1.5978)  mloss: 0.3282 (1.5978)  time: 0.6319  data: 0.0002  max mem: 29573
[22:20:58.809393] Epoch: [0]  [ 3050/13501]  eta: 1:50:22  lr: 0.000014  closs: 0.3299 (1.5937)  mloss: 0.3299 (1.5937)  time: 0.6310  data: 0.0002  max mem: 29573
[22:21:05.145870] Epoch: [0]  [ 3060/13501]  eta: 1:50:15  lr: 0.000014  closs: 0.3276 (1.5896)  mloss: 0.3276 (1.5896)  time: 0.6309  data: 0.0002  max mem: 29573
[22:21:11.428304] Epoch: [0]  [ 3070/13501]  eta: 1:50:09  lr: 0.000014  closs: 0.3441 (1.5855)  mloss: 0.3441 (1.5855)  time: 0.6309  data: 0.0003  max mem: 29573
[22:21:17.763220] Epoch: [0]  [ 3080/13501]  eta: 1:50:02  lr: 0.000014  closs: 0.3512 (1.5816)  mloss: 0.3512 (1.5816)  time: 0.6308  data: 0.0003  max mem: 29573
[22:21:24.066466] Epoch: [0]  [ 3090/13501]  eta: 1:49:56  lr: 0.000014  closs: 0.3507 (1.5775)  mloss: 0.3507 (1.5775)  time: 0.6318  data: 0.0002  max mem: 29573
[22:21:30.420128] Epoch: [0]  [ 3100/13501]  eta: 1:49:50  lr: 0.000014  closs: 0.3075 (1.5734)  mloss: 0.3075 (1.5734)  time: 0.6328  data: 0.0002  max mem: 29573
[22:21:36.714408] Epoch: [0]  [ 3110/13501]  eta: 1:49:43  lr: 0.000014  closs: 0.2980 (1.5694)  mloss: 0.2980 (1.5694)  time: 0.6323  data: 0.0002  max mem: 29573
[22:21:43.057124] Epoch: [0]  [ 3120/13501]  eta: 1:49:37  lr: 0.000014  closs: 0.3396 (1.5655)  mloss: 0.3396 (1.5655)  time: 0.6318  data: 0.0002  max mem: 29573
[22:21:49.344242] Epoch: [0]  [ 3130/13501]  eta: 1:49:30  lr: 0.000014  closs: 0.3396 (1.5615)  mloss: 0.3396 (1.5615)  time: 0.6314  data: 0.0002  max mem: 29573
[22:21:55.684659] Epoch: [0]  [ 3140/13501]  eta: 1:49:24  lr: 0.000015  closs: 0.3310 (1.5576)  mloss: 0.3310 (1.5576)  time: 0.6313  data: 0.0002  max mem: 29573
[22:22:01.978221] Epoch: [0]  [ 3150/13501]  eta: 1:49:17  lr: 0.000015  closs: 0.3393 (1.5539)  mloss: 0.3393 (1.5539)  time: 0.6316  data: 0.0002  max mem: 29573
[22:22:08.319384] Epoch: [0]  [ 3160/13501]  eta: 1:49:11  lr: 0.000015  closs: 0.3415 (1.5500)  mloss: 0.3415 (1.5500)  time: 0.6317  data: 0.0002  max mem: 29573
[22:22:14.605159] Epoch: [0]  [ 3170/13501]  eta: 1:49:05  lr: 0.000015  closs: 0.3179 (1.5462)  mloss: 0.3179 (1.5462)  time: 0.6313  data: 0.0002  max mem: 29573
[22:22:20.950229] Epoch: [0]  [ 3180/13501]  eta: 1:48:58  lr: 0.000015  closs: 0.3304 (1.5424)  mloss: 0.3304 (1.5424)  time: 0.6315  data: 0.0002  max mem: 29573
[22:22:27.245420] Epoch: [0]  [ 3190/13501]  eta: 1:48:52  lr: 0.000015  closs: 0.3522 (1.5387)  mloss: 0.3522 (1.5387)  time: 0.6319  data: 0.0002  max mem: 29573
[22:22:33.612284] Epoch: [0]  [ 3200/13501]  eta: 1:48:46  lr: 0.000015  closs: 0.3492 (1.5350)  mloss: 0.3492 (1.5350)  time: 0.6330  data: 0.0002  max mem: 29573
[22:22:39.901476] Epoch: [0]  [ 3210/13501]  eta: 1:48:39  lr: 0.000015  closs: 0.3549 (1.5314)  mloss: 0.3549 (1.5314)  time: 0.6327  data: 0.0002  max mem: 29573
[22:22:46.238012] Epoch: [0]  [ 3220/13501]  eta: 1:48:33  lr: 0.000015  closs: 0.3489 (1.5276)  mloss: 0.3489 (1.5276)  time: 0.6312  data: 0.0002  max mem: 29573
[22:22:52.520136] Epoch: [0]  [ 3230/13501]  eta: 1:48:26  lr: 0.000015  closs: 0.3326 (1.5239)  mloss: 0.3326 (1.5239)  time: 0.6309  data: 0.0002  max mem: 29573
[22:22:58.859184] Epoch: [0]  [ 3240/13501]  eta: 1:48:20  lr: 0.000015  closs: 0.3249 (1.5203)  mloss: 0.3249 (1.5203)  time: 0.6310  data: 0.0002  max mem: 29573
[22:23:05.144755] Epoch: [0]  [ 3250/13501]  eta: 1:48:14  lr: 0.000015  closs: 0.3217 (1.5166)  mloss: 0.3217 (1.5166)  time: 0.6312  data: 0.0002  max mem: 29573
[22:23:11.486156] Epoch: [0]  [ 3260/13501]  eta: 1:48:07  lr: 0.000015  closs: 0.3433 (1.5130)  mloss: 0.3433 (1.5130)  time: 0.6313  data: 0.0002  max mem: 29573
[22:23:17.769040] Epoch: [0]  [ 3270/13501]  eta: 1:48:01  lr: 0.000015  closs: 0.3544 (1.5096)  mloss: 0.3544 (1.5096)  time: 0.6311  data: 0.0002  max mem: 29573
[22:23:24.103802] Epoch: [0]  [ 3280/13501]  eta: 1:47:54  lr: 0.000015  closs: 0.3673 (1.5062)  mloss: 0.3673 (1.5062)  time: 0.6308  data: 0.0002  max mem: 29573
[22:23:30.390807] Epoch: [0]  [ 3290/13501]  eta: 1:47:48  lr: 0.000015  closs: 0.3673 (1.5027)  mloss: 0.3673 (1.5027)  time: 0.6310  data: 0.0002  max mem: 29573
[22:23:36.744263] Epoch: [0]  [ 3300/13501]  eta: 1:47:42  lr: 0.000015  closs: 0.3553 (1.4992)  mloss: 0.3553 (1.4992)  time: 0.6320  data: 0.0002  max mem: 29573
[22:23:43.048651] Epoch: [0]  [ 3310/13501]  eta: 1:47:35  lr: 0.000015  closs: 0.3310 (1.4957)  mloss: 0.3310 (1.4957)  time: 0.6328  data: 0.0002  max mem: 29573
[22:23:49.392708] Epoch: [0]  [ 3320/13501]  eta: 1:47:29  lr: 0.000015  closs: 0.3251 (1.4922)  mloss: 0.3251 (1.4922)  time: 0.6323  data: 0.0002  max mem: 29573
[22:23:55.674418] Epoch: [0]  [ 3330/13501]  eta: 1:47:22  lr: 0.000015  closs: 0.3416 (1.4888)  mloss: 0.3416 (1.4888)  time: 0.6312  data: 0.0002  max mem: 29573
[22:24:02.020287] Epoch: [0]  [ 3340/13501]  eta: 1:47:16  lr: 0.000015  closs: 0.3416 (1.4854)  mloss: 0.3416 (1.4854)  time: 0.6313  data: 0.0002  max mem: 29573
[22:24:08.311364] Epoch: [0]  [ 3350/13501]  eta: 1:47:10  lr: 0.000015  closs: 0.3301 (1.4819)  mloss: 0.3301 (1.4819)  time: 0.6318  data: 0.0003  max mem: 29573
[22:24:14.654865] Epoch: [0]  [ 3360/13501]  eta: 1:47:03  lr: 0.000016  closs: 0.3234 (1.4785)  mloss: 0.3234 (1.4785)  time: 0.6317  data: 0.0003  max mem: 29573
[22:24:20.948557] Epoch: [0]  [ 3370/13501]  eta: 1:46:57  lr: 0.000016  closs: 0.3448 (1.4751)  mloss: 0.3448 (1.4751)  time: 0.6318  data: 0.0002  max mem: 29573
[22:24:27.304769] Epoch: [0]  [ 3380/13501]  eta: 1:46:51  lr: 0.000016  closs: 0.3302 (1.4717)  mloss: 0.3302 (1.4717)  time: 0.6324  data: 0.0002  max mem: 29573
[22:24:33.611873] Epoch: [0]  [ 3390/13501]  eta: 1:46:44  lr: 0.000016  closs: 0.3302 (1.4684)  mloss: 0.3302 (1.4684)  time: 0.6331  data: 0.0002  max mem: 29573
[22:24:39.946720] Epoch: [0]  [ 3400/13501]  eta: 1:46:38  lr: 0.000016  closs: 0.3499 (1.4651)  mloss: 0.3499 (1.4651)  time: 0.6320  data: 0.0002  max mem: 29573
[22:24:46.249530] Epoch: [0]  [ 3410/13501]  eta: 1:46:31  lr: 0.000016  closs: 0.3376 (1.4618)  mloss: 0.3376 (1.4618)  time: 0.6318  data: 0.0002  max mem: 29573
[22:24:52.602058] Epoch: [0]  [ 3420/13501]  eta: 1:46:25  lr: 0.000016  closs: 0.3442 (1.4586)  mloss: 0.3442 (1.4586)  time: 0.6327  data: 0.0002  max mem: 29573
[22:24:58.889655] Epoch: [0]  [ 3430/13501]  eta: 1:46:19  lr: 0.000016  closs: 0.3500 (1.4553)  mloss: 0.3500 (1.4553)  time: 0.6319  data: 0.0002  max mem: 29573
[22:25:05.229695] Epoch: [0]  [ 3440/13501]  eta: 1:46:12  lr: 0.000016  closs: 0.3226 (1.4520)  mloss: 0.3226 (1.4520)  time: 0.6313  data: 0.0002  max mem: 29573
[22:25:11.531255] Epoch: [0]  [ 3450/13501]  eta: 1:46:06  lr: 0.000016  closs: 0.3413 (1.4488)  mloss: 0.3413 (1.4488)  time: 0.6320  data: 0.0002  max mem: 29573
[22:25:17.882123] Epoch: [0]  [ 3460/13501]  eta: 1:46:00  lr: 0.000016  closs: 0.3429 (1.4456)  mloss: 0.3429 (1.4456)  time: 0.6325  data: 0.0003  max mem: 29573
[22:25:24.170469] Epoch: [0]  [ 3470/13501]  eta: 1:45:53  lr: 0.000016  closs: 0.3164 (1.4424)  mloss: 0.3164 (1.4424)  time: 0.6319  data: 0.0003  max mem: 29573
[22:25:30.511147] Epoch: [0]  [ 3480/13501]  eta: 1:45:47  lr: 0.000016  closs: 0.3382 (1.4393)  mloss: 0.3382 (1.4393)  time: 0.6314  data: 0.0003  max mem: 29573
[22:25:36.790297] Epoch: [0]  [ 3490/13501]  eta: 1:45:40  lr: 0.000016  closs: 0.3560 (1.4363)  mloss: 0.3560 (1.4363)  time: 0.6309  data: 0.0002  max mem: 29573
[22:25:43.132261] Epoch: [0]  [ 3500/13501]  eta: 1:45:34  lr: 0.000016  closs: 0.3237 (1.4331)  mloss: 0.3237 (1.4331)  time: 0.6310  data: 0.0003  max mem: 29573
[22:25:49.422619] Epoch: [0]  [ 3510/13501]  eta: 1:45:28  lr: 0.000016  closs: 0.3370 (1.4300)  mloss: 0.3370 (1.4300)  time: 0.6315  data: 0.0003  max mem: 29573
[22:25:55.789756] Epoch: [0]  [ 3520/13501]  eta: 1:45:21  lr: 0.000016  closs: 0.3114 (1.4269)  mloss: 0.3114 (1.4269)  time: 0.6328  data: 0.0002  max mem: 29573
[22:26:02.072644] Epoch: [0]  [ 3530/13501]  eta: 1:45:15  lr: 0.000016  closs: 0.3094 (1.4239)  mloss: 0.3094 (1.4239)  time: 0.6324  data: 0.0002  max mem: 29573
[22:26:08.415278] Epoch: [0]  [ 3540/13501]  eta: 1:45:08  lr: 0.000016  closs: 0.3673 (1.4210)  mloss: 0.3673 (1.4210)  time: 0.6312  data: 0.0002  max mem: 29573
[22:26:14.706941] Epoch: [0]  [ 3550/13501]  eta: 1:45:02  lr: 0.000016  closs: 0.3673 (1.4179)  mloss: 0.3673 (1.4179)  time: 0.6316  data: 0.0002  max mem: 29573
[22:26:21.058368] Epoch: [0]  [ 3560/13501]  eta: 1:44:56  lr: 0.000016  closs: 0.3216 (1.4149)  mloss: 0.3216 (1.4149)  time: 0.6321  data: 0.0002  max mem: 29573
[22:26:27.341029] Epoch: [0]  [ 3570/13501]  eta: 1:44:49  lr: 0.000017  closs: 0.3529 (1.4119)  mloss: 0.3529 (1.4119)  time: 0.6316  data: 0.0002  max mem: 29573
[22:26:33.682212] Epoch: [0]  [ 3580/13501]  eta: 1:44:43  lr: 0.000017  closs: 0.3529 (1.4089)  mloss: 0.3529 (1.4089)  time: 0.6311  data: 0.0003  max mem: 29573
[22:26:39.968433] Epoch: [0]  [ 3590/13501]  eta: 1:44:36  lr: 0.000017  closs: 0.3291 (1.4060)  mloss: 0.3291 (1.4060)  time: 0.6313  data: 0.0003  max mem: 29573
[22:26:46.310187] Epoch: [0]  [ 3600/13501]  eta: 1:44:30  lr: 0.000017  closs: 0.3221 (1.4031)  mloss: 0.3221 (1.4031)  time: 0.6313  data: 0.0002  max mem: 29573
[22:26:52.596961] Epoch: [0]  [ 3610/13501]  eta: 1:44:24  lr: 0.000017  closs: 0.3117 (1.4001)  mloss: 0.3117 (1.4001)  time: 0.6313  data: 0.0002  max mem: 29573
[22:26:58.943094] Epoch: [0]  [ 3620/13501]  eta: 1:44:17  lr: 0.000017  closs: 0.3268 (1.3972)  mloss: 0.3268 (1.3972)  time: 0.6316  data: 0.0002  max mem: 29573
[22:27:05.238285] Epoch: [0]  [ 3630/13501]  eta: 1:44:11  lr: 0.000017  closs: 0.3346 (1.3942)  mloss: 0.3346 (1.3942)  time: 0.6320  data: 0.0002  max mem: 29573
[22:27:11.574379] Epoch: [0]  [ 3640/13501]  eta: 1:44:05  lr: 0.000017  closs: 0.3508 (1.3914)  mloss: 0.3508 (1.3914)  time: 0.6315  data: 0.0002  max mem: 29573
[22:27:17.894045] Epoch: [0]  [ 3650/13501]  eta: 1:43:58  lr: 0.000017  closs: 0.3385 (1.3884)  mloss: 0.3385 (1.3884)  time: 0.6327  data: 0.0002  max mem: 29573
[22:27:24.236795] Epoch: [0]  [ 3660/13501]  eta: 1:43:52  lr: 0.000017  closs: 0.3423 (1.3857)  mloss: 0.3423 (1.3857)  time: 0.6330  data: 0.0002  max mem: 29573
[22:27:30.522861] Epoch: [0]  [ 3670/13501]  eta: 1:43:46  lr: 0.000017  closs: 0.3252 (1.3828)  mloss: 0.3252 (1.3828)  time: 0.6314  data: 0.0003  max mem: 29573
[22:27:36.860281] Epoch: [0]  [ 3680/13501]  eta: 1:43:39  lr: 0.000017  closs: 0.3316 (1.3801)  mloss: 0.3316 (1.3801)  time: 0.6311  data: 0.0002  max mem: 29573
[22:27:43.145791] Epoch: [0]  [ 3690/13501]  eta: 1:43:33  lr: 0.000017  closs: 0.3571 (1.3773)  mloss: 0.3571 (1.3773)  time: 0.6311  data: 0.0002  max mem: 29573
[22:27:49.494768] Epoch: [0]  [ 3700/13501]  eta: 1:43:26  lr: 0.000017  closs: 0.3286 (1.3745)  mloss: 0.3286 (1.3745)  time: 0.6317  data: 0.0002  max mem: 29573
[22:27:55.785513] Epoch: [0]  [ 3710/13501]  eta: 1:43:20  lr: 0.000017  closs: 0.3155 (1.3716)  mloss: 0.3155 (1.3716)  time: 0.6319  data: 0.0002  max mem: 29573
[22:28:02.140552] Epoch: [0]  [ 3720/13501]  eta: 1:43:14  lr: 0.000017  closs: 0.3267 (1.3689)  mloss: 0.3267 (1.3689)  time: 0.6322  data: 0.0002  max mem: 29573
[22:28:08.437405] Epoch: [0]  [ 3730/13501]  eta: 1:43:07  lr: 0.000017  closs: 0.3290 (1.3662)  mloss: 0.3290 (1.3662)  time: 0.6325  data: 0.0002  max mem: 29573
[22:28:14.780619] Epoch: [0]  [ 3740/13501]  eta: 1:43:01  lr: 0.000017  closs: 0.3260 (1.3634)  mloss: 0.3260 (1.3634)  time: 0.6319  data: 0.0002  max mem: 29573
[22:28:21.059767] Epoch: [0]  [ 3750/13501]  eta: 1:42:54  lr: 0.000017  closs: 0.3450 (1.3608)  mloss: 0.3450 (1.3608)  time: 0.6310  data: 0.0002  max mem: 29573
[22:28:27.400652] Epoch: [0]  [ 3760/13501]  eta: 1:42:48  lr: 0.000017  closs: 0.3956 (1.3582)  mloss: 0.3956 (1.3582)  time: 0.6309  data: 0.0002  max mem: 29573
[22:28:33.689685] Epoch: [0]  [ 3770/13501]  eta: 1:42:42  lr: 0.000017  closs: 0.3762 (1.3556)  mloss: 0.3762 (1.3556)  time: 0.6314  data: 0.0002  max mem: 29573
[22:28:40.026026] Epoch: [0]  [ 3780/13501]  eta: 1:42:35  lr: 0.000017  closs: 0.3225 (1.3529)  mloss: 0.3225 (1.3529)  time: 0.6312  data: 0.0002  max mem: 29573
[22:28:46.319064] Epoch: [0]  [ 3790/13501]  eta: 1:42:29  lr: 0.000018  closs: 0.3154 (1.3503)  mloss: 0.3154 (1.3503)  time: 0.6314  data: 0.0002  max mem: 29573
[22:28:52.660628] Epoch: [0]  [ 3800/13501]  eta: 1:42:23  lr: 0.000018  closs: 0.3434 (1.3476)  mloss: 0.3434 (1.3476)  time: 0.6317  data: 0.0002  max mem: 29573
[22:28:58.955309] Epoch: [0]  [ 3810/13501]  eta: 1:42:16  lr: 0.000018  closs: 0.3137 (1.3449)  mloss: 0.3137 (1.3449)  time: 0.6317  data: 0.0002  max mem: 29573
[22:29:05.287415] Epoch: [0]  [ 3820/13501]  eta: 1:42:10  lr: 0.000018  closs: 0.3541 (1.3423)  mloss: 0.3541 (1.3423)  time: 0.6313  data: 0.0002  max mem: 29573
[22:29:11.586440] Epoch: [0]  [ 3830/13501]  eta: 1:42:03  lr: 0.000018  closs: 0.3686 (1.3398)  mloss: 0.3686 (1.3398)  time: 0.6315  data: 0.0002  max mem: 29573
[22:29:17.943449] Epoch: [0]  [ 3840/13501]  eta: 1:41:57  lr: 0.000018  closs: 0.3391 (1.3371)  mloss: 0.3391 (1.3371)  time: 0.6327  data: 0.0003  max mem: 29573
[22:29:24.237544] Epoch: [0]  [ 3850/13501]  eta: 1:41:51  lr: 0.000018  closs: 0.3345 (1.3345)  mloss: 0.3345 (1.3345)  time: 0.6325  data: 0.0004  max mem: 29573
[22:29:30.582924] Epoch: [0]  [ 3860/13501]  eta: 1:41:44  lr: 0.000018  closs: 0.3345 (1.3319)  mloss: 0.3345 (1.3319)  time: 0.6319  data: 0.0003  max mem: 29573
[22:29:36.865078] Epoch: [0]  [ 3870/13501]  eta: 1:41:38  lr: 0.000018  closs: 0.3259 (1.3293)  mloss: 0.3259 (1.3293)  time: 0.6313  data: 0.0002  max mem: 29573
[22:29:43.206496] Epoch: [0]  [ 3880/13501]  eta: 1:41:32  lr: 0.000018  closs: 0.3299 (1.3268)  mloss: 0.3299 (1.3268)  time: 0.6311  data: 0.0002  max mem: 29573
[22:29:49.503351] Epoch: [0]  [ 3890/13501]  eta: 1:41:25  lr: 0.000018  closs: 0.2968 (1.3241)  mloss: 0.2968 (1.3241)  time: 0.6318  data: 0.0002  max mem: 29573
[22:29:55.846950] Epoch: [0]  [ 3900/13501]  eta: 1:41:19  lr: 0.000018  closs: 0.3166 (1.3217)  mloss: 0.3166 (1.3217)  time: 0.6319  data: 0.0002  max mem: 29573
[22:30:02.129013] Epoch: [0]  [ 3910/13501]  eta: 1:41:13  lr: 0.000018  closs: 0.3518 (1.3192)  mloss: 0.3518 (1.3192)  time: 0.6312  data: 0.0002  max mem: 29573
[22:30:08.457532] Epoch: [0]  [ 3920/13501]  eta: 1:41:06  lr: 0.000018  closs: 0.3292 (1.3168)  mloss: 0.3292 (1.3168)  time: 0.6305  data: 0.0002  max mem: 29573
[22:30:14.745213] Epoch: [0]  [ 3930/13501]  eta: 1:41:00  lr: 0.000018  closs: 0.3536 (1.3143)  mloss: 0.3536 (1.3143)  time: 0.6307  data: 0.0002  max mem: 29573
[22:30:21.099833] Epoch: [0]  [ 3940/13501]  eta: 1:40:53  lr: 0.000018  closs: 0.3244 (1.3118)  mloss: 0.3244 (1.3118)  time: 0.6320  data: 0.0003  max mem: 29573
[22:30:27.388546] Epoch: [0]  [ 3950/13501]  eta: 1:40:47  lr: 0.000018  closs: 0.3484 (1.3095)  mloss: 0.3484 (1.3095)  time: 0.6321  data: 0.0003  max mem: 29573
[22:30:33.728030] Epoch: [0]  [ 3960/13501]  eta: 1:40:41  lr: 0.000018  closs: 0.3665 (1.3071)  mloss: 0.3665 (1.3071)  time: 0.6313  data: 0.0002  max mem: 29573
[22:30:40.005430] Epoch: [0]  [ 3970/13501]  eta: 1:40:34  lr: 0.000018  closs: 0.3485 (1.3048)  mloss: 0.3485 (1.3048)  time: 0.6308  data: 0.0002  max mem: 29573
[22:30:46.351860] Epoch: [0]  [ 3980/13501]  eta: 1:40:28  lr: 0.000018  closs: 0.3288 (1.3024)  mloss: 0.3288 (1.3024)  time: 0.6311  data: 0.0003  max mem: 29573
[22:30:52.640274] Epoch: [0]  [ 3990/13501]  eta: 1:40:22  lr: 0.000018  closs: 0.3097 (1.2999)  mloss: 0.3097 (1.2999)  time: 0.6317  data: 0.0002  max mem: 29573
[22:30:58.981546] Epoch: [0]  [ 4000/13501]  eta: 1:40:15  lr: 0.000019  closs: 0.3335 (1.2976)  mloss: 0.3335 (1.2976)  time: 0.6314  data: 0.0002  max mem: 29573
[22:31:05.268368] Epoch: [0]  [ 4010/13501]  eta: 1:40:09  lr: 0.000019  closs: 0.3341 (1.2952)  mloss: 0.3341 (1.2952)  time: 0.6313  data: 0.0002  max mem: 29573
[22:31:11.611750] Epoch: [0]  [ 4020/13501]  eta: 1:40:02  lr: 0.000019  closs: 0.3109 (1.2928)  mloss: 0.3109 (1.2928)  time: 0.6314  data: 0.0002  max mem: 29573
[22:31:17.901631] Epoch: [0]  [ 4030/13501]  eta: 1:39:56  lr: 0.000019  closs: 0.3243 (1.2904)  mloss: 0.3243 (1.2904)  time: 0.6316  data: 0.0002  max mem: 29573
[22:31:24.249634] Epoch: [0]  [ 4040/13501]  eta: 1:39:50  lr: 0.000019  closs: 0.3243 (1.2881)  mloss: 0.3243 (1.2881)  time: 0.6317  data: 0.0002  max mem: 29573
[22:31:30.569970] Epoch: [0]  [ 4050/13501]  eta: 1:39:43  lr: 0.000019  closs: 0.3639 (1.2859)  mloss: 0.3639 (1.2859)  time: 0.6332  data: 0.0007  max mem: 29573
[22:31:36.914984] Epoch: [0]  [ 4060/13501]  eta: 1:39:37  lr: 0.000019  closs: 0.3410 (1.2835)  mloss: 0.3410 (1.2835)  time: 0.6332  data: 0.0007  max mem: 29573
[22:31:43.196397] Epoch: [0]  [ 4070/13501]  eta: 1:39:31  lr: 0.000019  closs: 0.3225 (1.2813)  mloss: 0.3225 (1.2813)  time: 0.6312  data: 0.0002  max mem: 29573
[22:31:49.531947] Epoch: [0]  [ 4080/13501]  eta: 1:39:24  lr: 0.000019  closs: 0.3781 (1.2791)  mloss: 0.3781 (1.2791)  time: 0.6308  data: 0.0002  max mem: 29573
[22:31:55.810161] Epoch: [0]  [ 4090/13501]  eta: 1:39:18  lr: 0.000019  closs: 0.3525 (1.2768)  mloss: 0.3525 (1.2768)  time: 0.6306  data: 0.0002  max mem: 29573
[22:32:02.144936] Epoch: [0]  [ 4100/13501]  eta: 1:39:11  lr: 0.000019  closs: 0.3525 (1.2747)  mloss: 0.3525 (1.2747)  time: 0.6306  data: 0.0002  max mem: 29573
[22:32:08.436145] Epoch: [0]  [ 4110/13501]  eta: 1:39:05  lr: 0.000019  closs: 0.3705 (1.2724)  mloss: 0.3705 (1.2724)  time: 0.6312  data: 0.0002  max mem: 29573
[22:32:14.792350] Epoch: [0]  [ 4120/13501]  eta: 1:38:59  lr: 0.000019  closs: 0.3328 (1.2702)  mloss: 0.3328 (1.2702)  time: 0.6323  data: 0.0003  max mem: 29573
[22:32:21.085723] Epoch: [0]  [ 4130/13501]  eta: 1:38:52  lr: 0.000019  closs: 0.3444 (1.2679)  mloss: 0.3444 (1.2679)  time: 0.6324  data: 0.0002  max mem: 29573
[22:32:27.489268] Epoch: [0]  [ 4140/13501]  eta: 1:38:46  lr: 0.000019  closs: 0.3356 (1.2657)  mloss: 0.3356 (1.2657)  time: 0.6348  data: 0.0002  max mem: 29573
[22:32:33.783625] Epoch: [0]  [ 4150/13501]  eta: 1:38:40  lr: 0.000019  closs: 0.3513 (1.2635)  mloss: 0.3513 (1.2635)  time: 0.6348  data: 0.0002  max mem: 29573
[22:32:40.142623] Epoch: [0]  [ 4160/13501]  eta: 1:38:34  lr: 0.000019  closs: 0.3723 (1.2614)  mloss: 0.3723 (1.2614)  time: 0.6326  data: 0.0002  max mem: 29573
[22:32:46.435315] Epoch: [0]  [ 4170/13501]  eta: 1:38:27  lr: 0.000019  closs: 0.3192 (1.2591)  mloss: 0.3192 (1.2591)  time: 0.6325  data: 0.0002  max mem: 29573
[22:32:52.775967] Epoch: [0]  [ 4180/13501]  eta: 1:38:21  lr: 0.000019  closs: 0.3177 (1.2569)  mloss: 0.3177 (1.2569)  time: 0.6316  data: 0.0002  max mem: 29573
[22:32:59.060398] Epoch: [0]  [ 4190/13501]  eta: 1:38:14  lr: 0.000019  closs: 0.3521 (1.2547)  mloss: 0.3521 (1.2547)  time: 0.6312  data: 0.0002  max mem: 29573
[22:33:05.398802] Epoch: [0]  [ 4200/13501]  eta: 1:38:08  lr: 0.000019  closs: 0.3156 (1.2525)  mloss: 0.3156 (1.2525)  time: 0.6311  data: 0.0002  max mem: 29573
[22:33:11.695472] Epoch: [0]  [ 4210/13501]  eta: 1:38:02  lr: 0.000019  closs: 0.2858 (1.2502)  mloss: 0.2858 (1.2502)  time: 0.6317  data: 0.0002  max mem: 29573
[22:33:18.049744] Epoch: [0]  [ 4220/13501]  eta: 1:37:55  lr: 0.000020  closs: 0.2894 (1.2481)  mloss: 0.2894 (1.2481)  time: 0.6325  data: 0.0002  max mem: 29573
[22:33:24.343143] Epoch: [0]  [ 4230/13501]  eta: 1:37:49  lr: 0.000020  closs: 0.2923 (1.2458)  mloss: 0.2923 (1.2458)  time: 0.6323  data: 0.0002  max mem: 29573
[22:33:30.688121] Epoch: [0]  [ 4240/13501]  eta: 1:37:43  lr: 0.000020  closs: 0.3370 (1.2438)  mloss: 0.3370 (1.2438)  time: 0.6318  data: 0.0002  max mem: 29573
[22:33:36.971873] Epoch: [0]  [ 4250/13501]  eta: 1:37:36  lr: 0.000020  closs: 0.3558 (1.2417)  mloss: 0.3558 (1.2417)  time: 0.6314  data: 0.0003  max mem: 29573
[22:33:43.336919] Epoch: [0]  [ 4260/13501]  eta: 1:37:30  lr: 0.000020  closs: 0.3286 (1.2396)  mloss: 0.3286 (1.2396)  time: 0.6324  data: 0.0003  max mem: 29573
[22:33:49.626663] Epoch: [0]  [ 4270/13501]  eta: 1:37:24  lr: 0.000020  closs: 0.3286 (1.2375)  mloss: 0.3286 (1.2375)  time: 0.6327  data: 0.0002  max mem: 29573
[22:33:55.967871] Epoch: [0]  [ 4280/13501]  eta: 1:37:17  lr: 0.000020  closs: 0.3301 (1.2354)  mloss: 0.3301 (1.2354)  time: 0.6315  data: 0.0002  max mem: 29573
[22:34:02.250433] Epoch: [0]  [ 4290/13501]  eta: 1:37:11  lr: 0.000020  closs: 0.3218 (1.2332)  mloss: 0.3218 (1.2332)  time: 0.6311  data: 0.0002  max mem: 29573
[22:34:08.596322] Epoch: [0]  [ 4300/13501]  eta: 1:37:04  lr: 0.000020  closs: 0.3084 (1.2311)  mloss: 0.3084 (1.2311)  time: 0.6313  data: 0.0002  max mem: 29573
[22:34:14.883635] Epoch: [0]  [ 4310/13501]  eta: 1:36:58  lr: 0.000020  closs: 0.3275 (1.2291)  mloss: 0.3275 (1.2291)  time: 0.6316  data: 0.0003  max mem: 29573
[22:34:21.230750] Epoch: [0]  [ 4320/13501]  eta: 1:36:52  lr: 0.000020  closs: 0.3402 (1.2270)  mloss: 0.3402 (1.2270)  time: 0.6316  data: 0.0002  max mem: 29573
[22:34:27.524102] Epoch: [0]  [ 4330/13501]  eta: 1:36:45  lr: 0.000020  closs: 0.2963 (1.2250)  mloss: 0.2963 (1.2250)  time: 0.6319  data: 0.0002  max mem: 29573
[22:34:33.866425] Epoch: [0]  [ 4340/13501]  eta: 1:36:39  lr: 0.000020  closs: 0.3454 (1.2230)  mloss: 0.3454 (1.2230)  time: 0.6317  data: 0.0002  max mem: 29573
[22:34:40.155847] Epoch: [0]  [ 4350/13501]  eta: 1:36:33  lr: 0.000020  closs: 0.3738 (1.2210)  mloss: 0.3738 (1.2210)  time: 0.6315  data: 0.0002  max mem: 29573
[22:34:46.505162] Epoch: [0]  [ 4360/13501]  eta: 1:36:26  lr: 0.000020  closs: 0.3595 (1.2190)  mloss: 0.3595 (1.2190)  time: 0.6319  data: 0.0003  max mem: 29573
[22:34:52.818527] Epoch: [0]  [ 4370/13501]  eta: 1:36:20  lr: 0.000020  closs: 0.3216 (1.2170)  mloss: 0.3216 (1.2170)  time: 0.6331  data: 0.0003  max mem: 29573
[22:34:59.156247] Epoch: [0]  [ 4380/13501]  eta: 1:36:14  lr: 0.000020  closs: 0.3321 (1.2150)  mloss: 0.3321 (1.2150)  time: 0.6325  data: 0.0002  max mem: 29573
[22:35:05.526078] Epoch: [0]  [ 4390/13501]  eta: 1:36:07  lr: 0.000020  closs: 0.3366 (1.2130)  mloss: 0.3366 (1.2130)  time: 0.6353  data: 0.0002  max mem: 29573
[22:35:11.864732] Epoch: [0]  [ 4400/13501]  eta: 1:36:01  lr: 0.000020  closs: 0.3354 (1.2111)  mloss: 0.3354 (1.2111)  time: 0.6353  data: 0.0002  max mem: 29573
[22:35:18.151475] Epoch: [0]  [ 4410/13501]  eta: 1:35:55  lr: 0.000020  closs: 0.3288 (1.2090)  mloss: 0.3288 (1.2090)  time: 0.6312  data: 0.0002  max mem: 29573
[22:35:24.483503] Epoch: [0]  [ 4420/13501]  eta: 1:35:48  lr: 0.000020  closs: 0.3375 (1.2071)  mloss: 0.3375 (1.2071)  time: 0.6309  data: 0.0002  max mem: 29573
[22:35:30.767884] Epoch: [0]  [ 4430/13501]  eta: 1:35:42  lr: 0.000020  closs: 0.3329 (1.2051)  mloss: 0.3329 (1.2051)  time: 0.6307  data: 0.0002  max mem: 29573
[22:35:37.106059] Epoch: [0]  [ 4440/13501]  eta: 1:35:36  lr: 0.000021  closs: 0.3118 (1.2032)  mloss: 0.3118 (1.2032)  time: 0.6310  data: 0.0002  max mem: 29573
[22:35:43.391600] Epoch: [0]  [ 4450/13501]  eta: 1:35:29  lr: 0.000021  closs: 0.3118 (1.2012)  mloss: 0.3118 (1.2012)  time: 0.6311  data: 0.0002  max mem: 29573
[22:35:49.747733] Epoch: [0]  [ 4460/13501]  eta: 1:35:23  lr: 0.000021  closs: 0.2721 (1.1992)  mloss: 0.2721 (1.1992)  time: 0.6320  data: 0.0002  max mem: 29573
[22:35:56.049226] Epoch: [0]  [ 4470/13501]  eta: 1:35:16  lr: 0.000021  closs: 0.2906 (1.1972)  mloss: 0.2906 (1.1972)  time: 0.6328  data: 0.0002  max mem: 29573
[22:36:02.400456] Epoch: [0]  [ 4480/13501]  eta: 1:35:10  lr: 0.000021  closs: 0.3217 (1.1954)  mloss: 0.3217 (1.1954)  time: 0.6326  data: 0.0002  max mem: 29573
[22:36:08.686644] Epoch: [0]  [ 4490/13501]  eta: 1:35:04  lr: 0.000021  closs: 0.3771 (1.1936)  mloss: 0.3771 (1.1936)  time: 0.6318  data: 0.0002  max mem: 29573
[22:36:15.033797] Epoch: [0]  [ 4500/13501]  eta: 1:34:57  lr: 0.000021  closs: 0.3456 (1.1917)  mloss: 0.3456 (1.1917)  time: 0.6316  data: 0.0002  max mem: 29573
[22:36:21.328841] Epoch: [0]  [ 4510/13501]  eta: 1:34:51  lr: 0.000021  closs: 0.3235 (1.1898)  mloss: 0.3235 (1.1898)  time: 0.6320  data: 0.0003  max mem: 29573
[22:36:27.665634] Epoch: [0]  [ 4520/13501]  eta: 1:34:45  lr: 0.000021  closs: 0.3215 (1.1878)  mloss: 0.3215 (1.1878)  time: 0.6315  data: 0.0002  max mem: 29573
[22:36:33.953546] Epoch: [0]  [ 4530/13501]  eta: 1:34:38  lr: 0.000021  closs: 0.3137 (1.1860)  mloss: 0.3137 (1.1860)  time: 0.6312  data: 0.0002  max mem: 29573
[22:36:40.299044] Epoch: [0]  [ 4540/13501]  eta: 1:34:32  lr: 0.000021  closs: 0.3480 (1.1841)  mloss: 0.3480 (1.1841)  time: 0.6316  data: 0.0002  max mem: 29573
[22:36:46.589963] Epoch: [0]  [ 4550/13501]  eta: 1:34:26  lr: 0.000021  closs: 0.3278 (1.1822)  mloss: 0.3278 (1.1822)  time: 0.6317  data: 0.0002  max mem: 29573
[22:36:52.940243] Epoch: [0]  [ 4560/13501]  eta: 1:34:19  lr: 0.000021  closs: 0.3252 (1.1804)  mloss: 0.3252 (1.1804)  time: 0.6320  data: 0.0002  max mem: 29573
[22:36:59.229690] Epoch: [0]  [ 4570/13501]  eta: 1:34:13  lr: 0.000021  closs: 0.3297 (1.1785)  mloss: 0.3297 (1.1785)  time: 0.6319  data: 0.0002  max mem: 29573
[22:37:05.599207] Epoch: [0]  [ 4580/13501]  eta: 1:34:07  lr: 0.000021  closs: 0.3628 (1.1768)  mloss: 0.3628 (1.1768)  time: 0.6329  data: 0.0002  max mem: 29573
[22:37:11.883755] Epoch: [0]  [ 4590/13501]  eta: 1:34:00  lr: 0.000021  closs: 0.3467 (1.1750)  mloss: 0.3467 (1.1750)  time: 0.6326  data: 0.0002  max mem: 29573
[22:37:18.226551] Epoch: [0]  [ 4600/13501]  eta: 1:33:54  lr: 0.000021  closs: 0.3167 (1.1731)  mloss: 0.3167 (1.1731)  time: 0.6313  data: 0.0002  max mem: 29573
[22:37:24.510555] Epoch: [0]  [ 4610/13501]  eta: 1:33:48  lr: 0.000021  closs: 0.3553 (1.1714)  mloss: 0.3553 (1.1714)  time: 0.6313  data: 0.0003  max mem: 29573
[22:37:30.859087] Epoch: [0]  [ 4620/13501]  eta: 1:33:41  lr: 0.000021  closs: 0.3424 (1.1696)  mloss: 0.3424 (1.1696)  time: 0.6316  data: 0.0002  max mem: 29573
[22:37:37.153509] Epoch: [0]  [ 4630/13501]  eta: 1:33:35  lr: 0.000021  closs: 0.3421 (1.1679)  mloss: 0.3421 (1.1679)  time: 0.6321  data: 0.0002  max mem: 29573
[22:37:43.496747] Epoch: [0]  [ 4640/13501]  eta: 1:33:29  lr: 0.000021  closs: 0.3421 (1.1661)  mloss: 0.3421 (1.1661)  time: 0.6318  data: 0.0002  max mem: 29573
[22:37:49.778463] Epoch: [0]  [ 4650/13501]  eta: 1:33:22  lr: 0.000022  closs: 0.3234 (1.1643)  mloss: 0.3234 (1.1643)  time: 0.6312  data: 0.0002  max mem: 29573
[22:37:56.117164] Epoch: [0]  [ 4660/13501]  eta: 1:33:16  lr: 0.000022  closs: 0.3263 (1.1625)  mloss: 0.3263 (1.1625)  time: 0.6309  data: 0.0002  max mem: 29573
[22:38:02.404667] Epoch: [0]  [ 4670/13501]  eta: 1:33:09  lr: 0.000022  closs: 0.3673 (1.1609)  mloss: 0.3673 (1.1609)  time: 0.6312  data: 0.0002  max mem: 29573
[22:38:08.749833] Epoch: [0]  [ 4680/13501]  eta: 1:33:03  lr: 0.000022  closs: 0.3414 (1.1591)  mloss: 0.3414 (1.1591)  time: 0.6316  data: 0.0002  max mem: 29573
[22:38:15.055483] Epoch: [0]  [ 4690/13501]  eta: 1:32:57  lr: 0.000022  closs: 0.3298 (1.1574)  mloss: 0.3298 (1.1574)  time: 0.6325  data: 0.0002  max mem: 29573
[22:38:21.395282] Epoch: [0]  [ 4700/13501]  eta: 1:32:50  lr: 0.000022  closs: 0.3685 (1.1558)  mloss: 0.3685 (1.1558)  time: 0.6322  data: 0.0002  max mem: 29573
[22:38:27.685172] Epoch: [0]  [ 4710/13501]  eta: 1:32:44  lr: 0.000022  closs: 0.3754 (1.1541)  mloss: 0.3754 (1.1541)  time: 0.6314  data: 0.0002  max mem: 29573
[22:38:34.019633] Epoch: [0]  [ 4720/13501]  eta: 1:32:38  lr: 0.000022  closs: 0.3373 (1.1524)  mloss: 0.3373 (1.1524)  time: 0.6311  data: 0.0002  max mem: 29573
[22:38:40.311843] Epoch: [0]  [ 4730/13501]  eta: 1:32:31  lr: 0.000022  closs: 0.3373 (1.1507)  mloss: 0.3373 (1.1507)  time: 0.6313  data: 0.0002  max mem: 29573
[22:38:46.658565] Epoch: [0]  [ 4740/13501]  eta: 1:32:25  lr: 0.000022  closs: 0.3360 (1.1490)  mloss: 0.3360 (1.1490)  time: 0.6319  data: 0.0003  max mem: 29573
[22:38:52.943959] Epoch: [0]  [ 4750/13501]  eta: 1:32:19  lr: 0.000022  closs: 0.3184 (1.1473)  mloss: 0.3184 (1.1473)  time: 0.6315  data: 0.0004  max mem: 29573
[22:38:59.276606] Epoch: [0]  [ 4760/13501]  eta: 1:32:12  lr: 0.000022  closs: 0.3246 (1.1456)  mloss: 0.3246 (1.1456)  time: 0.6308  data: 0.0003  max mem: 29573
[22:39:05.559779] Epoch: [0]  [ 4770/13501]  eta: 1:32:06  lr: 0.000022  closs: 0.3264 (1.1439)  mloss: 0.3264 (1.1439)  time: 0.6307  data: 0.0003  max mem: 29573
[22:39:11.907562] Epoch: [0]  [ 4780/13501]  eta: 1:31:59  lr: 0.000022  closs: 0.3428 (1.1423)  mloss: 0.3428 (1.1423)  time: 0.6315  data: 0.0003  max mem: 29573
[22:39:18.217651] Epoch: [0]  [ 4790/13501]  eta: 1:31:53  lr: 0.000022  closs: 0.3663 (1.1408)  mloss: 0.3663 (1.1408)  time: 0.6328  data: 0.0002  max mem: 29573
[22:39:24.568215] Epoch: [0]  [ 4800/13501]  eta: 1:31:47  lr: 0.000022  closs: 0.3887 (1.1392)  mloss: 0.3887 (1.1392)  time: 0.6330  data: 0.0002  max mem: 29573
[22:39:30.857429] Epoch: [0]  [ 4810/13501]  eta: 1:31:40  lr: 0.000022  closs: 0.3887 (1.1376)  mloss: 0.3887 (1.1376)  time: 0.6319  data: 0.0002  max mem: 29573
[22:39:37.199476] Epoch: [0]  [ 4820/13501]  eta: 1:31:34  lr: 0.000022  closs: 0.3295 (1.1359)  mloss: 0.3295 (1.1359)  time: 0.6315  data: 0.0002  max mem: 29573
[22:39:43.488284] Epoch: [0]  [ 4830/13501]  eta: 1:31:28  lr: 0.000022  closs: 0.3119 (1.1343)  mloss: 0.3119 (1.1343)  time: 0.6315  data: 0.0002  max mem: 29573
[22:39:49.829499] Epoch: [0]  [ 4840/13501]  eta: 1:31:21  lr: 0.000022  closs: 0.3302 (1.1327)  mloss: 0.3302 (1.1327)  time: 0.6314  data: 0.0002  max mem: 29573
[22:39:56.121684] Epoch: [0]  [ 4850/13501]  eta: 1:31:15  lr: 0.000022  closs: 0.3302 (1.1310)  mloss: 0.3302 (1.1310)  time: 0.6316  data: 0.0002  max mem: 29573
[22:40:02.455295] Epoch: [0]  [ 4860/13501]  eta: 1:31:09  lr: 0.000022  closs: 0.3145 (1.1294)  mloss: 0.3145 (1.1294)  time: 0.6312  data: 0.0002  max mem: 29573
[22:40:08.741194] Epoch: [0]  [ 4870/13501]  eta: 1:31:02  lr: 0.000023  closs: 0.3543 (1.1278)  mloss: 0.3543 (1.1278)  time: 0.6309  data: 0.0002  max mem: 29573
[22:40:15.086749] Epoch: [0]  [ 4880/13501]  eta: 1:30:56  lr: 0.000023  closs: 0.3561 (1.1262)  mloss: 0.3561 (1.1262)  time: 0.6315  data: 0.0002  max mem: 29573
[22:40:21.385326] Epoch: [0]  [ 4890/13501]  eta: 1:30:50  lr: 0.000023  closs: 0.3477 (1.1247)  mloss: 0.3477 (1.1247)  time: 0.6321  data: 0.0002  max mem: 29573
[22:40:27.757471] Epoch: [0]  [ 4900/13501]  eta: 1:30:43  lr: 0.000023  closs: 0.3816 (1.1233)  mloss: 0.3816 (1.1233)  time: 0.6335  data: 0.0002  max mem: 29573
[22:40:34.043913] Epoch: [0]  [ 4910/13501]  eta: 1:30:37  lr: 0.000023  closs: 0.3661 (1.1217)  mloss: 0.3661 (1.1217)  time: 0.6329  data: 0.0002  max mem: 29573
[22:40:40.392055] Epoch: [0]  [ 4920/13501]  eta: 1:30:31  lr: 0.000023  closs: 0.3406 (1.1201)  mloss: 0.3406 (1.1201)  time: 0.6317  data: 0.0002  max mem: 29573
[22:40:46.682341] Epoch: [0]  [ 4930/13501]  eta: 1:30:24  lr: 0.000023  closs: 0.2842 (1.1184)  mloss: 0.2842 (1.1184)  time: 0.6318  data: 0.0002  max mem: 29573
[22:40:53.015038] Epoch: [0]  [ 4940/13501]  eta: 1:30:18  lr: 0.000023  closs: 0.2908 (1.1168)  mloss: 0.2908 (1.1168)  time: 0.6311  data: 0.0002  max mem: 29573
[22:40:59.307519] Epoch: [0]  [ 4950/13501]  eta: 1:30:12  lr: 0.000023  closs: 0.3016 (1.1152)  mloss: 0.3016 (1.1152)  time: 0.6312  data: 0.0002  max mem: 29573
[22:41:05.650626] Epoch: [0]  [ 4960/13501]  eta: 1:30:05  lr: 0.000023  closs: 0.3478 (1.1137)  mloss: 0.3478 (1.1137)  time: 0.6317  data: 0.0002  max mem: 29573
[22:41:11.933412] Epoch: [0]  [ 4970/13501]  eta: 1:29:59  lr: 0.000023  closs: 0.3690 (1.1122)  mloss: 0.3690 (1.1122)  time: 0.6312  data: 0.0002  max mem: 29573
[22:41:18.266859] Epoch: [0]  [ 4980/13501]  eta: 1:29:52  lr: 0.000023  closs: 0.3658 (1.1107)  mloss: 0.3658 (1.1107)  time: 0.6307  data: 0.0002  max mem: 29573
[22:41:24.552891] Epoch: [0]  [ 4990/13501]  eta: 1:29:46  lr: 0.000023  closs: 0.3554 (1.1092)  mloss: 0.3554 (1.1092)  time: 0.6309  data: 0.0002  max mem: 29573
[22:41:30.910586] Epoch: [0]  [ 5000/13501]  eta: 1:29:40  lr: 0.000023  closs: 0.3367 (1.1077)  mloss: 0.3367 (1.1077)  time: 0.6321  data: 0.0002  max mem: 29573
[22:41:37.209032] Epoch: [0]  [ 5010/13501]  eta: 1:29:33  lr: 0.000023  closs: 0.3438 (1.1062)  mloss: 0.3438 (1.1062)  time: 0.6327  data: 0.0002  max mem: 29573
[22:41:43.553409] Epoch: [0]  [ 5020/13501]  eta: 1:29:27  lr: 0.000023  closs: 0.3593 (1.1047)  mloss: 0.3593 (1.1047)  time: 0.6321  data: 0.0002  max mem: 29573
[22:41:49.843887] Epoch: [0]  [ 5030/13501]  eta: 1:29:21  lr: 0.000023  closs: 0.3484 (1.1032)  mloss: 0.3484 (1.1032)  time: 0.6317  data: 0.0002  max mem: 29573
[22:41:56.188329] Epoch: [0]  [ 5040/13501]  eta: 1:29:14  lr: 0.000023  closs: 0.3446 (1.1017)  mloss: 0.3446 (1.1017)  time: 0.6317  data: 0.0002  max mem: 29573
[22:42:02.476160] Epoch: [0]  [ 5050/13501]  eta: 1:29:08  lr: 0.000023  closs: 0.3506 (1.1002)  mloss: 0.3506 (1.1002)  time: 0.6315  data: 0.0002  max mem: 29573
[22:42:08.815521] Epoch: [0]  [ 5060/13501]  eta: 1:29:02  lr: 0.000023  closs: 0.3385 (1.0987)  mloss: 0.3385 (1.0987)  time: 0.6313  data: 0.0002  max mem: 29573
[22:42:15.108133] Epoch: [0]  [ 5070/13501]  eta: 1:28:55  lr: 0.000023  closs: 0.3385 (1.0971)  mloss: 0.3385 (1.0971)  time: 0.6315  data: 0.0002  max mem: 29573
[22:42:21.460460] Epoch: [0]  [ 5080/13501]  eta: 1:28:49  lr: 0.000024  closs: 0.3400 (1.0956)  mloss: 0.3400 (1.0956)  time: 0.6322  data: 0.0002  max mem: 29573
[22:42:27.750835] Epoch: [0]  [ 5090/13501]  eta: 1:28:43  lr: 0.000024  closs: 0.3499 (1.0942)  mloss: 0.3499 (1.0942)  time: 0.6321  data: 0.0002  max mem: 29573
[22:42:34.103289] Epoch: [0]  [ 5100/13501]  eta: 1:28:36  lr: 0.000024  closs: 0.3469 (1.0928)  mloss: 0.3469 (1.0928)  time: 0.6321  data: 0.0002  max mem: 29573
[22:42:40.406561] Epoch: [0]  [ 5110/13501]  eta: 1:28:30  lr: 0.000024  closs: 0.3427 (1.0913)  mloss: 0.3427 (1.0913)  time: 0.6327  data: 0.0002  max mem: 29573
[22:42:46.758241] Epoch: [0]  [ 5120/13501]  eta: 1:28:24  lr: 0.000024  closs: 0.3214 (1.0898)  mloss: 0.3214 (1.0898)  time: 0.6327  data: 0.0002  max mem: 29573
[22:42:53.054943] Epoch: [0]  [ 5130/13501]  eta: 1:28:17  lr: 0.000024  closs: 0.3147 (1.0885)  mloss: 0.3147 (1.0885)  time: 0.6323  data: 0.0002  max mem: 29573
[22:42:59.393208] Epoch: [0]  [ 5140/13501]  eta: 1:28:11  lr: 0.000024  closs: 0.3900 (1.0871)  mloss: 0.3900 (1.0871)  time: 0.6317  data: 0.0002  max mem: 29573
[22:43:05.687611] Epoch: [0]  [ 5150/13501]  eta: 1:28:05  lr: 0.000024  closs: 0.3748 (1.0857)  mloss: 0.3748 (1.0857)  time: 0.6316  data: 0.0002  max mem: 29573
[22:43:12.036020] Epoch: [0]  [ 5160/13501]  eta: 1:27:58  lr: 0.000024  closs: 0.3669 (1.0842)  mloss: 0.3669 (1.0842)  time: 0.6321  data: 0.0002  max mem: 29573
[22:43:18.326928] Epoch: [0]  [ 5170/13501]  eta: 1:27:52  lr: 0.000024  closs: 0.3339 (1.0828)  mloss: 0.3339 (1.0828)  time: 0.6319  data: 0.0002  max mem: 29573
[22:43:24.670932] Epoch: [0]  [ 5180/13501]  eta: 1:27:46  lr: 0.000024  closs: 0.3289 (1.0813)  mloss: 0.3289 (1.0813)  time: 0.6317  data: 0.0002  max mem: 29573
[22:43:30.966344] Epoch: [0]  [ 5190/13501]  eta: 1:27:39  lr: 0.000024  closs: 0.3395 (1.0800)  mloss: 0.3395 (1.0800)  time: 0.6319  data: 0.0002  max mem: 29573
[22:43:37.307897] Epoch: [0]  [ 5200/13501]  eta: 1:27:33  lr: 0.000024  closs: 0.3502 (1.0785)  mloss: 0.3502 (1.0785)  time: 0.6318  data: 0.0002  max mem: 29573
[22:43:43.607374] Epoch: [0]  [ 5210/13501]  eta: 1:27:27  lr: 0.000024  closs: 0.3198 (1.0772)  mloss: 0.3198 (1.0772)  time: 0.6320  data: 0.0002  max mem: 29573
[22:43:49.970636] Epoch: [0]  [ 5220/13501]  eta: 1:27:20  lr: 0.000024  closs: 0.3216 (1.0758)  mloss: 0.3216 (1.0758)  time: 0.6331  data: 0.0002  max mem: 29573
[22:43:56.264869] Epoch: [0]  [ 5230/13501]  eta: 1:27:14  lr: 0.000024  closs: 0.3176 (1.0744)  mloss: 0.3176 (1.0744)  time: 0.6328  data: 0.0002  max mem: 29573
[22:44:02.610456] Epoch: [0]  [ 5240/13501]  eta: 1:27:08  lr: 0.000024  closs: 0.3326 (1.0731)  mloss: 0.3326 (1.0731)  time: 0.6319  data: 0.0002  max mem: 29573
[22:44:08.903420] Epoch: [0]  [ 5250/13501]  eta: 1:27:01  lr: 0.000024  closs: 0.3664 (1.0717)  mloss: 0.3664 (1.0717)  time: 0.6319  data: 0.0002  max mem: 29573
[22:44:15.243321] Epoch: [0]  [ 5260/13501]  eta: 1:26:55  lr: 0.000024  closs: 0.3791 (1.0704)  mloss: 0.3791 (1.0704)  time: 0.6316  data: 0.0002  max mem: 29573
[22:44:21.532139] Epoch: [0]  [ 5270/13501]  eta: 1:26:48  lr: 0.000024  closs: 0.3391 (1.0691)  mloss: 0.3391 (1.0691)  time: 0.6314  data: 0.0002  max mem: 29573
[22:44:27.869620] Epoch: [0]  [ 5280/13501]  eta: 1:26:42  lr: 0.000024  closs: 0.3391 (1.0678)  mloss: 0.3391 (1.0678)  time: 0.6312  data: 0.0002  max mem: 29573
[22:44:34.158011] Epoch: [0]  [ 5290/13501]  eta: 1:26:36  lr: 0.000024  closs: 0.3214 (1.0664)  mloss: 0.3214 (1.0664)  time: 0.6312  data: 0.0002  max mem: 29573
[22:44:40.501280] Epoch: [0]  [ 5300/13501]  eta: 1:26:29  lr: 0.000025  closs: 0.3184 (1.0650)  mloss: 0.3184 (1.0650)  time: 0.6315  data: 0.0002  max mem: 29573
[22:44:46.788816] Epoch: [0]  [ 5310/13501]  eta: 1:26:23  lr: 0.000025  closs: 0.3276 (1.0636)  mloss: 0.3276 (1.0636)  time: 0.6315  data: 0.0002  max mem: 29573
[22:44:53.137745] Epoch: [0]  [ 5320/13501]  eta: 1:26:17  lr: 0.000025  closs: 0.3276 (1.0624)  mloss: 0.3276 (1.0624)  time: 0.6317  data: 0.0002  max mem: 29573
[22:44:59.438496] Epoch: [0]  [ 5330/13501]  eta: 1:26:10  lr: 0.000025  closs: 0.3338 (1.0610)  mloss: 0.3338 (1.0610)  time: 0.6324  data: 0.0002  max mem: 29573
[22:45:05.779926] Epoch: [0]  [ 5340/13501]  eta: 1:26:04  lr: 0.000025  closs: 0.3466 (1.0597)  mloss: 0.3466 (1.0597)  time: 0.6320  data: 0.0002  max mem: 29573
[22:45:12.077564] Epoch: [0]  [ 5350/13501]  eta: 1:25:58  lr: 0.000025  closs: 0.3482 (1.0583)  mloss: 0.3482 (1.0583)  time: 0.6319  data: 0.0003  max mem: 29573
[22:45:18.431257] Epoch: [0]  [ 5360/13501]  eta: 1:25:51  lr: 0.000025  closs: 0.3436 (1.0570)  mloss: 0.3436 (1.0570)  time: 0.6325  data: 0.0003  max mem: 29573
[22:45:24.735520] Epoch: [0]  [ 5370/13501]  eta: 1:25:45  lr: 0.000025  closs: 0.3738 (1.0558)  mloss: 0.3738 (1.0558)  time: 0.6328  data: 0.0002  max mem: 29573
[22:45:31.073515] Epoch: [0]  [ 5380/13501]  eta: 1:25:39  lr: 0.000025  closs: 0.3186 (1.0544)  mloss: 0.3186 (1.0544)  time: 0.6320  data: 0.0002  max mem: 29573
[22:45:37.369313] Epoch: [0]  [ 5390/13501]  eta: 1:25:32  lr: 0.000025  closs: 0.3201 (1.0531)  mloss: 0.3201 (1.0531)  time: 0.6316  data: 0.0002  max mem: 29573
[22:45:43.711274] Epoch: [0]  [ 5400/13501]  eta: 1:25:26  lr: 0.000025  closs: 0.3722 (1.0518)  mloss: 0.3722 (1.0518)  time: 0.6318  data: 0.0002  max mem: 29573
[22:45:50.000168] Epoch: [0]  [ 5410/13501]  eta: 1:25:20  lr: 0.000025  closs: 0.3433 (1.0505)  mloss: 0.3433 (1.0505)  time: 0.6315  data: 0.0002  max mem: 29573
[22:45:56.345561] Epoch: [0]  [ 5420/13501]  eta: 1:25:13  lr: 0.000025  closs: 0.3436 (1.0492)  mloss: 0.3436 (1.0492)  time: 0.6316  data: 0.0002  max mem: 29573
[22:46:02.662048] Epoch: [0]  [ 5430/13501]  eta: 1:25:07  lr: 0.000025  closs: 0.3223 (1.0479)  mloss: 0.3223 (1.0479)  time: 0.6330  data: 0.0002  max mem: 29573
[22:46:09.013735] Epoch: [0]  [ 5440/13501]  eta: 1:25:01  lr: 0.000025  closs: 0.3009 (1.0466)  mloss: 0.3009 (1.0466)  time: 0.6333  data: 0.0002  max mem: 29573
[22:46:15.299304] Epoch: [0]  [ 5450/13501]  eta: 1:24:54  lr: 0.000025  closs: 0.3487 (1.0453)  mloss: 0.3487 (1.0453)  time: 0.6318  data: 0.0002  max mem: 29573
[22:46:21.644430] Epoch: [0]  [ 5460/13501]  eta: 1:24:48  lr: 0.000025  closs: 0.3297 (1.0440)  mloss: 0.3297 (1.0440)  time: 0.6315  data: 0.0002  max mem: 29573
[22:46:27.928574] Epoch: [0]  [ 5470/13501]  eta: 1:24:42  lr: 0.000025  closs: 0.3297 (1.0427)  mloss: 0.3297 (1.0427)  time: 0.6314  data: 0.0002  max mem: 29573
[22:46:34.332429] Epoch: [0]  [ 5480/13501]  eta: 1:24:35  lr: 0.000025  closs: 0.3485 (1.0415)  mloss: 0.3485 (1.0415)  time: 0.6343  data: 0.0002  max mem: 29573
[22:46:40.619039] Epoch: [0]  [ 5490/13501]  eta: 1:24:29  lr: 0.000025  closs: 0.3433 (1.0402)  mloss: 0.3433 (1.0402)  time: 0.6345  data: 0.0002  max mem: 29573
[22:46:46.957656] Epoch: [0]  [ 5500/13501]  eta: 1:24:23  lr: 0.000025  closs: 0.3338 (1.0389)  mloss: 0.3338 (1.0389)  time: 0.6312  data: 0.0002  max mem: 29573
[22:46:53.251593] Epoch: [0]  [ 5510/13501]  eta: 1:24:16  lr: 0.000025  closs: 0.3376 (1.0377)  mloss: 0.3376 (1.0377)  time: 0.6315  data: 0.0002  max mem: 29573
[22:46:59.599972] Epoch: [0]  [ 5520/13501]  eta: 1:24:10  lr: 0.000026  closs: 0.3911 (1.0366)  mloss: 0.3911 (1.0366)  time: 0.6320  data: 0.0002  max mem: 29573
[22:47:05.905478] Epoch: [0]  [ 5530/13501]  eta: 1:24:04  lr: 0.000026  closs: 0.3751 (1.0353)  mloss: 0.3751 (1.0353)  time: 0.6326  data: 0.0002  max mem: 29573
[22:47:12.265681] Epoch: [0]  [ 5540/13501]  eta: 1:23:57  lr: 0.000026  closs: 0.3183 (1.0340)  mloss: 0.3183 (1.0340)  time: 0.6332  data: 0.0002  max mem: 29573
[22:47:18.552816] Epoch: [0]  [ 5550/13501]  eta: 1:23:51  lr: 0.000026  closs: 0.3183 (1.0327)  mloss: 0.3183 (1.0327)  time: 0.6323  data: 0.0002  max mem: 29573
[22:47:24.892058] Epoch: [0]  [ 5560/13501]  eta: 1:23:45  lr: 0.000026  closs: 0.3177 (1.0315)  mloss: 0.3177 (1.0315)  time: 0.6312  data: 0.0003  max mem: 29573
[22:47:31.185963] Epoch: [0]  [ 5570/13501]  eta: 1:23:38  lr: 0.000026  closs: 0.3795 (1.0303)  mloss: 0.3795 (1.0303)  time: 0.6316  data: 0.0003  max mem: 29573
[22:47:37.530278] Epoch: [0]  [ 5580/13501]  eta: 1:23:32  lr: 0.000026  closs: 0.3656 (1.0291)  mloss: 0.3656 (1.0291)  time: 0.6318  data: 0.0002  max mem: 29573
[22:47:43.820996] Epoch: [0]  [ 5590/13501]  eta: 1:23:26  lr: 0.000026  closs: 0.3667 (1.0279)  mloss: 0.3667 (1.0279)  time: 0.6317  data: 0.0002  max mem: 29573
[22:47:50.162340] Epoch: [0]  [ 5600/13501]  eta: 1:23:19  lr: 0.000026  closs: 0.3527 (1.0267)  mloss: 0.3527 (1.0267)  time: 0.6315  data: 0.0002  max mem: 29573
[22:47:56.454925] Epoch: [0]  [ 5610/13501]  eta: 1:23:13  lr: 0.000026  closs: 0.3142 (1.0255)  mloss: 0.3142 (1.0255)  time: 0.6316  data: 0.0002  max mem: 29573
[22:48:02.795961] Epoch: [0]  [ 5620/13501]  eta: 1:23:07  lr: 0.000026  closs: 0.3085 (1.0242)  mloss: 0.3085 (1.0242)  time: 0.6316  data: 0.0002  max mem: 29573
[22:48:09.079940] Epoch: [0]  [ 5630/13501]  eta: 1:23:00  lr: 0.000026  closs: 0.3282 (1.0230)  mloss: 0.3282 (1.0230)  time: 0.6312  data: 0.0002  max mem: 29573
[22:48:15.446898] Epoch: [0]  [ 5640/13501]  eta: 1:22:54  lr: 0.000026  closs: 0.3531 (1.0218)  mloss: 0.3531 (1.0218)  time: 0.6325  data: 0.0002  max mem: 29573
[22:48:21.740257] Epoch: [0]  [ 5650/13501]  eta: 1:22:48  lr: 0.000026  closs: 0.3531 (1.0207)  mloss: 0.3531 (1.0207)  time: 0.6329  data: 0.0002  max mem: 29573
[22:48:28.090380] Epoch: [0]  [ 5660/13501]  eta: 1:22:41  lr: 0.000026  closs: 0.3285 (1.0194)  mloss: 0.3285 (1.0194)  time: 0.6321  data: 0.0002  max mem: 29573
[22:48:34.379951] Epoch: [0]  [ 5670/13501]  eta: 1:22:35  lr: 0.000026  closs: 0.3247 (1.0183)  mloss: 0.3247 (1.0183)  time: 0.6319  data: 0.0002  max mem: 29573
[22:48:40.724038] Epoch: [0]  [ 5680/13501]  eta: 1:22:29  lr: 0.000026  closs: 0.3397 (1.0171)  mloss: 0.3397 (1.0171)  time: 0.6316  data: 0.0002  max mem: 29573
[22:48:47.014427] Epoch: [0]  [ 5690/13501]  eta: 1:22:22  lr: 0.000026  closs: 0.3514 (1.0159)  mloss: 0.3514 (1.0159)  time: 0.6316  data: 0.0002  max mem: 29573
[22:48:53.353138] Epoch: [0]  [ 5700/13501]  eta: 1:22:16  lr: 0.000026  closs: 0.3617 (1.0148)  mloss: 0.3617 (1.0148)  time: 0.6314  data: 0.0002  max mem: 29573
[22:48:59.640996] Epoch: [0]  [ 5710/13501]  eta: 1:22:09  lr: 0.000026  closs: 0.3396 (1.0136)  mloss: 0.3396 (1.0136)  time: 0.6312  data: 0.0002  max mem: 29573
[22:49:05.987023] Epoch: [0]  [ 5720/13501]  eta: 1:22:03  lr: 0.000026  closs: 0.3160 (1.0125)  mloss: 0.3160 (1.0125)  time: 0.6316  data: 0.0002  max mem: 29573
[22:49:12.272305] Epoch: [0]  [ 5730/13501]  eta: 1:21:57  lr: 0.000027  closs: 0.3482 (1.0113)  mloss: 0.3482 (1.0113)  time: 0.6315  data: 0.0002  max mem: 29573
[22:49:18.634507] Epoch: [0]  [ 5740/13501]  eta: 1:21:51  lr: 0.000027  closs: 0.3397 (1.0101)  mloss: 0.3397 (1.0101)  time: 0.6323  data: 0.0002  max mem: 29573
[22:49:24.946999] Epoch: [0]  [ 5750/13501]  eta: 1:21:44  lr: 0.000027  closs: 0.3201 (1.0090)  mloss: 0.3201 (1.0090)  time: 0.6337  data: 0.0002  max mem: 29573
[22:49:31.293121] Epoch: [0]  [ 5760/13501]  eta: 1:21:38  lr: 0.000027  closs: 0.3665 (1.0079)  mloss: 0.3665 (1.0079)  time: 0.6329  data: 0.0002  max mem: 29573
[22:49:37.584939] Epoch: [0]  [ 5770/13501]  eta: 1:21:31  lr: 0.000027  closs: 0.3858 (1.0068)  mloss: 0.3858 (1.0068)  time: 0.6318  data: 0.0002  max mem: 29573
[22:49:43.924749] Epoch: [0]  [ 5780/13501]  eta: 1:21:25  lr: 0.000027  closs: 0.3338 (1.0056)  mloss: 0.3338 (1.0056)  time: 0.6315  data: 0.0002  max mem: 29573
[22:49:50.213397] Epoch: [0]  [ 5790/13501]  eta: 1:21:19  lr: 0.000027  closs: 0.3132 (1.0045)  mloss: 0.3132 (1.0045)  time: 0.6313  data: 0.0002  max mem: 29573
[22:49:56.546979] Epoch: [0]  [ 5800/13501]  eta: 1:21:12  lr: 0.000027  closs: 0.3253 (1.0034)  mloss: 0.3253 (1.0034)  time: 0.6310  data: 0.0002  max mem: 29573
[22:50:02.843035] Epoch: [0]  [ 5810/13501]  eta: 1:21:06  lr: 0.000027  closs: 0.3253 (1.0023)  mloss: 0.3253 (1.0023)  time: 0.6314  data: 0.0002  max mem: 29573
[22:50:09.187560] Epoch: [0]  [ 5820/13501]  eta: 1:21:00  lr: 0.000027  closs: 0.3168 (1.0011)  mloss: 0.3168 (1.0011)  time: 0.6320  data: 0.0002  max mem: 29573
[22:50:15.474579] Epoch: [0]  [ 5830/13501]  eta: 1:20:53  lr: 0.000027  closs: 0.3168 (0.9999)  mloss: 0.3168 (0.9999)  time: 0.6315  data: 0.0002  max mem: 29573
[22:50:21.804733] Epoch: [0]  [ 5840/13501]  eta: 1:20:47  lr: 0.000027  closs: 0.3114 (0.9988)  mloss: 0.3114 (0.9988)  time: 0.6308  data: 0.0002  max mem: 29573
[22:50:28.108183] Epoch: [0]  [ 5850/13501]  eta: 1:20:41  lr: 0.000027  closs: 0.3499 (0.9976)  mloss: 0.3499 (0.9976)  time: 0.6316  data: 0.0002  max mem: 29573
[22:50:34.458554] Epoch: [0]  [ 5860/13501]  eta: 1:20:34  lr: 0.000027  closs: 0.3499 (0.9965)  mloss: 0.3499 (0.9965)  time: 0.6326  data: 0.0002  max mem: 29573
[22:50:40.745463] Epoch: [0]  [ 5870/13501]  eta: 1:20:28  lr: 0.000027  closs: 0.3547 (0.9954)  mloss: 0.3547 (0.9954)  time: 0.6318  data: 0.0002  max mem: 29573
[22:50:45.228332] /work/u8915687/big-superb/big-superb-train-data/SpeakerVerification_Aishell1Train/train/BAC009S0118W0478.wav
[22:50:47.095298] Epoch: [0]  [ 5880/13501]  eta: 1:20:22  lr: 0.000027  closs: 0.3589 (0.9944)  mloss: 0.3589 (0.9944)  time: 0.6318  data: 0.0002  max mem: 29573
[22:50:53.389759] Epoch: [0]  [ 5890/13501]  eta: 1:20:15  lr: 0.000027  closs: 0.3589 (0.9933)  mloss: 0.3589 (0.9933)  time: 0.6321  data: 0.0002  max mem: 29573
[22:50:59.732693] Epoch: [0]  [ 5900/13501]  eta: 1:20:09  lr: 0.000027  closs: 0.3310 (0.9922)  mloss: 0.3310 (0.9922)  time: 0.6318  data: 0.0002  max mem: 29573
[22:51:06.020394] Epoch: [0]  [ 5910/13501]  eta: 1:20:03  lr: 0.000027  closs: 0.3050 (0.9910)  mloss: 0.3050 (0.9910)  time: 0.6315  data: 0.0002  max mem: 29573
[22:51:12.366498] Epoch: [0]  [ 5920/13501]  eta: 1:19:56  lr: 0.000027  closs: 0.3416 (0.9899)  mloss: 0.3416 (0.9899)  time: 0.6316  data: 0.0002  max mem: 29573
[22:51:18.658577] Epoch: [0]  [ 5930/13501]  eta: 1:19:50  lr: 0.000027  closs: 0.3744 (0.9889)  mloss: 0.3744 (0.9889)  time: 0.6318  data: 0.0002  max mem: 29573
[22:51:25.016461] Epoch: [0]  [ 5940/13501]  eta: 1:19:44  lr: 0.000027  closs: 0.3456 (0.9878)  mloss: 0.3456 (0.9878)  time: 0.6324  data: 0.0002  max mem: 29573
[22:51:31.312111] Epoch: [0]  [ 5950/13501]  eta: 1:19:37  lr: 0.000028  closs: 0.3214 (0.9866)  mloss: 0.3214 (0.9866)  time: 0.6326  data: 0.0002  max mem: 29573
[22:51:37.673513] Epoch: [0]  [ 5960/13501]  eta: 1:19:31  lr: 0.000028  closs: 0.3014 (0.9856)  mloss: 0.3014 (0.9856)  time: 0.6328  data: 0.0003  max mem: 29573
[22:51:43.956958] Epoch: [0]  [ 5970/13501]  eta: 1:19:25  lr: 0.000028  closs: 0.2985 (0.9844)  mloss: 0.2985 (0.9844)  time: 0.6322  data: 0.0003  max mem: 29573
[22:51:50.305610] Epoch: [0]  [ 5980/13501]  eta: 1:19:18  lr: 0.000028  closs: 0.2985 (0.9834)  mloss: 0.2985 (0.9834)  time: 0.6315  data: 0.0002  max mem: 29573
[22:51:56.596791] Epoch: [0]  [ 5990/13501]  eta: 1:19:12  lr: 0.000028  closs: 0.2964 (0.9822)  mloss: 0.2964 (0.9822)  time: 0.6319  data: 0.0002  max mem: 29573
[22:52:02.936801] Epoch: [0]  [ 6000/13501]  eta: 1:19:06  lr: 0.000028  closs: 0.3432 (0.9812)  mloss: 0.3432 (0.9812)  time: 0.6315  data: 0.0002  max mem: 29573
[22:52:09.219977] Epoch: [0]  [ 6010/13501]  eta: 1:18:59  lr: 0.000028  closs: 0.3499 (0.9801)  mloss: 0.3499 (0.9801)  time: 0.6311  data: 0.0002  max mem: 29573
[22:52:15.561045] Epoch: [0]  [ 6020/13501]  eta: 1:18:53  lr: 0.000028  closs: 0.3286 (0.9790)  mloss: 0.3286 (0.9790)  time: 0.6311  data: 0.0002  max mem: 29573
[22:52:21.850913] Epoch: [0]  [ 6030/13501]  eta: 1:18:47  lr: 0.000028  closs: 0.3286 (0.9780)  mloss: 0.3286 (0.9780)  time: 0.6315  data: 0.0002  max mem: 29573
[22:52:28.200424] Epoch: [0]  [ 6040/13501]  eta: 1:18:40  lr: 0.000028  closs: 0.3624 (0.9770)  mloss: 0.3624 (0.9770)  time: 0.6319  data: 0.0002  max mem: 29573
[22:52:34.482726] Epoch: [0]  [ 6050/13501]  eta: 1:18:34  lr: 0.000028  closs: 0.3624 (0.9760)  mloss: 0.3624 (0.9760)  time: 0.6315  data: 0.0002  max mem: 29573
[22:52:40.842443] Epoch: [0]  [ 6060/13501]  eta: 1:18:28  lr: 0.000028  closs: 0.3654 (0.9750)  mloss: 0.3654 (0.9750)  time: 0.6320  data: 0.0002  max mem: 29573
[22:52:47.142983] Epoch: [0]  [ 6070/13501]  eta: 1:18:21  lr: 0.000028  closs: 0.3664 (0.9740)  mloss: 0.3664 (0.9740)  time: 0.6329  data: 0.0002  max mem: 29573
[22:52:50.607712] /work/u8915687/big-superb/big-superb-train-data/SpeechDetection_Aishell1Train/train/BAC009S0118W0478.wav
[22:52:53.490079] Epoch: [0]  [ 6080/13501]  eta: 1:18:15  lr: 0.000028  closs: 0.3683 (0.9730)  mloss: 0.3683 (0.9730)  time: 0.6323  data: 0.0002  max mem: 29573
[22:52:59.779640] Epoch: [0]  [ 6090/13501]  eta: 1:18:09  lr: 0.000028  closs: 0.3825 (0.9720)  mloss: 0.3825 (0.9720)  time: 0.6318  data: 0.0002  max mem: 29573
[22:53:06.119338] Epoch: [0]  [ 6100/13501]  eta: 1:18:02  lr: 0.000028  closs: 0.3666 (0.9710)  mloss: 0.3666 (0.9710)  time: 0.6314  data: 0.0002  max mem: 29573
[22:53:12.408948] Epoch: [0]  [ 6110/13501]  eta: 1:17:56  lr: 0.000028  closs: 0.3525 (0.9700)  mloss: 0.3525 (0.9700)  time: 0.6314  data: 0.0002  max mem: 29573
[22:53:18.756005] Epoch: [0]  [ 6120/13501]  eta: 1:17:50  lr: 0.000028  closs: 0.3427 (0.9690)  mloss: 0.3427 (0.9690)  time: 0.6318  data: 0.0003  max mem: 29573
[22:53:25.039800] Epoch: [0]  [ 6130/13501]  eta: 1:17:43  lr: 0.000028  closs: 0.3427 (0.9680)  mloss: 0.3427 (0.9680)  time: 0.6315  data: 0.0003  max mem: 29573
[22:53:31.386596] Epoch: [0]  [ 6140/13501]  eta: 1:17:37  lr: 0.000028  closs: 0.3240 (0.9670)  mloss: 0.3240 (0.9670)  time: 0.6315  data: 0.0003  max mem: 29573
[22:53:37.670547] Epoch: [0]  [ 6150/13501]  eta: 1:17:31  lr: 0.000028  closs: 0.3346 (0.9660)  mloss: 0.3346 (0.9660)  time: 0.6315  data: 0.0003  max mem: 29573
[22:53:44.008663] Epoch: [0]  [ 6160/13501]  eta: 1:17:24  lr: 0.000029  closs: 0.3477 (0.9650)  mloss: 0.3477 (0.9650)  time: 0.6310  data: 0.0002  max mem: 29573
[22:53:50.310989] Epoch: [0]  [ 6170/13501]  eta: 1:17:18  lr: 0.000029  closs: 0.3433 (0.9640)  mloss: 0.3433 (0.9640)  time: 0.6319  data: 0.0002  max mem: 29573
[22:53:56.654945] Epoch: [0]  [ 6180/13501]  eta: 1:17:12  lr: 0.000029  closs: 0.3332 (0.9630)  mloss: 0.3332 (0.9630)  time: 0.6322  data: 0.0002  max mem: 29573
[22:54:02.947280] Epoch: [0]  [ 6190/13501]  eta: 1:17:05  lr: 0.000029  closs: 0.3276 (0.9620)  mloss: 0.3276 (0.9620)  time: 0.6317  data: 0.0002  max mem: 29573
[22:54:09.287936] Epoch: [0]  [ 6200/13501]  eta: 1:16:59  lr: 0.000029  closs: 0.3213 (0.9610)  mloss: 0.3213 (0.9610)  time: 0.6316  data: 0.0002  max mem: 29573
[22:54:15.565551] Epoch: [0]  [ 6210/13501]  eta: 1:16:53  lr: 0.000029  closs: 0.3380 (0.9601)  mloss: 0.3380 (0.9601)  time: 0.6308  data: 0.0002  max mem: 29573
[22:54:21.904882] Epoch: [0]  [ 6220/13501]  eta: 1:16:46  lr: 0.000029  closs: 0.3681 (0.9591)  mloss: 0.3681 (0.9591)  time: 0.6308  data: 0.0002  max mem: 29573
[22:54:28.198369] Epoch: [0]  [ 6230/13501]  eta: 1:16:40  lr: 0.000029  closs: 0.3544 (0.9582)  mloss: 0.3544 (0.9582)  time: 0.6316  data: 0.0002  max mem: 29573
[22:54:34.542038] Epoch: [0]  [ 6240/13501]  eta: 1:16:34  lr: 0.000029  closs: 0.3290 (0.9572)  mloss: 0.3290 (0.9572)  time: 0.6318  data: 0.0002  max mem: 29573
[22:54:40.832550] Epoch: [0]  [ 6250/13501]  eta: 1:16:27  lr: 0.000029  closs: 0.3453 (0.9563)  mloss: 0.3453 (0.9563)  time: 0.6316  data: 0.0002  max mem: 29573
[22:54:47.173158] Epoch: [0]  [ 6260/13501]  eta: 1:16:21  lr: 0.000029  closs: 0.3639 (0.9553)  mloss: 0.3639 (0.9553)  time: 0.6315  data: 0.0002  max mem: 29573
[22:54:53.464744] Epoch: [0]  [ 6270/13501]  eta: 1:16:14  lr: 0.000029  closs: 0.3535 (0.9543)  mloss: 0.3535 (0.9543)  time: 0.6315  data: 0.0002  max mem: 29573
[22:54:59.821158] Epoch: [0]  [ 6280/13501]  eta: 1:16:08  lr: 0.000029  closs: 0.3359 (0.9533)  mloss: 0.3359 (0.9533)  time: 0.6323  data: 0.0002  max mem: 29573
[22:55:06.107810] Epoch: [0]  [ 6290/13501]  eta: 1:16:02  lr: 0.000029  closs: 0.3386 (0.9524)  mloss: 0.3386 (0.9524)  time: 0.6321  data: 0.0002  max mem: 29573
[22:55:12.446144] Epoch: [0]  [ 6300/13501]  eta: 1:15:55  lr: 0.000029  closs: 0.3274 (0.9514)  mloss: 0.3274 (0.9514)  time: 0.6312  data: 0.0002  max mem: 29573
[22:55:18.737029] Epoch: [0]  [ 6310/13501]  eta: 1:15:49  lr: 0.000029  closs: 0.3537 (0.9505)  mloss: 0.3537 (0.9505)  time: 0.6314  data: 0.0002  max mem: 29573
[22:55:25.075228] Epoch: [0]  [ 6320/13501]  eta: 1:15:43  lr: 0.000029  closs: 0.3797 (0.9496)  mloss: 0.3797 (0.9496)  time: 0.6314  data: 0.0002  max mem: 29573
[22:55:31.358997] Epoch: [0]  [ 6330/13501]  eta: 1:15:36  lr: 0.000029  closs: 0.3430 (0.9487)  mloss: 0.3430 (0.9487)  time: 0.6310  data: 0.0002  max mem: 29573
[22:55:37.710936] Epoch: [0]  [ 6340/13501]  eta: 1:15:30  lr: 0.000029  closs: 0.2967 (0.9477)  mloss: 0.2967 (0.9477)  time: 0.6317  data: 0.0002  max mem: 29573
[22:55:43.998874] Epoch: [0]  [ 6350/13501]  eta: 1:15:24  lr: 0.000029  closs: 0.3225 (0.9467)  mloss: 0.3225 (0.9467)  time: 0.6319  data: 0.0002  max mem: 29573
[22:55:50.339602] Epoch: [0]  [ 6360/13501]  eta: 1:15:17  lr: 0.000029  closs: 0.3374 (0.9457)  mloss: 0.3374 (0.9457)  time: 0.6314  data: 0.0002  max mem: 29573
[22:55:56.625694] Epoch: [0]  [ 6370/13501]  eta: 1:15:11  lr: 0.000029  closs: 0.3250 (0.9448)  mloss: 0.3250 (0.9448)  time: 0.6313  data: 0.0002  max mem: 29573
[22:56:02.974016] Epoch: [0]  [ 6380/13501]  eta: 1:15:05  lr: 0.000030  closs: 0.3482 (0.9439)  mloss: 0.3482 (0.9439)  time: 0.6316  data: 0.0002  max mem: 29573
[22:56:09.283754] Epoch: [0]  [ 6390/13501]  eta: 1:14:58  lr: 0.000030  closs: 0.3279 (0.9429)  mloss: 0.3279 (0.9429)  time: 0.6328  data: 0.0002  max mem: 29573
[22:56:15.627233] Epoch: [0]  [ 6400/13501]  eta: 1:14:52  lr: 0.000030  closs: 0.3111 (0.9420)  mloss: 0.3111 (0.9420)  time: 0.6326  data: 0.0002  max mem: 29573
[22:56:21.913381] Epoch: [0]  [ 6410/13501]  eta: 1:14:46  lr: 0.000030  closs: 0.3433 (0.9411)  mloss: 0.3433 (0.9411)  time: 0.6314  data: 0.0002  max mem: 29573
[22:56:28.251717] Epoch: [0]  [ 6420/13501]  eta: 1:14:39  lr: 0.000030  closs: 0.3831 (0.9402)  mloss: 0.3831 (0.9402)  time: 0.6311  data: 0.0002  max mem: 29573
[22:56:34.544356] Epoch: [0]  [ 6430/13501]  eta: 1:14:33  lr: 0.000030  closs: 0.3701 (0.9394)  mloss: 0.3701 (0.9394)  time: 0.6315  data: 0.0002  max mem: 29573
[22:56:40.885747] Epoch: [0]  [ 6440/13501]  eta: 1:14:27  lr: 0.000030  closs: 0.3339 (0.9384)  mloss: 0.3339 (0.9384)  time: 0.6316  data: 0.0002  max mem: 29573
[22:56:47.168128] Epoch: [0]  [ 6450/13501]  eta: 1:14:20  lr: 0.000030  closs: 0.3339 (0.9375)  mloss: 0.3339 (0.9375)  time: 0.6311  data: 0.0002  max mem: 29573
[22:56:53.507813] Epoch: [0]  [ 6460/13501]  eta: 1:14:14  lr: 0.000030  closs: 0.3210 (0.9366)  mloss: 0.3210 (0.9366)  time: 0.6310  data: 0.0002  max mem: 29573
[22:56:59.791978] Epoch: [0]  [ 6470/13501]  eta: 1:14:08  lr: 0.000030  closs: 0.3267 (0.9357)  mloss: 0.3267 (0.9357)  time: 0.6311  data: 0.0002  max mem: 29573
[22:57:06.136027] Epoch: [0]  [ 6480/13501]  eta: 1:14:01  lr: 0.000030  closs: 0.3826 (0.9348)  mloss: 0.3826 (0.9348)  time: 0.6313  data: 0.0003  max mem: 29573
[22:57:12.441463] Epoch: [0]  [ 6490/13501]  eta: 1:13:55  lr: 0.000030  closs: 0.3855 (0.9339)  mloss: 0.3855 (0.9339)  time: 0.6324  data: 0.0003  max mem: 29573
[22:57:18.793990] Epoch: [0]  [ 6500/13501]  eta: 1:13:49  lr: 0.000030  closs: 0.3419 (0.9330)  mloss: 0.3419 (0.9330)  time: 0.6328  data: 0.0002  max mem: 29573
[22:57:25.086044] Epoch: [0]  [ 6510/13501]  eta: 1:13:42  lr: 0.000030  closs: 0.3592 (0.9322)  mloss: 0.3592 (0.9322)  time: 0.6322  data: 0.0002  max mem: 29573
[22:57:31.434710] Epoch: [0]  [ 6520/13501]  eta: 1:13:36  lr: 0.000030  closs: 0.3313 (0.9313)  mloss: 0.3313 (0.9313)  time: 0.6320  data: 0.0002  max mem: 29573
[22:57:37.724671] Epoch: [0]  [ 6530/13501]  eta: 1:13:30  lr: 0.000030  closs: 0.3180 (0.9304)  mloss: 0.3180 (0.9304)  time: 0.6319  data: 0.0002  max mem: 29573
[22:57:44.068391] Epoch: [0]  [ 6540/13501]  eta: 1:13:23  lr: 0.000030  closs: 0.3227 (0.9295)  mloss: 0.3227 (0.9295)  time: 0.6316  data: 0.0003  max mem: 29573
[22:57:50.363266] Epoch: [0]  [ 6550/13501]  eta: 1:13:17  lr: 0.000030  closs: 0.3562 (0.9286)  mloss: 0.3562 (0.9286)  time: 0.6318  data: 0.0003  max mem: 29573
[22:57:56.703747] Epoch: [0]  [ 6560/13501]  eta: 1:13:11  lr: 0.000030  closs: 0.3268 (0.9276)  mloss: 0.3268 (0.9276)  time: 0.6317  data: 0.0002  max mem: 29573
[22:58:02.996934] Epoch: [0]  [ 6570/13501]  eta: 1:13:04  lr: 0.000030  closs: 0.3046 (0.9267)  mloss: 0.3046 (0.9267)  time: 0.6316  data: 0.0002  max mem: 29573
[22:58:09.336727] Epoch: [0]  [ 6580/13501]  eta: 1:12:58  lr: 0.000030  closs: 0.3421 (0.9259)  mloss: 0.3421 (0.9259)  time: 0.6316  data: 0.0002  max mem: 29573
[22:58:15.640330] Epoch: [0]  [ 6590/13501]  eta: 1:12:52  lr: 0.000030  closs: 0.3421 (0.9250)  mloss: 0.3421 (0.9250)  time: 0.6321  data: 0.0002  max mem: 29573
[22:58:21.997171] Epoch: [0]  [ 6600/13501]  eta: 1:12:45  lr: 0.000031  closs: 0.3266 (0.9241)  mloss: 0.3266 (0.9241)  time: 0.6329  data: 0.0002  max mem: 29573
[22:58:28.282615] Epoch: [0]  [ 6610/13501]  eta: 1:12:39  lr: 0.000031  closs: 0.3292 (0.9232)  mloss: 0.3292 (0.9232)  time: 0.6320  data: 0.0002  max mem: 29573
[22:58:34.623316] Epoch: [0]  [ 6620/13501]  eta: 1:12:33  lr: 0.000031  closs: 0.3107 (0.9222)  mloss: 0.3107 (0.9222)  time: 0.6312  data: 0.0002  max mem: 29573
[22:58:40.916977] Epoch: [0]  [ 6630/13501]  eta: 1:12:26  lr: 0.000031  closs: 0.3278 (0.9214)  mloss: 0.3278 (0.9214)  time: 0.6316  data: 0.0002  max mem: 29573
[22:58:47.256146] Epoch: [0]  [ 6640/13501]  eta: 1:12:20  lr: 0.000031  closs: 0.3397 (0.9205)  mloss: 0.3397 (0.9205)  time: 0.6316  data: 0.0003  max mem: 29573
[22:58:53.539375] Epoch: [0]  [ 6650/13501]  eta: 1:12:14  lr: 0.000031  closs: 0.3466 (0.9197)  mloss: 0.3466 (0.9197)  time: 0.6310  data: 0.0003  max mem: 29573
[22:58:59.877657] Epoch: [0]  [ 6660/13501]  eta: 1:12:07  lr: 0.000031  closs: 0.3470 (0.9188)  mloss: 0.3470 (0.9188)  time: 0.6310  data: 0.0002  max mem: 29573
[22:59:06.170481] Epoch: [0]  [ 6670/13501]  eta: 1:12:01  lr: 0.000031  closs: 0.3470 (0.9180)  mloss: 0.3470 (0.9180)  time: 0.6315  data: 0.0002  max mem: 29573
[22:59:12.519373] Epoch: [0]  [ 6680/13501]  eta: 1:11:55  lr: 0.000031  closs: 0.3547 (0.9171)  mloss: 0.3547 (0.9171)  time: 0.6320  data: 0.0002  max mem: 29573
[22:59:18.800637] Epoch: [0]  [ 6690/13501]  eta: 1:11:48  lr: 0.000031  closs: 0.3547 (0.9163)  mloss: 0.3547 (0.9163)  time: 0.6314  data: 0.0002  max mem: 29573
[22:59:25.155780] Epoch: [0]  [ 6700/13501]  eta: 1:11:42  lr: 0.000031  closs: 0.3495 (0.9154)  mloss: 0.3495 (0.9154)  time: 0.6317  data: 0.0002  max mem: 29573
[22:59:31.446793] Epoch: [0]  [ 6710/13501]  eta: 1:11:36  lr: 0.000031  closs: 0.3562 (0.9147)  mloss: 0.3562 (0.9147)  time: 0.6322  data: 0.0002  max mem: 29573
[22:59:37.781834] Epoch: [0]  [ 6720/13501]  eta: 1:11:29  lr: 0.000031  closs: 0.3667 (0.9138)  mloss: 0.3667 (0.9138)  time: 0.6312  data: 0.0002  max mem: 29573
[22:59:44.075953] Epoch: [0]  [ 6730/13501]  eta: 1:11:23  lr: 0.000031  closs: 0.3524 (0.9130)  mloss: 0.3524 (0.9130)  time: 0.6314  data: 0.0002  max mem: 29573
[22:59:50.420712] Epoch: [0]  [ 6740/13501]  eta: 1:11:17  lr: 0.000031  closs: 0.3402 (0.9122)  mloss: 0.3402 (0.9122)  time: 0.6319  data: 0.0003  max mem: 29573
[22:59:56.712940] Epoch: [0]  [ 6750/13501]  eta: 1:11:10  lr: 0.000031  closs: 0.3401 (0.9113)  mloss: 0.3401 (0.9113)  time: 0.6318  data: 0.0003  max mem: 29573
[23:00:03.052068] Epoch: [0]  [ 6760/13501]  eta: 1:11:04  lr: 0.000031  closs: 0.3437 (0.9105)  mloss: 0.3437 (0.9105)  time: 0.6315  data: 0.0002  max mem: 29573
[23:00:09.337150] Epoch: [0]  [ 6770/13501]  eta: 1:10:58  lr: 0.000031  closs: 0.3349 (0.9096)  mloss: 0.3349 (0.9096)  time: 0.6311  data: 0.0002  max mem: 29573
[23:00:15.682374] Epoch: [0]  [ 6780/13501]  eta: 1:10:51  lr: 0.000031  closs: 0.2887 (0.9087)  mloss: 0.2887 (0.9087)  time: 0.6314  data: 0.0003  max mem: 29573
[23:00:21.969169] Epoch: [0]  [ 6790/13501]  eta: 1:10:45  lr: 0.000031  closs: 0.2963 (0.9078)  mloss: 0.2963 (0.9078)  time: 0.6315  data: 0.0003  max mem: 29573
[23:00:28.311799] Epoch: [0]  [ 6800/13501]  eta: 1:10:39  lr: 0.000031  closs: 0.3375 (0.9070)  mloss: 0.3375 (0.9070)  time: 0.6314  data: 0.0002  max mem: 29573
[23:00:34.617292] Epoch: [0]  [ 6810/13501]  eta: 1:10:32  lr: 0.000032  closs: 0.3527 (0.9062)  mloss: 0.3527 (0.9062)  time: 0.6323  data: 0.0003  max mem: 29573
[23:00:40.955256] Epoch: [0]  [ 6820/13501]  eta: 1:10:26  lr: 0.000032  closs: 0.3298 (0.9053)  mloss: 0.3298 (0.9053)  time: 0.6321  data: 0.0003  max mem: 29573
[23:00:47.309132] Epoch: [0]  [ 6830/13501]  eta: 1:10:20  lr: 0.000032  closs: 0.3299 (0.9045)  mloss: 0.3299 (0.9045)  time: 0.6345  data: 0.0002  max mem: 29573
[23:00:53.649514] Epoch: [0]  [ 6840/13501]  eta: 1:10:13  lr: 0.000032  closs: 0.3552 (0.9037)  mloss: 0.3552 (0.9037)  time: 0.6346  data: 0.0002  max mem: 29573
[23:00:59.938411] Epoch: [0]  [ 6850/13501]  eta: 1:10:07  lr: 0.000032  closs: 0.3642 (0.9029)  mloss: 0.3642 (0.9029)  time: 0.6314  data: 0.0002  max mem: 29573
[23:01:06.276454] Epoch: [0]  [ 6860/13501]  eta: 1:10:01  lr: 0.000032  closs: 0.3230 (0.9021)  mloss: 0.3230 (0.9021)  time: 0.6313  data: 0.0003  max mem: 29573
[23:01:12.560023] Epoch: [0]  [ 6870/13501]  eta: 1:09:54  lr: 0.000032  closs: 0.3438 (0.9013)  mloss: 0.3438 (0.9013)  time: 0.6310  data: 0.0003  max mem: 29573
[23:01:18.899155] Epoch: [0]  [ 6880/13501]  eta: 1:09:48  lr: 0.000032  closs: 0.3789 (0.9006)  mloss: 0.3789 (0.9006)  time: 0.6311  data: 0.0002  max mem: 29573
[23:01:25.186129] Epoch: [0]  [ 6890/13501]  eta: 1:09:42  lr: 0.000032  closs: 0.3375 (0.8997)  mloss: 0.3375 (0.8997)  time: 0.6312  data: 0.0002  max mem: 29573
[23:01:31.526535] Epoch: [0]  [ 6900/13501]  eta: 1:09:35  lr: 0.000032  closs: 0.3375 (0.8990)  mloss: 0.3375 (0.8990)  time: 0.6313  data: 0.0002  max mem: 29573
[23:01:37.823006] Epoch: [0]  [ 6910/13501]  eta: 1:09:29  lr: 0.000032  closs: 0.3585 (0.8982)  mloss: 0.3585 (0.8982)  time: 0.6318  data: 0.0002  max mem: 29573
[23:01:44.175202] Epoch: [0]  [ 6920/13501]  eta: 1:09:23  lr: 0.000032  closs: 0.3299 (0.8973)  mloss: 0.3299 (0.8973)  time: 0.6324  data: 0.0002  max mem: 29573
[23:01:50.462744] Epoch: [0]  [ 6930/13501]  eta: 1:09:16  lr: 0.000032  closs: 0.3299 (0.8966)  mloss: 0.3299 (0.8966)  time: 0.6319  data: 0.0002  max mem: 29573
[23:01:56.807005] Epoch: [0]  [ 6940/13501]  eta: 1:09:10  lr: 0.000032  closs: 0.3501 (0.8958)  mloss: 0.3501 (0.8958)  time: 0.6315  data: 0.0002  max mem: 29573
[23:02:03.098677] Epoch: [0]  [ 6950/13501]  eta: 1:09:04  lr: 0.000032  closs: 0.3252 (0.8949)  mloss: 0.3252 (0.8949)  time: 0.6317  data: 0.0003  max mem: 29573
[23:02:09.435616] Epoch: [0]  [ 6960/13501]  eta: 1:08:57  lr: 0.000032  closs: 0.3043 (0.8941)  mloss: 0.3043 (0.8941)  time: 0.6313  data: 0.0003  max mem: 29573
[23:02:15.723558] Epoch: [0]  [ 6970/13501]  eta: 1:08:51  lr: 0.000032  closs: 0.2890 (0.8933)  mloss: 0.2890 (0.8933)  time: 0.6312  data: 0.0002  max mem: 29573
[23:02:22.066460] Epoch: [0]  [ 6980/13501]  eta: 1:08:45  lr: 0.000032  closs: 0.3017 (0.8924)  mloss: 0.3017 (0.8924)  time: 0.6315  data: 0.0002  max mem: 29573
[23:02:28.351235] Epoch: [0]  [ 6990/13501]  eta: 1:08:38  lr: 0.000032  closs: 0.3227 (0.8916)  mloss: 0.3227 (0.8916)  time: 0.6313  data: 0.0002  max mem: 29573
[23:02:34.694509] Epoch: [0]  [ 7000/13501]  eta: 1:08:32  lr: 0.000032  closs: 0.3253 (0.8908)  mloss: 0.3253 (0.8908)  time: 0.6313  data: 0.0003  max mem: 29573
[23:02:40.988538] Epoch: [0]  [ 7010/13501]  eta: 1:08:26  lr: 0.000032  closs: 0.3188 (0.8901)  mloss: 0.3188 (0.8901)  time: 0.6318  data: 0.0002  max mem: 29573
[23:02:47.350359] Epoch: [0]  [ 7020/13501]  eta: 1:08:19  lr: 0.000032  closs: 0.3188 (0.8893)  mloss: 0.3188 (0.8893)  time: 0.6327  data: 0.0002  max mem: 29573
[23:02:53.649774] Epoch: [0]  [ 7030/13501]  eta: 1:08:13  lr: 0.000033  closs: 0.3532 (0.8885)  mloss: 0.3532 (0.8885)  time: 0.6330  data: 0.0002  max mem: 29573
[23:02:59.993371] Epoch: [0]  [ 7040/13501]  eta: 1:08:07  lr: 0.000033  closs: 0.3532 (0.8878)  mloss: 0.3532 (0.8878)  time: 0.6321  data: 0.0002  max mem: 29573
[23:03:06.286264] Epoch: [0]  [ 7050/13501]  eta: 1:08:00  lr: 0.000033  closs: 0.3483 (0.8870)  mloss: 0.3483 (0.8870)  time: 0.6317  data: 0.0002  max mem: 29573
[23:03:12.626178] Epoch: [0]  [ 7060/13501]  eta: 1:07:54  lr: 0.000033  closs: 0.3512 (0.8863)  mloss: 0.3512 (0.8863)  time: 0.6316  data: 0.0002  max mem: 29573
[23:03:18.916938] Epoch: [0]  [ 7070/13501]  eta: 1:07:48  lr: 0.000033  closs: 0.3436 (0.8855)  mloss: 0.3436 (0.8855)  time: 0.6315  data: 0.0003  max mem: 29573
[23:03:25.265261] Epoch: [0]  [ 7080/13501]  eta: 1:07:41  lr: 0.000033  closs: 0.3504 (0.8847)  mloss: 0.3504 (0.8847)  time: 0.6319  data: 0.0003  max mem: 29573
[23:03:31.553413] Epoch: [0]  [ 7090/13501]  eta: 1:07:35  lr: 0.000033  closs: 0.3537 (0.8840)  mloss: 0.3537 (0.8840)  time: 0.6317  data: 0.0002  max mem: 29573
[23:03:37.891677] Epoch: [0]  [ 7100/13501]  eta: 1:07:29  lr: 0.000033  closs: 0.3346 (0.8832)  mloss: 0.3346 (0.8832)  time: 0.6312  data: 0.0002  max mem: 29573
[23:03:44.171045] Epoch: [0]  [ 7110/13501]  eta: 1:07:22  lr: 0.000033  closs: 0.3578 (0.8825)  mloss: 0.3578 (0.8825)  time: 0.6308  data: 0.0002  max mem: 29573
[23:03:50.527670] Epoch: [0]  [ 7120/13501]  eta: 1:07:16  lr: 0.000033  closs: 0.3578 (0.8817)  mloss: 0.3578 (0.8817)  time: 0.6317  data: 0.0002  max mem: 29573
[23:03:56.849211] Epoch: [0]  [ 7130/13501]  eta: 1:07:10  lr: 0.000033  closs: 0.3330 (0.8810)  mloss: 0.3330 (0.8810)  time: 0.6338  data: 0.0002  max mem: 29573
[23:04:03.193931] Epoch: [0]  [ 7140/13501]  eta: 1:07:03  lr: 0.000033  closs: 0.3828 (0.8803)  mloss: 0.3828 (0.8803)  time: 0.6332  data: 0.0002  max mem: 29573
[23:04:09.486477] Epoch: [0]  [ 7150/13501]  eta: 1:06:57  lr: 0.000033  closs: 0.3247 (0.8795)  mloss: 0.3247 (0.8795)  time: 0.6318  data: 0.0002  max mem: 29573
[23:04:15.826601] Epoch: [0]  [ 7160/13501]  eta: 1:06:51  lr: 0.000033  closs: 0.3610 (0.8788)  mloss: 0.3610 (0.8788)  time: 0.6316  data: 0.0002  max mem: 29573
[23:04:22.106997] Epoch: [0]  [ 7170/13501]  eta: 1:06:44  lr: 0.000033  closs: 0.3404 (0.8781)  mloss: 0.3404 (0.8781)  time: 0.6309  data: 0.0002  max mem: 29573
[23:04:28.449328] Epoch: [0]  [ 7180/13501]  eta: 1:06:38  lr: 0.000033  closs: 0.3179 (0.8773)  mloss: 0.3179 (0.8773)  time: 0.6311  data: 0.0002  max mem: 29573
[23:04:34.740891] Epoch: [0]  [ 7190/13501]  eta: 1:06:32  lr: 0.000033  closs: 0.3179 (0.8766)  mloss: 0.3179 (0.8766)  time: 0.6316  data: 0.0002  max mem: 29573
[23:04:41.078267] Epoch: [0]  [ 7200/13501]  eta: 1:06:25  lr: 0.000033  closs: 0.3947 (0.8760)  mloss: 0.3947 (0.8760)  time: 0.6314  data: 0.0002  max mem: 29573
[23:04:47.369850] Epoch: [0]  [ 7210/13501]  eta: 1:06:19  lr: 0.000033  closs: 0.3613 (0.8752)  mloss: 0.3613 (0.8752)  time: 0.6314  data: 0.0002  max mem: 29573
[23:04:53.715846] Epoch: [0]  [ 7220/13501]  eta: 1:06:13  lr: 0.000033  closs: 0.3410 (0.8745)  mloss: 0.3410 (0.8745)  time: 0.6318  data: 0.0002  max mem: 29573
[23:05:00.012900] Epoch: [0]  [ 7230/13501]  eta: 1:06:06  lr: 0.000033  closs: 0.3544 (0.8738)  mloss: 0.3544 (0.8738)  time: 0.6321  data: 0.0002  max mem: 29573
[23:05:06.363097] Epoch: [0]  [ 7240/13501]  eta: 1:06:00  lr: 0.000034  closs: 0.3735 (0.8730)  mloss: 0.3735 (0.8730)  time: 0.6323  data: 0.0002  max mem: 29573
[23:05:12.657337] Epoch: [0]  [ 7250/13501]  eta: 1:05:54  lr: 0.000034  closs: 0.3361 (0.8723)  mloss: 0.3361 (0.8723)  time: 0.6321  data: 0.0002  max mem: 29573
[23:05:18.993786] Epoch: [0]  [ 7260/13501]  eta: 1:05:47  lr: 0.000034  closs: 0.3332 (0.8716)  mloss: 0.3332 (0.8716)  time: 0.6315  data: 0.0002  max mem: 29573
[23:05:25.288508] Epoch: [0]  [ 7270/13501]  eta: 1:05:41  lr: 0.000034  closs: 0.3332 (0.8708)  mloss: 0.3332 (0.8708)  time: 0.6315  data: 0.0002  max mem: 29573
[23:05:31.635705] Epoch: [0]  [ 7280/13501]  eta: 1:05:35  lr: 0.000034  closs: 0.3344 (0.8701)  mloss: 0.3344 (0.8701)  time: 0.6320  data: 0.0002  max mem: 29573
[23:05:37.930244] Epoch: [0]  [ 7290/13501]  eta: 1:05:28  lr: 0.000034  closs: 0.3349 (0.8694)  mloss: 0.3349 (0.8694)  time: 0.6320  data: 0.0002  max mem: 29573
[23:05:44.273705] Epoch: [0]  [ 7300/13501]  eta: 1:05:22  lr: 0.000034  closs: 0.3289 (0.8687)  mloss: 0.3289 (0.8687)  time: 0.6318  data: 0.0002  max mem: 29573
[23:05:50.559716] Epoch: [0]  [ 7310/13501]  eta: 1:05:16  lr: 0.000034  closs: 0.3289 (0.8680)  mloss: 0.3289 (0.8680)  time: 0.6314  data: 0.0002  max mem: 29573
[23:05:56.910439] Epoch: [0]  [ 7320/13501]  eta: 1:05:09  lr: 0.000034  closs: 0.3188 (0.8673)  mloss: 0.3188 (0.8673)  time: 0.6318  data: 0.0002  max mem: 29573
[23:06:03.198412] Epoch: [0]  [ 7330/13501]  eta: 1:05:03  lr: 0.000034  closs: 0.3280 (0.8666)  mloss: 0.3280 (0.8666)  time: 0.6319  data: 0.0002  max mem: 29573
[23:06:09.561432] Epoch: [0]  [ 7340/13501]  eta: 1:04:57  lr: 0.000034  closs: 0.3439 (0.8659)  mloss: 0.3439 (0.8659)  time: 0.6325  data: 0.0002  max mem: 29573
[23:06:15.860397] Epoch: [0]  [ 7350/13501]  eta: 1:04:50  lr: 0.000034  closs: 0.3397 (0.8651)  mloss: 0.3397 (0.8651)  time: 0.6330  data: 0.0003  max mem: 29573
[23:06:22.213141] Epoch: [0]  [ 7360/13501]  eta: 1:04:44  lr: 0.000034  closs: 0.3502 (0.8645)  mloss: 0.3502 (0.8645)  time: 0.6325  data: 0.0003  max mem: 29573
[23:06:28.508003] Epoch: [0]  [ 7370/13501]  eta: 1:04:38  lr: 0.000034  closs: 0.3367 (0.8637)  mloss: 0.3367 (0.8637)  time: 0.6323  data: 0.0002  max mem: 29573
[23:06:34.845800] Epoch: [0]  [ 7380/13501]  eta: 1:04:31  lr: 0.000034  closs: 0.3208 (0.8631)  mloss: 0.3208 (0.8631)  time: 0.6316  data: 0.0002  max mem: 29573
[23:06:41.136118] Epoch: [0]  [ 7390/13501]  eta: 1:04:25  lr: 0.000034  closs: 0.3209 (0.8624)  mloss: 0.3209 (0.8624)  time: 0.6313  data: 0.0002  max mem: 29573
[23:06:47.481134] Epoch: [0]  [ 7400/13501]  eta: 1:04:19  lr: 0.000034  closs: 0.3418 (0.8617)  mloss: 0.3418 (0.8617)  time: 0.6317  data: 0.0002  max mem: 29573
[23:06:53.771269] Epoch: [0]  [ 7410/13501]  eta: 1:04:12  lr: 0.000034  closs: 0.3249 (0.8609)  mloss: 0.3249 (0.8609)  time: 0.6317  data: 0.0002  max mem: 29573
[23:07:00.113685] Epoch: [0]  [ 7420/13501]  eta: 1:04:06  lr: 0.000034  closs: 0.3249 (0.8602)  mloss: 0.3249 (0.8602)  time: 0.6315  data: 0.0002  max mem: 29573
[23:07:06.402884] Epoch: [0]  [ 7430/13501]  eta: 1:04:00  lr: 0.000034  closs: 0.3283 (0.8595)  mloss: 0.3283 (0.8595)  time: 0.6315  data: 0.0002  max mem: 29573
[23:07:12.751399] Epoch: [0]  [ 7440/13501]  eta: 1:03:53  lr: 0.000034  closs: 0.3152 (0.8588)  mloss: 0.3152 (0.8588)  time: 0.6318  data: 0.0002  max mem: 29573
[23:07:19.055023] Epoch: [0]  [ 7450/13501]  eta: 1:03:47  lr: 0.000034  closs: 0.3152 (0.8581)  mloss: 0.3152 (0.8581)  time: 0.6325  data: 0.0002  max mem: 29573
[23:07:25.398620] Epoch: [0]  [ 7460/13501]  eta: 1:03:41  lr: 0.000035  closs: 0.3448 (0.8575)  mloss: 0.3448 (0.8575)  time: 0.6323  data: 0.0002  max mem: 29573
[23:07:31.679968] Epoch: [0]  [ 7470/13501]  eta: 1:03:34  lr: 0.000035  closs: 0.3399 (0.8568)  mloss: 0.3399 (0.8568)  time: 0.6312  data: 0.0002  max mem: 29573
[23:07:38.021543] Epoch: [0]  [ 7480/13501]  eta: 1:03:28  lr: 0.000035  closs: 0.3296 (0.8561)  mloss: 0.3296 (0.8561)  time: 0.6311  data: 0.0002  max mem: 29573
[23:07:44.312938] Epoch: [0]  [ 7490/13501]  eta: 1:03:22  lr: 0.000035  closs: 0.3409 (0.8555)  mloss: 0.3409 (0.8555)  time: 0.6316  data: 0.0002  max mem: 29573
[23:07:50.652499] Epoch: [0]  [ 7500/13501]  eta: 1:03:15  lr: 0.000035  closs: 0.3497 (0.8547)  mloss: 0.3497 (0.8547)  time: 0.6315  data: 0.0002  max mem: 29573
[23:07:56.939712] Epoch: [0]  [ 7510/13501]  eta: 1:03:09  lr: 0.000035  closs: 0.3087 (0.8540)  mloss: 0.3087 (0.8540)  time: 0.6313  data: 0.0002  max mem: 29573
[23:08:03.287597] Epoch: [0]  [ 7520/13501]  eta: 1:03:03  lr: 0.000035  closs: 0.3138 (0.8534)  mloss: 0.3138 (0.8534)  time: 0.6317  data: 0.0002  max mem: 29573
[23:08:09.580243] Epoch: [0]  [ 7530/13501]  eta: 1:02:56  lr: 0.000035  closs: 0.3350 (0.8527)  mloss: 0.3350 (0.8527)  time: 0.6319  data: 0.0002  max mem: 29573
[23:08:15.925000] Epoch: [0]  [ 7540/13501]  eta: 1:02:50  lr: 0.000035  closs: 0.3392 (0.8520)  mloss: 0.3392 (0.8520)  time: 0.6318  data: 0.0002  max mem: 29573
[23:08:22.229635] Epoch: [0]  [ 7550/13501]  eta: 1:02:44  lr: 0.000035  closs: 0.3469 (0.8514)  mloss: 0.3469 (0.8514)  time: 0.6324  data: 0.0002  max mem: 29573
[23:08:28.586451] Epoch: [0]  [ 7560/13501]  eta: 1:02:37  lr: 0.000035  closs: 0.3408 (0.8507)  mloss: 0.3408 (0.8507)  time: 0.6330  data: 0.0002  max mem: 29573
[23:08:34.880316] Epoch: [0]  [ 7570/13501]  eta: 1:02:31  lr: 0.000035  closs: 0.3049 (0.8500)  mloss: 0.3049 (0.8500)  time: 0.6325  data: 0.0002  max mem: 29573
[23:08:41.213647] Epoch: [0]  [ 7580/13501]  eta: 1:02:25  lr: 0.000035  closs: 0.3049 (0.8493)  mloss: 0.3049 (0.8493)  time: 0.6313  data: 0.0002  max mem: 29573
[23:08:47.500790] Epoch: [0]  [ 7590/13501]  eta: 1:02:18  lr: 0.000035  closs: 0.3067 (0.8486)  mloss: 0.3067 (0.8486)  time: 0.6309  data: 0.0002  max mem: 29573
[23:08:53.836990] Epoch: [0]  [ 7600/13501]  eta: 1:02:12  lr: 0.000035  closs: 0.3022 (0.8479)  mloss: 0.3022 (0.8479)  time: 0.6311  data: 0.0002  max mem: 29573
[23:09:00.122719] Epoch: [0]  [ 7610/13501]  eta: 1:02:06  lr: 0.000035  closs: 0.3397 (0.8473)  mloss: 0.3397 (0.8473)  time: 0.6310  data: 0.0002  max mem: 29573
[23:09:06.466431] Epoch: [0]  [ 7620/13501]  eta: 1:01:59  lr: 0.000035  closs: 0.3494 (0.8467)  mloss: 0.3494 (0.8467)  time: 0.6314  data: 0.0003  max mem: 29573
[23:09:12.757870] Epoch: [0]  [ 7630/13501]  eta: 1:01:53  lr: 0.000035  closs: 0.3702 (0.8461)  mloss: 0.3702 (0.8461)  time: 0.6317  data: 0.0003  max mem: 29573
[23:09:19.102298] Epoch: [0]  [ 7640/13501]  eta: 1:01:47  lr: 0.000035  closs: 0.3476 (0.8454)  mloss: 0.3476 (0.8454)  time: 0.6317  data: 0.0002  max mem: 29573
[23:09:25.394835] Epoch: [0]  [ 7650/13501]  eta: 1:01:40  lr: 0.000035  closs: 0.3279 (0.8447)  mloss: 0.3279 (0.8447)  time: 0.6318  data: 0.0002  max mem: 29573
[23:09:31.755153] Epoch: [0]  [ 7660/13501]  eta: 1:01:34  lr: 0.000035  closs: 0.3141 (0.8440)  mloss: 0.3141 (0.8440)  time: 0.6326  data: 0.0002  max mem: 29573
[23:09:38.033450] Epoch: [0]  [ 7670/13501]  eta: 1:01:28  lr: 0.000035  closs: 0.3083 (0.8434)  mloss: 0.3083 (0.8434)  time: 0.6319  data: 0.0002  max mem: 29573
[23:09:44.367785] Epoch: [0]  [ 7680/13501]  eta: 1:01:21  lr: 0.000036  closs: 0.3543 (0.8428)  mloss: 0.3543 (0.8428)  time: 0.6306  data: 0.0002  max mem: 29573
[23:09:50.652815] Epoch: [0]  [ 7690/13501]  eta: 1:01:15  lr: 0.000036  closs: 0.3631 (0.8421)  mloss: 0.3631 (0.8421)  time: 0.6309  data: 0.0002  max mem: 29573
[23:09:56.991001] Epoch: [0]  [ 7700/13501]  eta: 1:01:09  lr: 0.000036  closs: 0.3380 (0.8415)  mloss: 0.3380 (0.8415)  time: 0.6311  data: 0.0002  max mem: 29573
[23:10:03.281381] Epoch: [0]  [ 7710/13501]  eta: 1:01:02  lr: 0.000036  closs: 0.3332 (0.8408)  mloss: 0.3332 (0.8408)  time: 0.6313  data: 0.0002  max mem: 29573
[23:10:09.628357] Epoch: [0]  [ 7720/13501]  eta: 1:00:56  lr: 0.000036  closs: 0.3241 (0.8402)  mloss: 0.3241 (0.8402)  time: 0.6318  data: 0.0002  max mem: 29573
[23:10:15.918079] Epoch: [0]  [ 7730/13501]  eta: 1:00:50  lr: 0.000036  closs: 0.3530 (0.8395)  mloss: 0.3530 (0.8395)  time: 0.6318  data: 0.0002  max mem: 29573
[23:10:22.262718] Epoch: [0]  [ 7740/13501]  eta: 1:00:43  lr: 0.000036  closs: 0.3530 (0.8389)  mloss: 0.3530 (0.8389)  time: 0.6316  data: 0.0002  max mem: 29573
[23:10:28.547580] Epoch: [0]  [ 7750/13501]  eta: 1:00:37  lr: 0.000036  closs: 0.3360 (0.8382)  mloss: 0.3360 (0.8382)  time: 0.6314  data: 0.0002  max mem: 29573
[23:10:34.901382] Epoch: [0]  [ 7760/13501]  eta: 1:00:31  lr: 0.000036  closs: 0.3360 (0.8376)  mloss: 0.3360 (0.8376)  time: 0.6319  data: 0.0002  max mem: 29573
[23:10:41.203314] Epoch: [0]  [ 7770/13501]  eta: 1:00:24  lr: 0.000036  closs: 0.3420 (0.8370)  mloss: 0.3420 (0.8370)  time: 0.6327  data: 0.0002  max mem: 29573
[23:10:47.555068] Epoch: [0]  [ 7780/13501]  eta: 1:00:18  lr: 0.000036  closs: 0.3171 (0.8363)  mloss: 0.3171 (0.8363)  time: 0.6326  data: 0.0002  max mem: 29573
[23:10:53.842644] Epoch: [0]  [ 7790/13501]  eta: 1:00:12  lr: 0.000036  closs: 0.3311 (0.8356)  mloss: 0.3311 (0.8356)  time: 0.6319  data: 0.0002  max mem: 29573
[23:11:00.178610] Epoch: [0]  [ 7800/13501]  eta: 1:00:05  lr: 0.000036  closs: 0.3251 (0.8350)  mloss: 0.3251 (0.8350)  time: 0.6311  data: 0.0002  max mem: 29573
[23:11:06.458097] Epoch: [0]  [ 7810/13501]  eta: 0:59:59  lr: 0.000036  closs: 0.3314 (0.8344)  mloss: 0.3314 (0.8344)  time: 0.6307  data: 0.0002  max mem: 29573
[23:11:12.809205] Epoch: [0]  [ 7820/13501]  eta: 0:59:53  lr: 0.000036  closs: 0.3587 (0.8338)  mloss: 0.3587 (0.8338)  time: 0.6315  data: 0.0002  max mem: 29573
[23:11:19.096425] Epoch: [0]  [ 7830/13501]  eta: 0:59:46  lr: 0.000036  closs: 0.3509 (0.8332)  mloss: 0.3509 (0.8332)  time: 0.6318  data: 0.0002  max mem: 29573
[23:11:25.435297] Epoch: [0]  [ 7840/13501]  eta: 0:59:40  lr: 0.000036  closs: 0.3509 (0.8326)  mloss: 0.3509 (0.8326)  time: 0.6312  data: 0.0002  max mem: 29573
[23:11:31.716515] Epoch: [0]  [ 7850/13501]  eta: 0:59:34  lr: 0.000036  closs: 0.3494 (0.8320)  mloss: 0.3494 (0.8320)  time: 0.6309  data: 0.0002  max mem: 29573
[23:11:38.052378] Epoch: [0]  [ 7860/13501]  eta: 0:59:27  lr: 0.000036  closs: 0.2826 (0.8313)  mloss: 0.2826 (0.8313)  time: 0.6308  data: 0.0002  max mem: 29573
[23:11:44.369019] Epoch: [0]  [ 7870/13501]  eta: 0:59:21  lr: 0.000036  closs: 0.3000 (0.8307)  mloss: 0.3000 (0.8307)  time: 0.6325  data: 0.0003  max mem: 29573
[23:11:50.721314] Epoch: [0]  [ 7880/13501]  eta: 0:59:15  lr: 0.000036  closs: 0.3209 (0.8301)  mloss: 0.3209 (0.8301)  time: 0.6334  data: 0.0004  max mem: 29573
[23:11:57.008854] Epoch: [0]  [ 7890/13501]  eta: 0:59:08  lr: 0.000037  closs: 0.3260 (0.8295)  mloss: 0.3260 (0.8295)  time: 0.6319  data: 0.0003  max mem: 29573
[23:12:03.353900] Epoch: [0]  [ 7900/13501]  eta: 0:59:02  lr: 0.000037  closs: 0.3488 (0.8289)  mloss: 0.3488 (0.8289)  time: 0.6315  data: 0.0002  max mem: 29573
[23:12:09.645457] Epoch: [0]  [ 7910/13501]  eta: 0:58:56  lr: 0.000037  closs: 0.3214 (0.8283)  mloss: 0.3214 (0.8283)  time: 0.6318  data: 0.0002  max mem: 29573
[23:12:16.000344] Epoch: [0]  [ 7920/13501]  eta: 0:58:49  lr: 0.000037  closs: 0.3287 (0.8277)  mloss: 0.3287 (0.8277)  time: 0.6322  data: 0.0002  max mem: 29573
[23:12:22.285720] Epoch: [0]  [ 7930/13501]  eta: 0:58:43  lr: 0.000037  closs: 0.3349 (0.8271)  mloss: 0.3349 (0.8271)  time: 0.6319  data: 0.0002  max mem: 29573
[23:12:28.634946] Epoch: [0]  [ 7940/13501]  eta: 0:58:37  lr: 0.000037  closs: 0.3436 (0.8264)  mloss: 0.3436 (0.8264)  time: 0.6316  data: 0.0002  max mem: 29573
[23:12:34.924270] Epoch: [0]  [ 7950/13501]  eta: 0:58:30  lr: 0.000037  closs: 0.3207 (0.8258)  mloss: 0.3207 (0.8258)  time: 0.6318  data: 0.0002  max mem: 29573
[23:12:41.272566] Epoch: [0]  [ 7960/13501]  eta: 0:58:24  lr: 0.000037  closs: 0.3210 (0.8252)  mloss: 0.3210 (0.8252)  time: 0.6318  data: 0.0003  max mem: 29573
[23:12:47.574618] Epoch: [0]  [ 7970/13501]  eta: 0:58:18  lr: 0.000037  closs: 0.3524 (0.8246)  mloss: 0.3524 (0.8246)  time: 0.6324  data: 0.0002  max mem: 29573
[23:12:53.938281] Epoch: [0]  [ 7980/13501]  eta: 0:58:11  lr: 0.000037  closs: 0.3623 (0.8240)  mloss: 0.3623 (0.8240)  time: 0.6332  data: 0.0002  max mem: 29573
[23:13:00.233385] Epoch: [0]  [ 7990/13501]  eta: 0:58:05  lr: 0.000037  closs: 0.3563 (0.8234)  mloss: 0.3563 (0.8234)  time: 0.6329  data: 0.0002  max mem: 29573
[23:13:06.574923] Epoch: [0]  [ 8000/13501]  eta: 0:57:59  lr: 0.000037  closs: 0.3144 (0.8228)  mloss: 0.3144 (0.8228)  time: 0.6318  data: 0.0002  max mem: 29573
[23:13:12.868171] Epoch: [0]  [ 8010/13501]  eta: 0:57:52  lr: 0.000037  closs: 0.3446 (0.8223)  mloss: 0.3446 (0.8223)  time: 0.6317  data: 0.0002  max mem: 29573
[23:13:19.207252] Epoch: [0]  [ 8020/13501]  eta: 0:57:46  lr: 0.000037  closs: 0.3453 (0.8217)  mloss: 0.3453 (0.8217)  time: 0.6315  data: 0.0002  max mem: 29573
[23:13:25.490830] Epoch: [0]  [ 8030/13501]  eta: 0:57:40  lr: 0.000037  closs: 0.3222 (0.8211)  mloss: 0.3222 (0.8211)  time: 0.6311  data: 0.0002  max mem: 29573
[23:13:31.831137] Epoch: [0]  [ 8040/13501]  eta: 0:57:34  lr: 0.000037  closs: 0.3360 (0.8205)  mloss: 0.3360 (0.8205)  time: 0.6311  data: 0.0002  max mem: 29573
[23:13:38.119842] Epoch: [0]  [ 8050/13501]  eta: 0:57:27  lr: 0.000037  closs: 0.3445 (0.8199)  mloss: 0.3445 (0.8199)  time: 0.6314  data: 0.0002  max mem: 29573
[23:13:44.464891] Epoch: [0]  [ 8060/13501]  eta: 0:57:21  lr: 0.000037  closs: 0.3667 (0.8194)  mloss: 0.3667 (0.8194)  time: 0.6316  data: 0.0002  max mem: 29573
[23:13:50.762584] Epoch: [0]  [ 8070/13501]  eta: 0:57:15  lr: 0.000037  closs: 0.3546 (0.8188)  mloss: 0.3546 (0.8188)  time: 0.6321  data: 0.0002  max mem: 29573
[23:13:57.117924] Epoch: [0]  [ 8080/13501]  eta: 0:57:08  lr: 0.000037  closs: 0.3394 (0.8182)  mloss: 0.3394 (0.8182)  time: 0.6326  data: 0.0002  max mem: 29573
[23:14:03.420895] Epoch: [0]  [ 8090/13501]  eta: 0:57:02  lr: 0.000037  closs: 0.3540 (0.8176)  mloss: 0.3540 (0.8176)  time: 0.6328  data: 0.0002  max mem: 29573
[23:14:09.768523] Epoch: [0]  [ 8100/13501]  eta: 0:56:56  lr: 0.000037  closs: 0.3352 (0.8171)  mloss: 0.3352 (0.8171)  time: 0.6325  data: 0.0002  max mem: 29573
[23:14:16.066300] Epoch: [0]  [ 8110/13501]  eta: 0:56:49  lr: 0.000038  closs: 0.3565 (0.8165)  mloss: 0.3565 (0.8165)  time: 0.6322  data: 0.0002  max mem: 29573
[23:14:22.406658] Epoch: [0]  [ 8120/13501]  eta: 0:56:43  lr: 0.000038  closs: 0.3565 (0.8159)  mloss: 0.3565 (0.8159)  time: 0.6318  data: 0.0002  max mem: 29573
[23:14:28.695137] Epoch: [0]  [ 8130/13501]  eta: 0:56:37  lr: 0.000038  closs: 0.3011 (0.8153)  mloss: 0.3011 (0.8153)  time: 0.6314  data: 0.0002  max mem: 29573
[23:14:35.041599] Epoch: [0]  [ 8140/13501]  eta: 0:56:30  lr: 0.000038  closs: 0.2986 (0.8147)  mloss: 0.2986 (0.8147)  time: 0.6317  data: 0.0002  max mem: 29573
[23:14:41.332981] Epoch: [0]  [ 8150/13501]  eta: 0:56:24  lr: 0.000038  closs: 0.3255 (0.8142)  mloss: 0.3255 (0.8142)  time: 0.6318  data: 0.0002  max mem: 29573
[23:14:47.668851] Epoch: [0]  [ 8160/13501]  eta: 0:56:18  lr: 0.000038  closs: 0.3366 (0.8136)  mloss: 0.3366 (0.8136)  time: 0.6313  data: 0.0002  max mem: 29573
[23:14:53.953926] Epoch: [0]  [ 8170/13501]  eta: 0:56:11  lr: 0.000038  closs: 0.3366 (0.8130)  mloss: 0.3366 (0.8130)  time: 0.6310  data: 0.0002  max mem: 29573
[23:15:00.295646] Epoch: [0]  [ 8180/13501]  eta: 0:56:05  lr: 0.000038  closs: 0.3740 (0.8125)  mloss: 0.3740 (0.8125)  time: 0.6313  data: 0.0002  max mem: 29573
[23:15:06.658729] Epoch: [0]  [ 8190/13501]  eta: 0:55:59  lr: 0.000038  closs: 0.3468 (0.8119)  mloss: 0.3468 (0.8119)  time: 0.6352  data: 0.0002  max mem: 29573
[23:15:13.005735] Epoch: [0]  [ 8200/13501]  eta: 0:55:52  lr: 0.000038  closs: 0.3293 (0.8114)  mloss: 0.3293 (0.8114)  time: 0.6354  data: 0.0002  max mem: 29573
[23:15:19.292942] Epoch: [0]  [ 8210/13501]  eta: 0:55:46  lr: 0.000038  closs: 0.3535 (0.8108)  mloss: 0.3535 (0.8108)  time: 0.6316  data: 0.0002  max mem: 29573
[23:15:25.640691] Epoch: [0]  [ 8220/13501]  eta: 0:55:40  lr: 0.000038  closs: 0.3393 (0.8103)  mloss: 0.3393 (0.8103)  time: 0.6317  data: 0.0002  max mem: 29573
[23:15:31.929844] Epoch: [0]  [ 8230/13501]  eta: 0:55:33  lr: 0.000038  closs: 0.3393 (0.8097)  mloss: 0.3393 (0.8097)  time: 0.6318  data: 0.0002  max mem: 29573
[23:15:38.266310] Epoch: [0]  [ 8240/13501]  eta: 0:55:27  lr: 0.000038  closs: 0.3330 (0.8092)  mloss: 0.3330 (0.8092)  time: 0.6312  data: 0.0002  max mem: 29573
[23:15:44.565295] Epoch: [0]  [ 8250/13501]  eta: 0:55:21  lr: 0.000038  closs: 0.2946 (0.8086)  mloss: 0.2946 (0.8086)  time: 0.6317  data: 0.0002  max mem: 29573
[23:15:50.906050] Epoch: [0]  [ 8260/13501]  eta: 0:55:14  lr: 0.000038  closs: 0.3047 (0.8080)  mloss: 0.3047 (0.8080)  time: 0.6319  data: 0.0002  max mem: 29573
[23:15:57.192140] Epoch: [0]  [ 8270/13501]  eta: 0:55:08  lr: 0.000038  closs: 0.3224 (0.8075)  mloss: 0.3224 (0.8075)  time: 0.6313  data: 0.0002  max mem: 29573
[23:16:03.538705] Epoch: [0]  [ 8280/13501]  eta: 0:55:02  lr: 0.000038  closs: 0.3439 (0.8069)  mloss: 0.3439 (0.8069)  time: 0.6316  data: 0.0002  max mem: 29573
[23:16:09.835089] Epoch: [0]  [ 8290/13501]  eta: 0:54:55  lr: 0.000038  closs: 0.3468 (0.8064)  mloss: 0.3468 (0.8064)  time: 0.6321  data: 0.0002  max mem: 29573
[23:16:16.197308] Epoch: [0]  [ 8300/13501]  eta: 0:54:49  lr: 0.000038  closs: 0.3425 (0.8058)  mloss: 0.3425 (0.8058)  time: 0.6328  data: 0.0002  max mem: 29573
[23:16:22.489710] Epoch: [0]  [ 8310/13501]  eta: 0:54:43  lr: 0.000038  closs: 0.3242 (0.8052)  mloss: 0.3242 (0.8052)  time: 0.6326  data: 0.0002  max mem: 29573
[23:16:28.836858] Epoch: [0]  [ 8320/13501]  eta: 0:54:36  lr: 0.000039  closs: 0.3242 (0.8046)  mloss: 0.3242 (0.8046)  time: 0.6319  data: 0.0002  max mem: 29573
[23:16:35.125386] Epoch: [0]  [ 8330/13501]  eta: 0:54:30  lr: 0.000039  closs: 0.3612 (0.8041)  mloss: 0.3612 (0.8041)  time: 0.6317  data: 0.0002  max mem: 29573
[23:16:41.467418] Epoch: [0]  [ 8340/13501]  eta: 0:54:24  lr: 0.000039  closs: 0.3683 (0.8036)  mloss: 0.3683 (0.8036)  time: 0.6315  data: 0.0002  max mem: 29573
[23:16:47.757395] Epoch: [0]  [ 8350/13501]  eta: 0:54:17  lr: 0.000039  closs: 0.3961 (0.8031)  mloss: 0.3961 (0.8031)  time: 0.6315  data: 0.0002  max mem: 29573
[23:16:54.097578] Epoch: [0]  [ 8360/13501]  eta: 0:54:11  lr: 0.000039  closs: 0.3317 (0.8025)  mloss: 0.3317 (0.8025)  time: 0.6314  data: 0.0002  max mem: 29573
[23:17:00.373242] Epoch: [0]  [ 8370/13501]  eta: 0:54:05  lr: 0.000039  closs: 0.3142 (0.8020)  mloss: 0.3142 (0.8020)  time: 0.6307  data: 0.0002  max mem: 29573
[23:17:06.702565] Epoch: [0]  [ 8380/13501]  eta: 0:53:58  lr: 0.000039  closs: 0.3353 (0.8014)  mloss: 0.3353 (0.8014)  time: 0.6302  data: 0.0002  max mem: 29573
[23:17:12.970688] Epoch: [0]  [ 8390/13501]  eta: 0:53:52  lr: 0.000039  closs: 0.3116 (0.8008)  mloss: 0.3116 (0.8008)  time: 0.6298  data: 0.0002  max mem: 29573
[23:17:19.308016] Epoch: [0]  [ 8400/13501]  eta: 0:53:46  lr: 0.000039  closs: 0.3186 (0.8003)  mloss: 0.3186 (0.8003)  time: 0.6302  data: 0.0002  max mem: 29573
[23:17:25.588329] Epoch: [0]  [ 8410/13501]  eta: 0:53:39  lr: 0.000039  closs: 0.3789 (0.7998)  mloss: 0.3789 (0.7998)  time: 0.6308  data: 0.0002  max mem: 29573
[23:17:31.919803] Epoch: [0]  [ 8420/13501]  eta: 0:53:33  lr: 0.000039  closs: 0.3210 (0.7992)  mloss: 0.3210 (0.7992)  time: 0.6305  data: 0.0002  max mem: 29573
[23:17:38.213097] Epoch: [0]  [ 8430/13501]  eta: 0:53:27  lr: 0.000039  closs: 0.2867 (0.7986)  mloss: 0.2867 (0.7986)  time: 0.6312  data: 0.0002  max mem: 29573
[23:17:44.553726] Epoch: [0]  [ 8440/13501]  eta: 0:53:20  lr: 0.000039  closs: 0.3270 (0.7981)  mloss: 0.3270 (0.7981)  time: 0.6316  data: 0.0002  max mem: 29573
[23:17:50.833889] Epoch: [0]  [ 8450/13501]  eta: 0:53:14  lr: 0.000039  closs: 0.3475 (0.7976)  mloss: 0.3475 (0.7976)  time: 0.6310  data: 0.0002  max mem: 29573
[23:17:57.175817] Epoch: [0]  [ 8460/13501]  eta: 0:53:08  lr: 0.000039  closs: 0.3341 (0.7970)  mloss: 0.3341 (0.7970)  time: 0.6310  data: 0.0002  max mem: 29573
[23:18:03.471032] Epoch: [0]  [ 8470/13501]  eta: 0:53:01  lr: 0.000039  closs: 0.3341 (0.7965)  mloss: 0.3341 (0.7965)  time: 0.6318  data: 0.0002  max mem: 29573
[23:18:09.817833] Epoch: [0]  [ 8480/13501]  eta: 0:52:55  lr: 0.000039  closs: 0.3387 (0.7959)  mloss: 0.3387 (0.7959)  time: 0.6320  data: 0.0002  max mem: 29573
[23:18:16.101391] Epoch: [0]  [ 8490/13501]  eta: 0:52:49  lr: 0.000039  closs: 0.3125 (0.7954)  mloss: 0.3125 (0.7954)  time: 0.6314  data: 0.0002  max mem: 29573
[23:18:22.451706] Epoch: [0]  [ 8500/13501]  eta: 0:52:42  lr: 0.000039  closs: 0.3134 (0.7948)  mloss: 0.3134 (0.7948)  time: 0.6316  data: 0.0002  max mem: 29573
[23:18:28.762099] Epoch: [0]  [ 8510/13501]  eta: 0:52:36  lr: 0.000039  closs: 0.3503 (0.7943)  mloss: 0.3503 (0.7943)  time: 0.6330  data: 0.0002  max mem: 29573
[23:18:35.108418] Epoch: [0]  [ 8520/13501]  eta: 0:52:30  lr: 0.000039  closs: 0.3468 (0.7938)  mloss: 0.3468 (0.7938)  time: 0.6328  data: 0.0003  max mem: 29573
[23:18:41.407914] Epoch: [0]  [ 8530/13501]  eta: 0:52:23  lr: 0.000039  closs: 0.3468 (0.7933)  mloss: 0.3468 (0.7933)  time: 0.6322  data: 0.0003  max mem: 29573
[23:18:47.751171] Epoch: [0]  [ 8540/13501]  eta: 0:52:17  lr: 0.000040  closs: 0.2968 (0.7927)  mloss: 0.2968 (0.7927)  time: 0.6321  data: 0.0002  max mem: 29573
[23:18:54.047686] Epoch: [0]  [ 8550/13501]  eta: 0:52:11  lr: 0.000040  closs: 0.3004 (0.7922)  mloss: 0.3004 (0.7922)  time: 0.6319  data: 0.0002  max mem: 29573
[23:19:00.386673] Epoch: [0]  [ 8560/13501]  eta: 0:52:04  lr: 0.000040  closs: 0.3017 (0.7916)  mloss: 0.3017 (0.7916)  time: 0.6317  data: 0.0002  max mem: 29573
[23:19:06.684412] Epoch: [0]  [ 8570/13501]  eta: 0:51:58  lr: 0.000040  closs: 0.3331 (0.7911)  mloss: 0.3331 (0.7911)  time: 0.6318  data: 0.0002  max mem: 29573
[23:19:13.027890] Epoch: [0]  [ 8580/13501]  eta: 0:51:52  lr: 0.000040  closs: 0.3436 (0.7906)  mloss: 0.3436 (0.7906)  time: 0.6320  data: 0.0002  max mem: 29573
[23:19:19.314618] Epoch: [0]  [ 8590/13501]  eta: 0:51:45  lr: 0.000040  closs: 0.3279 (0.7901)  mloss: 0.3279 (0.7901)  time: 0.6314  data: 0.0002  max mem: 29573
[23:19:25.658687] Epoch: [0]  [ 8600/13501]  eta: 0:51:39  lr: 0.000040  closs: 0.3258 (0.7895)  mloss: 0.3258 (0.7895)  time: 0.6315  data: 0.0002  max mem: 29573
[23:19:31.956932] Epoch: [0]  [ 8610/13501]  eta: 0:51:33  lr: 0.000040  closs: 0.3252 (0.7891)  mloss: 0.3252 (0.7891)  time: 0.6320  data: 0.0002  max mem: 29573
[23:19:38.314755] Epoch: [0]  [ 8620/13501]  eta: 0:51:26  lr: 0.000040  closs: 0.3534 (0.7886)  mloss: 0.3534 (0.7886)  time: 0.6327  data: 0.0002  max mem: 29573
[23:19:44.599811] Epoch: [0]  [ 8630/13501]  eta: 0:51:20  lr: 0.000040  closs: 0.3274 (0.7880)  mloss: 0.3274 (0.7880)  time: 0.6321  data: 0.0002  max mem: 29573
[23:19:50.942076] Epoch: [0]  [ 8640/13501]  eta: 0:51:14  lr: 0.000040  closs: 0.3111 (0.7875)  mloss: 0.3111 (0.7875)  time: 0.6313  data: 0.0002  max mem: 29573
[23:19:57.238707] Epoch: [0]  [ 8650/13501]  eta: 0:51:07  lr: 0.000040  closs: 0.3368 (0.7870)  mloss: 0.3368 (0.7870)  time: 0.6319  data: 0.0002  max mem: 29573
[23:20:03.584024] Epoch: [0]  [ 8660/13501]  eta: 0:51:01  lr: 0.000040  closs: 0.3546 (0.7866)  mloss: 0.3546 (0.7866)  time: 0.6320  data: 0.0002  max mem: 29573
[23:20:09.869794] Epoch: [0]  [ 8670/13501]  eta: 0:50:55  lr: 0.000040  closs: 0.3559 (0.7861)  mloss: 0.3559 (0.7861)  time: 0.6315  data: 0.0002  max mem: 29573
[23:20:16.212626] Epoch: [0]  [ 8680/13501]  eta: 0:50:48  lr: 0.000040  closs: 0.3453 (0.7857)  mloss: 0.3453 (0.7857)  time: 0.6314  data: 0.0002  max mem: 29573
[23:20:22.500781] Epoch: [0]  [ 8690/13501]  eta: 0:50:42  lr: 0.000040  closs: 0.3565 (0.7852)  mloss: 0.3565 (0.7852)  time: 0.6315  data: 0.0002  max mem: 29573
[23:20:28.839233] Epoch: [0]  [ 8700/13501]  eta: 0:50:36  lr: 0.000040  closs: 0.3540 (0.7846)  mloss: 0.3540 (0.7846)  time: 0.6312  data: 0.0002  max mem: 29573
[23:20:35.137575] Epoch: [0]  [ 8710/13501]  eta: 0:50:29  lr: 0.000040  closs: 0.3427 (0.7841)  mloss: 0.3427 (0.7841)  time: 0.6318  data: 0.0002  max mem: 29573
[23:20:41.493260] Epoch: [0]  [ 8720/13501]  eta: 0:50:23  lr: 0.000040  closs: 0.3753 (0.7837)  mloss: 0.3753 (0.7837)  time: 0.6326  data: 0.0002  max mem: 29573
[23:20:47.787236] Epoch: [0]  [ 8730/13501]  eta: 0:50:17  lr: 0.000040  closs: 0.3562 (0.7832)  mloss: 0.3562 (0.7832)  time: 0.6324  data: 0.0002  max mem: 29573
[23:20:54.129273] Epoch: [0]  [ 8740/13501]  eta: 0:50:10  lr: 0.000040  closs: 0.3021 (0.7827)  mloss: 0.3021 (0.7827)  time: 0.6317  data: 0.0002  max mem: 29573
[23:21:00.412966] Epoch: [0]  [ 8750/13501]  eta: 0:50:04  lr: 0.000040  closs: 0.3342 (0.7822)  mloss: 0.3342 (0.7822)  time: 0.6312  data: 0.0002  max mem: 29573
[23:21:06.752701] Epoch: [0]  [ 8760/13501]  eta: 0:49:58  lr: 0.000041  closs: 0.3595 (0.7817)  mloss: 0.3595 (0.7817)  time: 0.6311  data: 0.0002  max mem: 29573
[23:21:13.039414] Epoch: [0]  [ 8770/13501]  eta: 0:49:51  lr: 0.000041  closs: 0.3762 (0.7812)  mloss: 0.3762 (0.7812)  time: 0.6312  data: 0.0002  max mem: 29573
[23:21:19.386394] Epoch: [0]  [ 8780/13501]  eta: 0:49:45  lr: 0.000041  closs: 0.3294 (0.7807)  mloss: 0.3294 (0.7807)  time: 0.6316  data: 0.0002  max mem: 29573
[23:21:25.672254] Epoch: [0]  [ 8790/13501]  eta: 0:49:39  lr: 0.000041  closs: 0.3245 (0.7802)  mloss: 0.3245 (0.7802)  time: 0.6316  data: 0.0002  max mem: 29573
[23:21:32.021905] Epoch: [0]  [ 8800/13501]  eta: 0:49:33  lr: 0.000041  closs: 0.3197 (0.7797)  mloss: 0.3197 (0.7797)  time: 0.6317  data: 0.0002  max mem: 29573
[23:21:38.304191] Epoch: [0]  [ 8810/13501]  eta: 0:49:26  lr: 0.000041  closs: 0.3564 (0.7792)  mloss: 0.3564 (0.7792)  time: 0.6315  data: 0.0002  max mem: 29573
[23:21:44.660534] Epoch: [0]  [ 8820/13501]  eta: 0:49:20  lr: 0.000041  closs: 0.3771 (0.7787)  mloss: 0.3771 (0.7787)  time: 0.6319  data: 0.0002  max mem: 29573
[23:21:50.965640] Epoch: [0]  [ 8830/13501]  eta: 0:49:14  lr: 0.000041  closs: 0.3430 (0.7782)  mloss: 0.3430 (0.7782)  time: 0.6330  data: 0.0002  max mem: 29573
[23:21:57.306811] Epoch: [0]  [ 8840/13501]  eta: 0:49:07  lr: 0.000041  closs: 0.3117 (0.7777)  mloss: 0.3117 (0.7777)  time: 0.6322  data: 0.0002  max mem: 29573
[23:22:03.605709] Epoch: [0]  [ 8850/13501]  eta: 0:49:01  lr: 0.000041  closs: 0.3263 (0.7772)  mloss: 0.3263 (0.7772)  time: 0.6319  data: 0.0002  max mem: 29573
[23:22:09.943384] Epoch: [0]  [ 8860/13501]  eta: 0:48:55  lr: 0.000041  closs: 0.3566 (0.7768)  mloss: 0.3566 (0.7768)  time: 0.6317  data: 0.0002  max mem: 29573
[23:22:16.239255] Epoch: [0]  [ 8870/13501]  eta: 0:48:48  lr: 0.000041  closs: 0.3429 (0.7763)  mloss: 0.3429 (0.7763)  time: 0.6316  data: 0.0002  max mem: 29573
[23:22:22.585648] Epoch: [0]  [ 8880/13501]  eta: 0:48:42  lr: 0.000041  closs: 0.3429 (0.7758)  mloss: 0.3429 (0.7758)  time: 0.6320  data: 0.0002  max mem: 29573
[23:22:28.871742] Epoch: [0]  [ 8890/13501]  eta: 0:48:36  lr: 0.000041  closs: 0.3527 (0.7753)  mloss: 0.3527 (0.7753)  time: 0.6315  data: 0.0002  max mem: 29573
[23:22:35.211572] Epoch: [0]  [ 8900/13501]  eta: 0:48:29  lr: 0.000041  closs: 0.3213 (0.7748)  mloss: 0.3213 (0.7748)  time: 0.6312  data: 0.0002  max mem: 29573
[23:22:41.497556] Epoch: [0]  [ 8910/13501]  eta: 0:48:23  lr: 0.000041  closs: 0.3430 (0.7744)  mloss: 0.3430 (0.7744)  time: 0.6312  data: 0.0002  max mem: 29573
[23:22:47.844116] Epoch: [0]  [ 8920/13501]  eta: 0:48:17  lr: 0.000041  closs: 0.3583 (0.7739)  mloss: 0.3583 (0.7739)  time: 0.6315  data: 0.0003  max mem: 29573
[23:22:54.140031] Epoch: [0]  [ 8930/13501]  eta: 0:48:10  lr: 0.000041  closs: 0.3527 (0.7734)  mloss: 0.3527 (0.7734)  time: 0.6320  data: 0.0003  max mem: 29573
[23:23:00.492118] Epoch: [0]  [ 8940/13501]  eta: 0:48:04  lr: 0.000041  closs: 0.3901 (0.7731)  mloss: 0.3901 (0.7731)  time: 0.6323  data: 0.0002  max mem: 29573
[23:23:06.782312] Epoch: [0]  [ 8950/13501]  eta: 0:47:58  lr: 0.000041  closs: 0.3673 (0.7726)  mloss: 0.3673 (0.7726)  time: 0.6320  data: 0.0002  max mem: 29573
[23:23:13.125705] Epoch: [0]  [ 8960/13501]  eta: 0:47:51  lr: 0.000041  closs: 0.3084 (0.7721)  mloss: 0.3084 (0.7721)  time: 0.6316  data: 0.0002  max mem: 29573
[23:23:19.413880] Epoch: [0]  [ 8970/13501]  eta: 0:47:45  lr: 0.000042  closs: 0.3367 (0.7716)  mloss: 0.3367 (0.7716)  time: 0.6315  data: 0.0002  max mem: 29573
[23:23:25.761145] Epoch: [0]  [ 8980/13501]  eta: 0:47:39  lr: 0.000042  closs: 0.3165 (0.7711)  mloss: 0.3165 (0.7711)  time: 0.6317  data: 0.0002  max mem: 29573
[23:23:32.046143] Epoch: [0]  [ 8990/13501]  eta: 0:47:32  lr: 0.000042  closs: 0.3401 (0.7707)  mloss: 0.3401 (0.7707)  time: 0.6315  data: 0.0002  max mem: 29573
[23:23:38.397106] Epoch: [0]  [ 9000/13501]  eta: 0:47:26  lr: 0.000042  closs: 0.3658 (0.7702)  mloss: 0.3658 (0.7702)  time: 0.6317  data: 0.0002  max mem: 29573
[23:23:44.689207] Epoch: [0]  [ 9010/13501]  eta: 0:47:20  lr: 0.000042  closs: 0.3349 (0.7698)  mloss: 0.3349 (0.7698)  time: 0.6321  data: 0.0002  max mem: 29573
[23:23:51.029537] Epoch: [0]  [ 9020/13501]  eta: 0:47:13  lr: 0.000042  closs: 0.3268 (0.7693)  mloss: 0.3268 (0.7693)  time: 0.6315  data: 0.0002  max mem: 29573
[23:23:57.321400] Epoch: [0]  [ 9030/13501]  eta: 0:47:07  lr: 0.000042  closs: 0.3222 (0.7688)  mloss: 0.3222 (0.7688)  time: 0.6315  data: 0.0002  max mem: 29573
[23:24:03.671762] Epoch: [0]  [ 9040/13501]  eta: 0:47:01  lr: 0.000042  closs: 0.3338 (0.7684)  mloss: 0.3338 (0.7684)  time: 0.6320  data: 0.0002  max mem: 29573
[23:24:09.962611] Epoch: [0]  [ 9050/13501]  eta: 0:46:54  lr: 0.000042  closs: 0.3937 (0.7680)  mloss: 0.3937 (0.7680)  time: 0.6320  data: 0.0002  max mem: 29573
[23:24:16.303276] Epoch: [0]  [ 9060/13501]  eta: 0:46:48  lr: 0.000042  closs: 0.3343 (0.7675)  mloss: 0.3343 (0.7675)  time: 0.6315  data: 0.0002  max mem: 29573
[23:24:22.603762] Epoch: [0]  [ 9070/13501]  eta: 0:46:42  lr: 0.000042  closs: 0.3124 (0.7670)  mloss: 0.3124 (0.7670)  time: 0.6320  data: 0.0002  max mem: 29573
[23:24:28.949622] Epoch: [0]  [ 9080/13501]  eta: 0:46:35  lr: 0.000042  closs: 0.3217 (0.7665)  mloss: 0.3217 (0.7665)  time: 0.6322  data: 0.0002  max mem: 29573
[23:24:35.237694] Epoch: [0]  [ 9090/13501]  eta: 0:46:29  lr: 0.000042  closs: 0.3359 (0.7660)  mloss: 0.3359 (0.7660)  time: 0.6316  data: 0.0002  max mem: 29573
[23:24:41.572345] Epoch: [0]  [ 9100/13501]  eta: 0:46:23  lr: 0.000042  closs: 0.3524 (0.7656)  mloss: 0.3524 (0.7656)  time: 0.6311  data: 0.0002  max mem: 29573
[23:24:47.856096] Epoch: [0]  [ 9110/13501]  eta: 0:46:16  lr: 0.000042  closs: 0.3512 (0.7651)  mloss: 0.3512 (0.7651)  time: 0.6308  data: 0.0002  max mem: 29573
[23:24:54.199454] Epoch: [0]  [ 9120/13501]  eta: 0:46:10  lr: 0.000042  closs: 0.3763 (0.7647)  mloss: 0.3763 (0.7647)  time: 0.6313  data: 0.0002  max mem: 29573
[23:25:00.493343] Epoch: [0]  [ 9130/13501]  eta: 0:46:04  lr: 0.000042  closs: 0.3755 (0.7643)  mloss: 0.3755 (0.7643)  time: 0.6318  data: 0.0002  max mem: 29573
[23:25:06.850031] Epoch: [0]  [ 9140/13501]  eta: 0:45:57  lr: 0.000042  closs: 0.3491 (0.7638)  mloss: 0.3491 (0.7638)  time: 0.6325  data: 0.0003  max mem: 29573
[23:25:13.150918] Epoch: [0]  [ 9150/13501]  eta: 0:45:51  lr: 0.000042  closs: 0.3423 (0.7634)  mloss: 0.3423 (0.7634)  time: 0.6328  data: 0.0003  max mem: 29573
[23:25:19.491555] Epoch: [0]  [ 9160/13501]  eta: 0:45:45  lr: 0.000042  closs: 0.3162 (0.7629)  mloss: 0.3162 (0.7629)  time: 0.6320  data: 0.0002  max mem: 29573
[23:25:25.776186] Epoch: [0]  [ 9170/13501]  eta: 0:45:38  lr: 0.000042  closs: 0.3202 (0.7624)  mloss: 0.3202 (0.7624)  time: 0.6312  data: 0.0002  max mem: 29573
[23:25:32.113919] Epoch: [0]  [ 9180/13501]  eta: 0:45:32  lr: 0.000042  closs: 0.3422 (0.7620)  mloss: 0.3422 (0.7620)  time: 0.6310  data: 0.0002  max mem: 29573
[23:25:38.405156] Epoch: [0]  [ 9190/13501]  eta: 0:45:26  lr: 0.000043  closs: 0.3422 (0.7615)  mloss: 0.3422 (0.7615)  time: 0.6314  data: 0.0002  max mem: 29573
[23:25:44.747917] Epoch: [0]  [ 9200/13501]  eta: 0:45:19  lr: 0.000043  closs: 0.3316 (0.7611)  mloss: 0.3316 (0.7611)  time: 0.6316  data: 0.0002  max mem: 29573
[23:25:51.035828] Epoch: [0]  [ 9210/13501]  eta: 0:45:13  lr: 0.000043  closs: 0.3510 (0.7606)  mloss: 0.3510 (0.7606)  time: 0.6315  data: 0.0002  max mem: 29573
[23:25:57.373697] Epoch: [0]  [ 9220/13501]  eta: 0:45:07  lr: 0.000043  closs: 0.3454 (0.7602)  mloss: 0.3454 (0.7602)  time: 0.6312  data: 0.0002  max mem: 29573
[23:26:03.659276] Epoch: [0]  [ 9230/13501]  eta: 0:45:00  lr: 0.000043  closs: 0.3183 (0.7597)  mloss: 0.3183 (0.7597)  time: 0.6311  data: 0.0002  max mem: 29573
[23:26:10.004728] Epoch: [0]  [ 9240/13501]  eta: 0:44:54  lr: 0.000043  closs: 0.3136 (0.7592)  mloss: 0.3136 (0.7592)  time: 0.6315  data: 0.0002  max mem: 29573
[23:26:16.311055] Epoch: [0]  [ 9250/13501]  eta: 0:44:48  lr: 0.000043  closs: 0.3055 (0.7588)  mloss: 0.3055 (0.7588)  time: 0.6325  data: 0.0002  max mem: 29573
[23:26:22.657727] Epoch: [0]  [ 9260/13501]  eta: 0:44:41  lr: 0.000043  closs: 0.3165 (0.7584)  mloss: 0.3165 (0.7584)  time: 0.6326  data: 0.0002  max mem: 29573
[23:26:28.941109] Epoch: [0]  [ 9270/13501]  eta: 0:44:35  lr: 0.000043  closs: 0.3881 (0.7580)  mloss: 0.3881 (0.7580)  time: 0.6314  data: 0.0002  max mem: 29573
[23:26:35.282730] Epoch: [0]  [ 9280/13501]  eta: 0:44:29  lr: 0.000043  closs: 0.3656 (0.7575)  mloss: 0.3656 (0.7575)  time: 0.6312  data: 0.0002  max mem: 29573
[23:26:41.582794] Epoch: [0]  [ 9290/13501]  eta: 0:44:22  lr: 0.000043  closs: 0.3250 (0.7571)  mloss: 0.3250 (0.7571)  time: 0.6320  data: 0.0002  max mem: 29573
[23:26:47.921498] Epoch: [0]  [ 9300/13501]  eta: 0:44:16  lr: 0.000043  closs: 0.3087 (0.7566)  mloss: 0.3087 (0.7566)  time: 0.6319  data: 0.0002  max mem: 29573
[23:26:54.217790] Epoch: [0]  [ 9310/13501]  eta: 0:44:10  lr: 0.000043  closs: 0.3169 (0.7562)  mloss: 0.3169 (0.7562)  time: 0.6317  data: 0.0002  max mem: 29573
[23:27:00.561811] Epoch: [0]  [ 9320/13501]  eta: 0:44:03  lr: 0.000043  closs: 0.3515 (0.7558)  mloss: 0.3515 (0.7558)  time: 0.6319  data: 0.0002  max mem: 29573
[23:27:06.851466] Epoch: [0]  [ 9330/13501]  eta: 0:43:57  lr: 0.000043  closs: 0.3483 (0.7553)  mloss: 0.3483 (0.7553)  time: 0.6316  data: 0.0002  max mem: 29573
[23:27:13.191230] Epoch: [0]  [ 9340/13501]  eta: 0:43:51  lr: 0.000043  closs: 0.3483 (0.7549)  mloss: 0.3483 (0.7549)  time: 0.6314  data: 0.0002  max mem: 29573
[23:27:19.488095] Epoch: [0]  [ 9350/13501]  eta: 0:43:45  lr: 0.000043  closs: 0.3586 (0.7545)  mloss: 0.3586 (0.7545)  time: 0.6318  data: 0.0002  max mem: 29573
[23:27:25.861171] Epoch: [0]  [ 9360/13501]  eta: 0:43:38  lr: 0.000043  closs: 0.3380 (0.7540)  mloss: 0.3380 (0.7540)  time: 0.6334  data: 0.0002  max mem: 29573
[23:27:32.151399] Epoch: [0]  [ 9370/13501]  eta: 0:43:32  lr: 0.000043  closs: 0.3329 (0.7536)  mloss: 0.3329 (0.7536)  time: 0.6331  data: 0.0003  max mem: 29573
[23:27:38.502807] Epoch: [0]  [ 9380/13501]  eta: 0:43:26  lr: 0.000043  closs: 0.3329 (0.7532)  mloss: 0.3329 (0.7532)  time: 0.6320  data: 0.0003  max mem: 29573
[23:27:44.797343] Epoch: [0]  [ 9390/13501]  eta: 0:43:19  lr: 0.000043  closs: 0.3238 (0.7527)  mloss: 0.3238 (0.7527)  time: 0.6322  data: 0.0002  max mem: 29573
[23:27:51.147276] Epoch: [0]  [ 9400/13501]  eta: 0:43:13  lr: 0.000044  closs: 0.3224 (0.7523)  mloss: 0.3224 (0.7523)  time: 0.6321  data: 0.0002  max mem: 29573
[23:27:57.436906] Epoch: [0]  [ 9410/13501]  eta: 0:43:07  lr: 0.000044  closs: 0.2919 (0.7518)  mloss: 0.2919 (0.7518)  time: 0.6319  data: 0.0002  max mem: 29573
[23:28:03.795064] Epoch: [0]  [ 9420/13501]  eta: 0:43:00  lr: 0.000044  closs: 0.3771 (0.7515)  mloss: 0.3771 (0.7515)  time: 0.6323  data: 0.0002  max mem: 29573
[23:28:10.088853] Epoch: [0]  [ 9430/13501]  eta: 0:42:54  lr: 0.000044  closs: 0.3697 (0.7510)  mloss: 0.3697 (0.7510)  time: 0.6325  data: 0.0002  max mem: 29573
[23:28:16.425581] Epoch: [0]  [ 9440/13501]  eta: 0:42:48  lr: 0.000044  closs: 0.3474 (0.7506)  mloss: 0.3474 (0.7506)  time: 0.6314  data: 0.0003  max mem: 29573
[23:28:22.704116] Epoch: [0]  [ 9450/13501]  eta: 0:42:41  lr: 0.000044  closs: 0.3467 (0.7502)  mloss: 0.3467 (0.7502)  time: 0.6307  data: 0.0002  max mem: 29573
[23:28:29.057878] Epoch: [0]  [ 9460/13501]  eta: 0:42:35  lr: 0.000044  closs: 0.3467 (0.7498)  mloss: 0.3467 (0.7498)  time: 0.6315  data: 0.0002  max mem: 29573
[23:28:35.357753] Epoch: [0]  [ 9470/13501]  eta: 0:42:29  lr: 0.000044  closs: 0.3376 (0.7493)  mloss: 0.3376 (0.7493)  time: 0.6326  data: 0.0002  max mem: 29573
[23:28:41.699469] Epoch: [0]  [ 9480/13501]  eta: 0:42:22  lr: 0.000044  closs: 0.3305 (0.7489)  mloss: 0.3305 (0.7489)  time: 0.6320  data: 0.0002  max mem: 29573
[23:28:47.991781] Epoch: [0]  [ 9490/13501]  eta: 0:42:16  lr: 0.000044  closs: 0.3566 (0.7485)  mloss: 0.3566 (0.7485)  time: 0.6316  data: 0.0002  max mem: 29573
[23:28:54.329445] Epoch: [0]  [ 9500/13501]  eta: 0:42:10  lr: 0.000044  closs: 0.3434 (0.7481)  mloss: 0.3434 (0.7481)  time: 0.6314  data: 0.0002  max mem: 29573
[23:29:00.619066] Epoch: [0]  [ 9510/13501]  eta: 0:42:03  lr: 0.000044  closs: 0.3410 (0.7477)  mloss: 0.3410 (0.7477)  time: 0.6313  data: 0.0002  max mem: 29573
[23:29:06.959918] Epoch: [0]  [ 9520/13501]  eta: 0:41:57  lr: 0.000044  closs: 0.3485 (0.7472)  mloss: 0.3485 (0.7472)  time: 0.6314  data: 0.0002  max mem: 29573
[23:29:13.324383] Epoch: [0]  [ 9530/13501]  eta: 0:41:51  lr: 0.000044  closs: 0.3485 (0.7468)  mloss: 0.3485 (0.7468)  time: 0.6352  data: 0.0002  max mem: 29573
[23:29:19.654815] Epoch: [0]  [ 9540/13501]  eta: 0:41:44  lr: 0.000044  closs: 0.3400 (0.7464)  mloss: 0.3400 (0.7464)  time: 0.6347  data: 0.0002  max mem: 29573
[23:29:25.946821] Epoch: [0]  [ 9550/13501]  eta: 0:41:38  lr: 0.000044  closs: 0.3347 (0.7460)  mloss: 0.3347 (0.7460)  time: 0.6310  data: 0.0002  max mem: 29573
[23:29:32.292037] Epoch: [0]  [ 9560/13501]  eta: 0:41:32  lr: 0.000044  closs: 0.3295 (0.7455)  mloss: 0.3295 (0.7455)  time: 0.6318  data: 0.0002  max mem: 29573
[23:29:38.602263] Epoch: [0]  [ 9570/13501]  eta: 0:41:25  lr: 0.000044  closs: 0.3342 (0.7451)  mloss: 0.3342 (0.7451)  time: 0.6327  data: 0.0002  max mem: 29573
[23:29:44.945967] Epoch: [0]  [ 9580/13501]  eta: 0:41:19  lr: 0.000044  closs: 0.3484 (0.7447)  mloss: 0.3484 (0.7447)  time: 0.6326  data: 0.0002  max mem: 29573
[23:29:51.237429] Epoch: [0]  [ 9590/13501]  eta: 0:41:13  lr: 0.000044  closs: 0.2946 (0.7443)  mloss: 0.2946 (0.7443)  time: 0.6317  data: 0.0002  max mem: 29573
[23:29:57.579030] Epoch: [0]  [ 9600/13501]  eta: 0:41:06  lr: 0.000044  closs: 0.3149 (0.7439)  mloss: 0.3149 (0.7439)  time: 0.6316  data: 0.0002  max mem: 29573
[23:30:03.860368] Epoch: [0]  [ 9610/13501]  eta: 0:41:00  lr: 0.000044  closs: 0.3720 (0.7435)  mloss: 0.3720 (0.7435)  time: 0.6311  data: 0.0002  max mem: 29573
[23:30:10.206343] Epoch: [0]  [ 9620/13501]  eta: 0:40:54  lr: 0.000045  closs: 0.3683 (0.7431)  mloss: 0.3683 (0.7431)  time: 0.6313  data: 0.0002  max mem: 29573
[23:30:16.499110] Epoch: [0]  [ 9630/13501]  eta: 0:40:47  lr: 0.000045  closs: 0.3858 (0.7427)  mloss: 0.3858 (0.7427)  time: 0.6319  data: 0.0002  max mem: 29573
[23:30:22.839917] Epoch: [0]  [ 9640/13501]  eta: 0:40:41  lr: 0.000045  closs: 0.3858 (0.7424)  mloss: 0.3858 (0.7424)  time: 0.6316  data: 0.0002  max mem: 29573
[23:30:29.126570] Epoch: [0]  [ 9650/13501]  eta: 0:40:35  lr: 0.000045  closs: 0.3465 (0.7419)  mloss: 0.3465 (0.7419)  time: 0.6313  data: 0.0002  max mem: 29573
[23:30:35.474332] Epoch: [0]  [ 9660/13501]  eta: 0:40:28  lr: 0.000045  closs: 0.3199 (0.7415)  mloss: 0.3199 (0.7415)  time: 0.6316  data: 0.0002  max mem: 29573
[23:30:41.772749] Epoch: [0]  [ 9670/13501]  eta: 0:40:22  lr: 0.000045  closs: 0.3199 (0.7411)  mloss: 0.3199 (0.7411)  time: 0.6322  data: 0.0002  max mem: 29573
[23:30:48.133267] Epoch: [0]  [ 9680/13501]  eta: 0:40:16  lr: 0.000045  closs: 0.3163 (0.7406)  mloss: 0.3163 (0.7406)  time: 0.6329  data: 0.0002  max mem: 29573
[23:30:54.431107] Epoch: [0]  [ 9690/13501]  eta: 0:40:09  lr: 0.000045  closs: 0.3266 (0.7402)  mloss: 0.3266 (0.7402)  time: 0.6328  data: 0.0002  max mem: 29573
[23:31:00.775711] Epoch: [0]  [ 9700/13501]  eta: 0:40:03  lr: 0.000045  closs: 0.3357 (0.7398)  mloss: 0.3357 (0.7398)  time: 0.6320  data: 0.0002  max mem: 29573
[23:31:07.056542] Epoch: [0]  [ 9710/13501]  eta: 0:39:57  lr: 0.000045  closs: 0.3265 (0.7394)  mloss: 0.3265 (0.7394)  time: 0.6312  data: 0.0002  max mem: 29573
[23:31:13.397608] Epoch: [0]  [ 9720/13501]  eta: 0:39:50  lr: 0.000045  closs: 0.2677 (0.7389)  mloss: 0.2677 (0.7389)  time: 0.6310  data: 0.0002  max mem: 29573
[23:31:19.690561] Epoch: [0]  [ 9730/13501]  eta: 0:39:44  lr: 0.000045  closs: 0.2677 (0.7385)  mloss: 0.2677 (0.7385)  time: 0.6316  data: 0.0002  max mem: 29573
[23:31:26.034372] Epoch: [0]  [ 9740/13501]  eta: 0:39:38  lr: 0.000045  closs: 0.3349 (0.7381)  mloss: 0.3349 (0.7381)  time: 0.6318  data: 0.0002  max mem: 29573
[23:31:32.322886] Epoch: [0]  [ 9750/13501]  eta: 0:39:31  lr: 0.000045  closs: 0.3349 (0.7377)  mloss: 0.3349 (0.7377)  time: 0.6315  data: 0.0002  max mem: 29573
[23:31:38.657293] Epoch: [0]  [ 9760/13501]  eta: 0:39:25  lr: 0.000045  closs: 0.3302 (0.7373)  mloss: 0.3302 (0.7373)  time: 0.6311  data: 0.0002  max mem: 29573
[23:31:44.937088] Epoch: [0]  [ 9770/13501]  eta: 0:39:19  lr: 0.000045  closs: 0.3220 (0.7369)  mloss: 0.3220 (0.7369)  time: 0.6306  data: 0.0002  max mem: 29573
[23:31:51.295937] Epoch: [0]  [ 9780/13501]  eta: 0:39:13  lr: 0.000045  closs: 0.3123 (0.7365)  mloss: 0.3123 (0.7365)  time: 0.6319  data: 0.0002  max mem: 29573
[23:31:57.585817] Epoch: [0]  [ 9790/13501]  eta: 0:39:06  lr: 0.000045  closs: 0.3341 (0.7361)  mloss: 0.3341 (0.7361)  time: 0.6324  data: 0.0002  max mem: 29573
[23:32:03.924625] Epoch: [0]  [ 9800/13501]  eta: 0:39:00  lr: 0.000045  closs: 0.3341 (0.7357)  mloss: 0.3341 (0.7357)  time: 0.6314  data: 0.0002  max mem: 29573
[23:32:10.211406] Epoch: [0]  [ 9810/13501]  eta: 0:38:54  lr: 0.000045  closs: 0.3273 (0.7353)  mloss: 0.3273 (0.7353)  time: 0.6312  data: 0.0002  max mem: 29573
[23:32:16.541745] Epoch: [0]  [ 9820/13501]  eta: 0:38:47  lr: 0.000045  closs: 0.3287 (0.7349)  mloss: 0.3287 (0.7349)  time: 0.6308  data: 0.0002  max mem: 29573
[23:32:22.828231] Epoch: [0]  [ 9830/13501]  eta: 0:38:41  lr: 0.000045  closs: 0.3100 (0.7345)  mloss: 0.3100 (0.7345)  time: 0.6308  data: 0.0002  max mem: 29573
[23:32:29.175945] Epoch: [0]  [ 9840/13501]  eta: 0:38:35  lr: 0.000046  closs: 0.3094 (0.7341)  mloss: 0.3094 (0.7341)  time: 0.6316  data: 0.0002  max mem: 29573
[23:32:35.460740] Epoch: [0]  [ 9850/13501]  eta: 0:38:28  lr: 0.000046  closs: 0.3156 (0.7337)  mloss: 0.3156 (0.7337)  time: 0.6316  data: 0.0002  max mem: 29573
[23:32:41.801669] Epoch: [0]  [ 9860/13501]  eta: 0:38:22  lr: 0.000046  closs: 0.3329 (0.7333)  mloss: 0.3329 (0.7333)  time: 0.6312  data: 0.0002  max mem: 29573
[23:32:48.085014] Epoch: [0]  [ 9870/13501]  eta: 0:38:16  lr: 0.000046  closs: 0.3326 (0.7329)  mloss: 0.3326 (0.7329)  time: 0.6311  data: 0.0002  max mem: 29573
[23:32:54.440069] Epoch: [0]  [ 9880/13501]  eta: 0:38:09  lr: 0.000046  closs: 0.3019 (0.7325)  mloss: 0.3019 (0.7325)  time: 0.6318  data: 0.0002  max mem: 29573
[23:33:00.753217] Epoch: [0]  [ 9890/13501]  eta: 0:38:03  lr: 0.000046  closs: 0.3299 (0.7321)  mloss: 0.3299 (0.7321)  time: 0.6333  data: 0.0002  max mem: 29573
[23:33:07.099899] Epoch: [0]  [ 9900/13501]  eta: 0:37:57  lr: 0.000046  closs: 0.3454 (0.7317)  mloss: 0.3454 (0.7317)  time: 0.6329  data: 0.0002  max mem: 29573
[23:33:09.721358] /work/u8915687/big-superb/big-superb-train-data/SpeechDetection_Aishell1Train/train/BAC009S0423W0203.wav
[23:33:13.385839] Epoch: [0]  [ 9910/13501]  eta: 0:37:50  lr: 0.000046  closs: 0.3576 (0.7313)  mloss: 0.3576 (0.7313)  time: 0.6316  data: 0.0002  max mem: 29573
[23:33:19.724591] Epoch: [0]  [ 9920/13501]  eta: 0:37:44  lr: 0.000046  closs: 0.3591 (0.7310)  mloss: 0.3591 (0.7310)  time: 0.6312  data: 0.0002  max mem: 29573
[23:33:26.015267] Epoch: [0]  [ 9930/13501]  eta: 0:37:38  lr: 0.000046  closs: 0.3011 (0.7305)  mloss: 0.3011 (0.7305)  time: 0.6314  data: 0.0002  max mem: 29573
[23:33:32.360650] Epoch: [0]  [ 9940/13501]  eta: 0:37:31  lr: 0.000046  closs: 0.3138 (0.7302)  mloss: 0.3138 (0.7302)  time: 0.6317  data: 0.0002  max mem: 29573
[23:33:38.649563] Epoch: [0]  [ 9950/13501]  eta: 0:37:25  lr: 0.000046  closs: 0.3587 (0.7298)  mloss: 0.3587 (0.7298)  time: 0.6316  data: 0.0002  max mem: 29573
[23:33:44.987271] Epoch: [0]  [ 9960/13501]  eta: 0:37:19  lr: 0.000046  closs: 0.3542 (0.7295)  mloss: 0.3542 (0.7295)  time: 0.6313  data: 0.0002  max mem: 29573
[23:33:51.288766] Epoch: [0]  [ 9970/13501]  eta: 0:37:12  lr: 0.000046  closs: 0.3576 (0.7291)  mloss: 0.3576 (0.7291)  time: 0.6319  data: 0.0002  max mem: 29573
[23:33:57.629037] Epoch: [0]  [ 9980/13501]  eta: 0:37:06  lr: 0.000046  closs: 0.3430 (0.7287)  mloss: 0.3430 (0.7287)  time: 0.6320  data: 0.0002  max mem: 29573
[23:34:03.931623] Epoch: [0]  [ 9990/13501]  eta: 0:37:00  lr: 0.000046  closs: 0.3354 (0.7283)  mloss: 0.3354 (0.7283)  time: 0.6321  data: 0.0002  max mem: 29573
[23:34:10.279961] Epoch: [0]  [10000/13501]  eta: 0:36:53  lr: 0.000046  closs: 0.3193 (0.7279)  mloss: 0.3193 (0.7279)  time: 0.6325  data: 0.0003  max mem: 29573
[23:34:16.575775] Epoch: [0]  [10010/13501]  eta: 0:36:47  lr: 0.000046  closs: 0.3193 (0.7275)  mloss: 0.3193 (0.7275)  time: 0.6321  data: 0.0003  max mem: 29573
[23:34:22.919326] Epoch: [0]  [10020/13501]  eta: 0:36:41  lr: 0.000046  closs: 0.3735 (0.7271)  mloss: 0.3735 (0.7271)  time: 0.6319  data: 0.0002  max mem: 29573
[23:34:29.213237] Epoch: [0]  [10030/13501]  eta: 0:36:34  lr: 0.000046  closs: 0.3313 (0.7268)  mloss: 0.3313 (0.7268)  time: 0.6318  data: 0.0003  max mem: 29573
[23:34:35.555152] Epoch: [0]  [10040/13501]  eta: 0:36:28  lr: 0.000046  closs: 0.3287 (0.7264)  mloss: 0.3287 (0.7264)  time: 0.6317  data: 0.0003  max mem: 29573
[23:34:41.851803] Epoch: [0]  [10050/13501]  eta: 0:36:22  lr: 0.000047  closs: 0.3307 (0.7260)  mloss: 0.3307 (0.7260)  time: 0.6318  data: 0.0002  max mem: 29573
[23:34:48.192506] Epoch: [0]  [10060/13501]  eta: 0:36:15  lr: 0.000047  closs: 0.3172 (0.7256)  mloss: 0.3172 (0.7256)  time: 0.6318  data: 0.0002  max mem: 29573
[23:34:54.487004] Epoch: [0]  [10070/13501]  eta: 0:36:09  lr: 0.000047  closs: 0.3337 (0.7252)  mloss: 0.3337 (0.7252)  time: 0.6317  data: 0.0002  max mem: 29573
[23:35:00.828064] Epoch: [0]  [10080/13501]  eta: 0:36:03  lr: 0.000047  closs: 0.3411 (0.7249)  mloss: 0.3411 (0.7249)  time: 0.6317  data: 0.0002  max mem: 29573
[23:35:07.119768] Epoch: [0]  [10090/13501]  eta: 0:35:56  lr: 0.000047  closs: 0.3602 (0.7245)  mloss: 0.3602 (0.7245)  time: 0.6316  data: 0.0002  max mem: 29573
[23:35:13.474090] Epoch: [0]  [10100/13501]  eta: 0:35:50  lr: 0.000047  closs: 0.3574 (0.7242)  mloss: 0.3574 (0.7242)  time: 0.6322  data: 0.0002  max mem: 29573
[23:35:19.759592] Epoch: [0]  [10110/13501]  eta: 0:35:44  lr: 0.000047  closs: 0.3484 (0.7238)  mloss: 0.3484 (0.7238)  time: 0.6319  data: 0.0002  max mem: 29573
[23:35:26.085453] Epoch: [0]  [10120/13501]  eta: 0:35:37  lr: 0.000047  closs: 0.3417 (0.7234)  mloss: 0.3417 (0.7234)  time: 0.6305  data: 0.0002  max mem: 29573
[23:35:32.371099] Epoch: [0]  [10130/13501]  eta: 0:35:31  lr: 0.000047  closs: 0.3221 (0.7231)  mloss: 0.3221 (0.7231)  time: 0.6305  data: 0.0002  max mem: 29573
[23:35:38.713013] Epoch: [0]  [10140/13501]  eta: 0:35:25  lr: 0.000047  closs: 0.3448 (0.7227)  mloss: 0.3448 (0.7227)  time: 0.6313  data: 0.0003  max mem: 29573
[23:35:45.009106] Epoch: [0]  [10150/13501]  eta: 0:35:18  lr: 0.000047  closs: 0.3448 (0.7223)  mloss: 0.3448 (0.7223)  time: 0.6318  data: 0.0003  max mem: 29573
[23:35:51.348508] Epoch: [0]  [10160/13501]  eta: 0:35:12  lr: 0.000047  closs: 0.3380 (0.7220)  mloss: 0.3380 (0.7220)  time: 0.6317  data: 0.0002  max mem: 29573
[23:35:57.642328] Epoch: [0]  [10170/13501]  eta: 0:35:06  lr: 0.000047  closs: 0.3380 (0.7216)  mloss: 0.3380 (0.7216)  time: 0.6316  data: 0.0002  max mem: 29573
[23:36:03.978703] Epoch: [0]  [10180/13501]  eta: 0:34:59  lr: 0.000047  closs: 0.3379 (0.7212)  mloss: 0.3379 (0.7212)  time: 0.6314  data: 0.0003  max mem: 29573
[23:36:10.274952] Epoch: [0]  [10190/13501]  eta: 0:34:53  lr: 0.000047  closs: 0.3479 (0.7209)  mloss: 0.3479 (0.7209)  time: 0.6316  data: 0.0003  max mem: 29573
[23:36:16.625791] Epoch: [0]  [10200/13501]  eta: 0:34:47  lr: 0.000047  closs: 0.3695 (0.7205)  mloss: 0.3695 (0.7205)  time: 0.6323  data: 0.0002  max mem: 29573
[23:36:22.933546] Epoch: [0]  [10210/13501]  eta: 0:34:41  lr: 0.000047  closs: 0.3863 (0.7202)  mloss: 0.3863 (0.7202)  time: 0.6329  data: 0.0002  max mem: 29573
[23:36:29.274516] Epoch: [0]  [10220/13501]  eta: 0:34:34  lr: 0.000047  closs: 0.3528 (0.7198)  mloss: 0.3528 (0.7198)  time: 0.6324  data: 0.0002  max mem: 29573
[23:36:35.569248] Epoch: [0]  [10230/13501]  eta: 0:34:28  lr: 0.000047  closs: 0.3152 (0.7194)  mloss: 0.3152 (0.7194)  time: 0.6317  data: 0.0002  max mem: 29573
[23:36:41.911808] Epoch: [0]  [10240/13501]  eta: 0:34:22  lr: 0.000047  closs: 0.3142 (0.7190)  mloss: 0.3142 (0.7190)  time: 0.6318  data: 0.0002  max mem: 29573
[23:36:48.199433] Epoch: [0]  [10250/13501]  eta: 0:34:15  lr: 0.000047  closs: 0.3243 (0.7187)  mloss: 0.3243 (0.7187)  time: 0.6314  data: 0.0002  max mem: 29573
[23:36:54.545648] Epoch: [0]  [10260/13501]  eta: 0:34:09  lr: 0.000047  closs: 0.3696 (0.7184)  mloss: 0.3696 (0.7184)  time: 0.6316  data: 0.0002  max mem: 29573
[23:37:00.834171] Epoch: [0]  [10270/13501]  eta: 0:34:03  lr: 0.000048  closs: 0.3648 (0.7179)  mloss: 0.3648 (0.7179)  time: 0.6317  data: 0.0002  max mem: 29573
[23:37:07.177312] Epoch: [0]  [10280/13501]  eta: 0:33:56  lr: 0.000048  closs: 0.2982 (0.7176)  mloss: 0.2982 (0.7176)  time: 0.6315  data: 0.0002  max mem: 29573
[23:37:13.465071] Epoch: [0]  [10290/13501]  eta: 0:33:50  lr: 0.000048  closs: 0.3792 (0.7172)  mloss: 0.3792 (0.7172)  time: 0.6315  data: 0.0002  max mem: 29573
[23:37:19.816956] Epoch: [0]  [10300/13501]  eta: 0:33:44  lr: 0.000048  closs: 0.3521 (0.7169)  mloss: 0.3521 (0.7169)  time: 0.6319  data: 0.0002  max mem: 29573
[23:37:26.119009] Epoch: [0]  [10310/13501]  eta: 0:33:37  lr: 0.000048  closs: 0.3453 (0.7165)  mloss: 0.3453 (0.7165)  time: 0.6326  data: 0.0002  max mem: 29573
[23:37:32.466500] Epoch: [0]  [10320/13501]  eta: 0:33:31  lr: 0.000048  closs: 0.3604 (0.7162)  mloss: 0.3604 (0.7162)  time: 0.6324  data: 0.0002  max mem: 29573
[23:37:38.754388] Epoch: [0]  [10330/13501]  eta: 0:33:25  lr: 0.000048  closs: 0.3488 (0.7158)  mloss: 0.3488 (0.7158)  time: 0.6317  data: 0.0002  max mem: 29573
[23:37:45.098128] Epoch: [0]  [10340/13501]  eta: 0:33:18  lr: 0.000048  closs: 0.3228 (0.7155)  mloss: 0.3228 (0.7155)  time: 0.6315  data: 0.0002  max mem: 29573
[23:37:51.388268] Epoch: [0]  [10350/13501]  eta: 0:33:12  lr: 0.000048  closs: 0.3222 (0.7151)  mloss: 0.3222 (0.7151)  time: 0.6316  data: 0.0002  max mem: 29573
[23:37:57.741040] Epoch: [0]  [10360/13501]  eta: 0:33:06  lr: 0.000048  closs: 0.3185 (0.7147)  mloss: 0.3185 (0.7147)  time: 0.6321  data: 0.0002  max mem: 29573
[23:38:04.030954] Epoch: [0]  [10370/13501]  eta: 0:32:59  lr: 0.000048  closs: 0.3291 (0.7144)  mloss: 0.3291 (0.7144)  time: 0.6321  data: 0.0002  max mem: 29573
[23:38:10.380274] Epoch: [0]  [10380/13501]  eta: 0:32:53  lr: 0.000048  closs: 0.3503 (0.7140)  mloss: 0.3503 (0.7140)  time: 0.6319  data: 0.0002  max mem: 29573
[23:38:16.677749] Epoch: [0]  [10390/13501]  eta: 0:32:47  lr: 0.000048  closs: 0.3040 (0.7137)  mloss: 0.3040 (0.7137)  time: 0.6323  data: 0.0002  max mem: 29573
[23:38:23.025836] Epoch: [0]  [10400/13501]  eta: 0:32:40  lr: 0.000048  closs: 0.3112 (0.7133)  mloss: 0.3112 (0.7133)  time: 0.6322  data: 0.0002  max mem: 29573
[23:38:29.311000] Epoch: [0]  [10410/13501]  eta: 0:32:34  lr: 0.000048  closs: 0.3212 (0.7129)  mloss: 0.3212 (0.7129)  time: 0.6316  data: 0.0002  max mem: 29573
[23:38:35.682261] Epoch: [0]  [10420/13501]  eta: 0:32:28  lr: 0.000048  closs: 0.3092 (0.7126)  mloss: 0.3092 (0.7126)  time: 0.6327  data: 0.0002  max mem: 29573
[23:38:41.977187] Epoch: [0]  [10430/13501]  eta: 0:32:21  lr: 0.000048  closs: 0.3572 (0.7123)  mloss: 0.3572 (0.7123)  time: 0.6332  data: 0.0002  max mem: 29573
[23:38:48.314181] Epoch: [0]  [10440/13501]  eta: 0:32:15  lr: 0.000048  closs: 0.3666 (0.7119)  mloss: 0.3666 (0.7119)  time: 0.6315  data: 0.0002  max mem: 29573
[23:38:54.606970] Epoch: [0]  [10450/13501]  eta: 0:32:09  lr: 0.000048  closs: 0.3455 (0.7115)  mloss: 0.3455 (0.7115)  time: 0.6314  data: 0.0002  max mem: 29573
[23:39:00.941909] Epoch: [0]  [10460/13501]  eta: 0:32:02  lr: 0.000048  closs: 0.3220 (0.7112)  mloss: 0.3220 (0.7112)  time: 0.6313  data: 0.0002  max mem: 29573
[23:39:07.231096] Epoch: [0]  [10470/13501]  eta: 0:31:56  lr: 0.000048  closs: 0.3220 (0.7109)  mloss: 0.3220 (0.7109)  time: 0.6311  data: 0.0002  max mem: 29573
[23:39:13.563871] Epoch: [0]  [10480/13501]  eta: 0:31:50  lr: 0.000049  closs: 0.3764 (0.7105)  mloss: 0.3764 (0.7105)  time: 0.6310  data: 0.0002  max mem: 29573
[23:39:19.851217] Epoch: [0]  [10490/13501]  eta: 0:31:43  lr: 0.000049  closs: 0.3669 (0.7102)  mloss: 0.3669 (0.7102)  time: 0.6309  data: 0.0002  max mem: 29573
[23:39:26.194074] Epoch: [0]  [10500/13501]  eta: 0:31:37  lr: 0.000049  closs: 0.3669 (0.7099)  mloss: 0.3669 (0.7099)  time: 0.6314  data: 0.0002  max mem: 29573
[23:39:32.485991] Epoch: [0]  [10510/13501]  eta: 0:31:31  lr: 0.000049  closs: 0.3447 (0.7095)  mloss: 0.3447 (0.7095)  time: 0.6317  data: 0.0002  max mem: 29573
[23:39:38.851043] Epoch: [0]  [10520/13501]  eta: 0:31:24  lr: 0.000049  closs: 0.3232 (0.7092)  mloss: 0.3232 (0.7092)  time: 0.6328  data: 0.0002  max mem: 29573
[23:39:45.161945] Epoch: [0]  [10530/13501]  eta: 0:31:18  lr: 0.000049  closs: 0.3202 (0.7088)  mloss: 0.3202 (0.7088)  time: 0.6337  data: 0.0002  max mem: 29573
[23:39:51.497375] Epoch: [0]  [10540/13501]  eta: 0:31:12  lr: 0.000049  closs: 0.3230 (0.7084)  mloss: 0.3230 (0.7084)  time: 0.6322  data: 0.0002  max mem: 29573
[23:39:57.785109] Epoch: [0]  [10550/13501]  eta: 0:31:05  lr: 0.000049  closs: 0.3387 (0.7081)  mloss: 0.3387 (0.7081)  time: 0.6311  data: 0.0002  max mem: 29573
[23:40:04.131090] Epoch: [0]  [10560/13501]  eta: 0:30:59  lr: 0.000049  closs: 0.3515 (0.7078)  mloss: 0.3515 (0.7078)  time: 0.6316  data: 0.0002  max mem: 29573
[23:40:10.413890] Epoch: [0]  [10570/13501]  eta: 0:30:53  lr: 0.000049  closs: 0.3515 (0.7074)  mloss: 0.3515 (0.7074)  time: 0.6314  data: 0.0002  max mem: 29573
[23:40:16.750162] Epoch: [0]  [10580/13501]  eta: 0:30:46  lr: 0.000049  closs: 0.3206 (0.7071)  mloss: 0.3206 (0.7071)  time: 0.6309  data: 0.0002  max mem: 29573
[23:40:23.037750] Epoch: [0]  [10590/13501]  eta: 0:30:40  lr: 0.000049  closs: 0.3150 (0.7067)  mloss: 0.3150 (0.7067)  time: 0.6311  data: 0.0002  max mem: 29573
[23:40:29.392782] Epoch: [0]  [10600/13501]  eta: 0:30:34  lr: 0.000049  closs: 0.3150 (0.7064)  mloss: 0.3150 (0.7064)  time: 0.6320  data: 0.0002  max mem: 29573
[23:40:35.690673] Epoch: [0]  [10610/13501]  eta: 0:30:28  lr: 0.000049  closs: 0.3444 (0.7061)  mloss: 0.3444 (0.7061)  time: 0.6326  data: 0.0002  max mem: 29573
[23:40:42.029535] Epoch: [0]  [10620/13501]  eta: 0:30:21  lr: 0.000049  closs: 0.3612 (0.7057)  mloss: 0.3612 (0.7057)  time: 0.6318  data: 0.0002  max mem: 29573
[23:40:48.321790] Epoch: [0]  [10630/13501]  eta: 0:30:15  lr: 0.000049  closs: 0.3534 (0.7054)  mloss: 0.3534 (0.7054)  time: 0.6315  data: 0.0002  max mem: 29573
[23:40:54.660241] Epoch: [0]  [10640/13501]  eta: 0:30:09  lr: 0.000049  closs: 0.3551 (0.7051)  mloss: 0.3551 (0.7051)  time: 0.6315  data: 0.0002  max mem: 29573
[23:41:00.949006] Epoch: [0]  [10650/13501]  eta: 0:30:02  lr: 0.000049  closs: 0.3934 (0.7048)  mloss: 0.3934 (0.7048)  time: 0.6313  data: 0.0002  max mem: 29573
[23:41:07.287699] Epoch: [0]  [10660/13501]  eta: 0:29:56  lr: 0.000049  closs: 0.3966 (0.7045)  mloss: 0.3966 (0.7045)  time: 0.6313  data: 0.0002  max mem: 29573
[23:41:13.578194] Epoch: [0]  [10670/13501]  eta: 0:29:50  lr: 0.000049  closs: 0.3401 (0.7042)  mloss: 0.3401 (0.7042)  time: 0.6314  data: 0.0002  max mem: 29573
[23:41:19.913093] Epoch: [0]  [10680/13501]  eta: 0:29:43  lr: 0.000049  closs: 0.3401 (0.7038)  mloss: 0.3401 (0.7038)  time: 0.6312  data: 0.0002  max mem: 29573
[23:41:26.199932] Epoch: [0]  [10690/13501]  eta: 0:29:37  lr: 0.000049  closs: 0.3078 (0.7035)  mloss: 0.3078 (0.7035)  time: 0.6310  data: 0.0002  max mem: 29573
[23:41:32.505919] Epoch: [0]  [10700/13501]  eta: 0:29:31  lr: 0.000050  closs: 0.3078 (0.7031)  mloss: 0.3078 (0.7031)  time: 0.6296  data: 0.0002  max mem: 29573
[23:41:38.800326] Epoch: [0]  [10710/13501]  eta: 0:29:24  lr: 0.000050  closs: 0.3008 (0.7028)  mloss: 0.3008 (0.7028)  time: 0.6299  data: 0.0002  max mem: 29573
[23:41:45.140133] Epoch: [0]  [10720/13501]  eta: 0:29:18  lr: 0.000050  closs: 0.3242 (0.7024)  mloss: 0.3242 (0.7024)  time: 0.6316  data: 0.0002  max mem: 29573
[23:41:51.442841] Epoch: [0]  [10730/13501]  eta: 0:29:12  lr: 0.000050  closs: 0.3376 (0.7021)  mloss: 0.3376 (0.7021)  time: 0.6321  data: 0.0002  max mem: 29573
[23:41:57.807766] Epoch: [0]  [10740/13501]  eta: 0:29:05  lr: 0.000050  closs: 0.3258 (0.7018)  mloss: 0.3258 (0.7018)  time: 0.6333  data: 0.0002  max mem: 29573
[23:42:04.101906] Epoch: [0]  [10750/13501]  eta: 0:28:59  lr: 0.000050  closs: 0.3258 (0.7014)  mloss: 0.3258 (0.7014)  time: 0.6329  data: 0.0002  max mem: 29573
[23:42:10.444349] Epoch: [0]  [10760/13501]  eta: 0:28:53  lr: 0.000050  closs: 0.3136 (0.7011)  mloss: 0.3136 (0.7011)  time: 0.6318  data: 0.0002  max mem: 29573
[23:42:16.742350] Epoch: [0]  [10770/13501]  eta: 0:28:46  lr: 0.000050  closs: 0.3189 (0.7008)  mloss: 0.3189 (0.7008)  time: 0.6319  data: 0.0002  max mem: 29573
[23:42:23.074611] Epoch: [0]  [10780/13501]  eta: 0:28:40  lr: 0.000050  closs: 0.3365 (0.7005)  mloss: 0.3365 (0.7005)  time: 0.6314  data: 0.0002  max mem: 29573
[23:42:29.366372] Epoch: [0]  [10790/13501]  eta: 0:28:34  lr: 0.000050  closs: 0.3254 (0.7001)  mloss: 0.3254 (0.7001)  time: 0.6311  data: 0.0002  max mem: 29573
[23:42:35.704263] Epoch: [0]  [10800/13501]  eta: 0:28:27  lr: 0.000050  closs: 0.3240 (0.6998)  mloss: 0.3240 (0.6998)  time: 0.6314  data: 0.0002  max mem: 29573
[23:42:41.984813] Epoch: [0]  [10810/13501]  eta: 0:28:21  lr: 0.000050  closs: 0.3437 (0.6995)  mloss: 0.3437 (0.6995)  time: 0.6308  data: 0.0002  max mem: 29573
[23:42:48.328320] Epoch: [0]  [10820/13501]  eta: 0:28:15  lr: 0.000050  closs: 0.3415 (0.6992)  mloss: 0.3415 (0.6992)  time: 0.6311  data: 0.0002  max mem: 29573
[23:42:54.623409] Epoch: [0]  [10830/13501]  eta: 0:28:08  lr: 0.000050  closs: 0.3415 (0.6989)  mloss: 0.3415 (0.6989)  time: 0.6319  data: 0.0002  max mem: 29573
[23:43:00.979717] Epoch: [0]  [10840/13501]  eta: 0:28:02  lr: 0.000050  closs: 0.3720 (0.6986)  mloss: 0.3720 (0.6986)  time: 0.6325  data: 0.0002  max mem: 29573
[23:43:07.284373] Epoch: [0]  [10850/13501]  eta: 0:27:56  lr: 0.000050  closs: 0.3603 (0.6982)  mloss: 0.3603 (0.6982)  time: 0.6330  data: 0.0002  max mem: 29573
[23:43:13.634565] Epoch: [0]  [10860/13501]  eta: 0:27:49  lr: 0.000050  closs: 0.3621 (0.6979)  mloss: 0.3621 (0.6979)  time: 0.6327  data: 0.0002  max mem: 29573
[23:43:19.916356] Epoch: [0]  [10870/13501]  eta: 0:27:43  lr: 0.000050  closs: 0.3644 (0.6976)  mloss: 0.3644 (0.6976)  time: 0.6315  data: 0.0002  max mem: 29573
[23:43:26.259048] Epoch: [0]  [10880/13501]  eta: 0:27:37  lr: 0.000050  closs: 0.3644 (0.6973)  mloss: 0.3644 (0.6973)  time: 0.6311  data: 0.0002  max mem: 29573
[23:43:32.611939] Epoch: [0]  [10890/13501]  eta: 0:27:30  lr: 0.000050  closs: 0.3432 (0.6970)  mloss: 0.3432 (0.6970)  time: 0.6347  data: 0.0002  max mem: 29573
[23:43:38.947988] Epoch: [0]  [10900/13501]  eta: 0:27:24  lr: 0.000050  closs: 0.3432 (0.6967)  mloss: 0.3432 (0.6967)  time: 0.6344  data: 0.0002  max mem: 29573
[23:43:45.243214] Epoch: [0]  [10910/13501]  eta: 0:27:18  lr: 0.000050  closs: 0.3467 (0.6964)  mloss: 0.3467 (0.6964)  time: 0.6315  data: 0.0002  max mem: 29573
[23:43:51.577519] Epoch: [0]  [10920/13501]  eta: 0:27:11  lr: 0.000051  closs: 0.3489 (0.6960)  mloss: 0.3489 (0.6960)  time: 0.6314  data: 0.0002  max mem: 29573
[23:43:57.861997] Epoch: [0]  [10930/13501]  eta: 0:27:05  lr: 0.000051  closs: 0.3460 (0.6957)  mloss: 0.3460 (0.6957)  time: 0.6309  data: 0.0002  max mem: 29573
[23:44:04.204409] Epoch: [0]  [10940/13501]  eta: 0:26:59  lr: 0.000051  closs: 0.3460 (0.6954)  mloss: 0.3460 (0.6954)  time: 0.6313  data: 0.0002  max mem: 29573
[23:44:10.515182] Epoch: [0]  [10950/13501]  eta: 0:26:52  lr: 0.000051  closs: 0.3592 (0.6951)  mloss: 0.3592 (0.6951)  time: 0.6326  data: 0.0002  max mem: 29573
[23:44:16.867712] Epoch: [0]  [10960/13501]  eta: 0:26:46  lr: 0.000051  closs: 0.3178 (0.6947)  mloss: 0.3178 (0.6947)  time: 0.6331  data: 0.0002  max mem: 29573
[23:44:23.154384] Epoch: [0]  [10970/13501]  eta: 0:26:40  lr: 0.000051  closs: 0.3410 (0.6944)  mloss: 0.3410 (0.6944)  time: 0.6319  data: 0.0002  max mem: 29573
[23:44:29.495479] Epoch: [0]  [10980/13501]  eta: 0:26:34  lr: 0.000051  closs: 0.3501 (0.6941)  mloss: 0.3501 (0.6941)  time: 0.6313  data: 0.0002  max mem: 29573
[23:44:35.785879] Epoch: [0]  [10990/13501]  eta: 0:26:27  lr: 0.000051  closs: 0.3722 (0.6938)  mloss: 0.3722 (0.6938)  time: 0.6315  data: 0.0002  max mem: 29573
[23:44:42.134085] Epoch: [0]  [11000/13501]  eta: 0:26:21  lr: 0.000051  closs: 0.3642 (0.6935)  mloss: 0.3642 (0.6935)  time: 0.6319  data: 0.0002  max mem: 29573
[23:44:48.425998] Epoch: [0]  [11010/13501]  eta: 0:26:15  lr: 0.000051  closs: 0.3284 (0.6932)  mloss: 0.3284 (0.6932)  time: 0.6319  data: 0.0002  max mem: 29573
[23:44:54.768391] Epoch: [0]  [11020/13501]  eta: 0:26:08  lr: 0.000051  closs: 0.3183 (0.6929)  mloss: 0.3183 (0.6929)  time: 0.6316  data: 0.0002  max mem: 29573
[23:45:01.056619] Epoch: [0]  [11030/13501]  eta: 0:26:02  lr: 0.000051  closs: 0.3205 (0.6925)  mloss: 0.3205 (0.6925)  time: 0.6315  data: 0.0002  max mem: 29573
[23:45:07.407412] Epoch: [0]  [11040/13501]  eta: 0:25:56  lr: 0.000051  closs: 0.3289 (0.6922)  mloss: 0.3289 (0.6922)  time: 0.6319  data: 0.0002  max mem: 29573
[23:45:13.709159] Epoch: [0]  [11050/13501]  eta: 0:25:49  lr: 0.000051  closs: 0.3318 (0.6919)  mloss: 0.3318 (0.6919)  time: 0.6325  data: 0.0002  max mem: 29573
[23:45:20.070293] Epoch: [0]  [11060/13501]  eta: 0:25:43  lr: 0.000051  closs: 0.3397 (0.6916)  mloss: 0.3397 (0.6916)  time: 0.6331  data: 0.0002  max mem: 29573
[23:45:26.364093] Epoch: [0]  [11070/13501]  eta: 0:25:37  lr: 0.000051  closs: 0.3397 (0.6912)  mloss: 0.3397 (0.6912)  time: 0.6327  data: 0.0002  max mem: 29573
[23:45:32.701359] Epoch: [0]  [11080/13501]  eta: 0:25:30  lr: 0.000051  closs: 0.3099 (0.6909)  mloss: 0.3099 (0.6909)  time: 0.6315  data: 0.0002  max mem: 29573
[23:45:39.000074] Epoch: [0]  [11090/13501]  eta: 0:25:24  lr: 0.000051  closs: 0.2633 (0.6905)  mloss: 0.2633 (0.6905)  time: 0.6317  data: 0.0002  max mem: 29573
[23:45:45.352699] Epoch: [0]  [11100/13501]  eta: 0:25:18  lr: 0.000051  closs: 0.3141 (0.6903)  mloss: 0.3141 (0.6903)  time: 0.6325  data: 0.0002  max mem: 29573
[23:45:51.642181] Epoch: [0]  [11110/13501]  eta: 0:25:11  lr: 0.000051  closs: 0.3493 (0.6899)  mloss: 0.3493 (0.6899)  time: 0.6320  data: 0.0002  max mem: 29573
[23:45:57.998470] Epoch: [0]  [11120/13501]  eta: 0:25:05  lr: 0.000051  closs: 0.2931 (0.6896)  mloss: 0.2931 (0.6896)  time: 0.6322  data: 0.0004  max mem: 29573
[23:46:04.294306] Epoch: [0]  [11130/13501]  eta: 0:24:59  lr: 0.000052  closs: 0.2897 (0.6893)  mloss: 0.2897 (0.6893)  time: 0.6325  data: 0.0004  max mem: 29573
[23:46:10.638097] Epoch: [0]  [11140/13501]  eta: 0:24:52  lr: 0.000052  closs: 0.3639 (0.6890)  mloss: 0.3639 (0.6890)  time: 0.6319  data: 0.0003  max mem: 29573
[23:46:16.923788] Epoch: [0]  [11150/13501]  eta: 0:24:46  lr: 0.000052  closs: 0.3639 (0.6887)  mloss: 0.3639 (0.6887)  time: 0.6314  data: 0.0003  max mem: 29573
[23:46:23.274005] Epoch: [0]  [11160/13501]  eta: 0:24:40  lr: 0.000052  closs: 0.3440 (0.6884)  mloss: 0.3440 (0.6884)  time: 0.6317  data: 0.0002  max mem: 29573
[23:46:29.569420] Epoch: [0]  [11170/13501]  eta: 0:24:33  lr: 0.000052  closs: 0.2911 (0.6881)  mloss: 0.2911 (0.6881)  time: 0.6322  data: 0.0002  max mem: 29573
[23:46:35.910296] Epoch: [0]  [11180/13501]  eta: 0:24:27  lr: 0.000052  closs: 0.3150 (0.6878)  mloss: 0.3150 (0.6878)  time: 0.6317  data: 0.0002  max mem: 29573
[23:46:42.198408] Epoch: [0]  [11190/13501]  eta: 0:24:21  lr: 0.000052  closs: 0.3183 (0.6874)  mloss: 0.3183 (0.6874)  time: 0.6314  data: 0.0002  max mem: 29573
[23:46:48.540222] Epoch: [0]  [11200/13501]  eta: 0:24:14  lr: 0.000052  closs: 0.3167 (0.6871)  mloss: 0.3167 (0.6871)  time: 0.6314  data: 0.0002  max mem: 29573
[23:46:54.834948] Epoch: [0]  [11210/13501]  eta: 0:24:08  lr: 0.000052  closs: 0.3227 (0.6868)  mloss: 0.3227 (0.6868)  time: 0.6318  data: 0.0003  max mem: 29573
[23:47:01.180722] Epoch: [0]  [11220/13501]  eta: 0:24:02  lr: 0.000052  closs: 0.3489 (0.6865)  mloss: 0.3489 (0.6865)  time: 0.6319  data: 0.0002  max mem: 29573
[23:47:07.471053] Epoch: [0]  [11230/13501]  eta: 0:23:55  lr: 0.000052  closs: 0.3664 (0.6862)  mloss: 0.3664 (0.6862)  time: 0.6317  data: 0.0002  max mem: 29573
[23:47:13.814782] Epoch: [0]  [11240/13501]  eta: 0:23:49  lr: 0.000052  closs: 0.2962 (0.6859)  mloss: 0.2962 (0.6859)  time: 0.6316  data: 0.0002  max mem: 29573
[23:47:20.105288] Epoch: [0]  [11250/13501]  eta: 0:23:43  lr: 0.000052  closs: 0.2962 (0.6856)  mloss: 0.2962 (0.6856)  time: 0.6316  data: 0.0002  max mem: 29573
[23:47:26.457028] Epoch: [0]  [11260/13501]  eta: 0:23:36  lr: 0.000052  closs: 0.3597 (0.6854)  mloss: 0.3597 (0.6854)  time: 0.6320  data: 0.0002  max mem: 29573
[23:47:32.759612] Epoch: [0]  [11270/13501]  eta: 0:23:30  lr: 0.000052  closs: 0.3611 (0.6851)  mloss: 0.3611 (0.6851)  time: 0.6326  data: 0.0002  max mem: 29573
[23:47:39.102422] Epoch: [0]  [11280/13501]  eta: 0:23:24  lr: 0.000052  closs: 0.3611 (0.6848)  mloss: 0.3611 (0.6848)  time: 0.6322  data: 0.0002  max mem: 29573
[23:47:45.403226] Epoch: [0]  [11290/13501]  eta: 0:23:17  lr: 0.000052  closs: 0.3542 (0.6845)  mloss: 0.3542 (0.6845)  time: 0.6321  data: 0.0002  max mem: 29573
[23:47:51.749412] Epoch: [0]  [11300/13501]  eta: 0:23:11  lr: 0.000052  closs: 0.3510 (0.6842)  mloss: 0.3510 (0.6842)  time: 0.6323  data: 0.0002  max mem: 29573
[23:47:58.046672] Epoch: [0]  [11310/13501]  eta: 0:23:05  lr: 0.000052  closs: 0.3246 (0.6839)  mloss: 0.3246 (0.6839)  time: 0.6321  data: 0.0002  max mem: 29573
[23:48:04.389725] Epoch: [0]  [11320/13501]  eta: 0:22:59  lr: 0.000052  closs: 0.3020 (0.6836)  mloss: 0.3020 (0.6836)  time: 0.6319  data: 0.0002  max mem: 29573
[23:48:10.673763] Epoch: [0]  [11330/13501]  eta: 0:22:52  lr: 0.000052  closs: 0.2775 (0.6832)  mloss: 0.2775 (0.6832)  time: 0.6313  data: 0.0002  max mem: 29573
[23:48:17.018774] Epoch: [0]  [11340/13501]  eta: 0:22:46  lr: 0.000052  closs: 0.2898 (0.6829)  mloss: 0.2898 (0.6829)  time: 0.6314  data: 0.0002  max mem: 29573
[23:48:23.316719] Epoch: [0]  [11350/13501]  eta: 0:22:40  lr: 0.000053  closs: 0.3567 (0.6827)  mloss: 0.3567 (0.6827)  time: 0.6321  data: 0.0002  max mem: 29573
[23:48:29.653034] Epoch: [0]  [11360/13501]  eta: 0:22:33  lr: 0.000053  closs: 0.3442 (0.6824)  mloss: 0.3442 (0.6824)  time: 0.6316  data: 0.0002  max mem: 29573
[23:48:35.952957] Epoch: [0]  [11370/13501]  eta: 0:22:27  lr: 0.000053  closs: 0.3134 (0.6821)  mloss: 0.3134 (0.6821)  time: 0.6317  data: 0.0002  max mem: 29573
[23:48:42.318080] Epoch: [0]  [11380/13501]  eta: 0:22:21  lr: 0.000053  closs: 0.3425 (0.6818)  mloss: 0.3425 (0.6818)  time: 0.6332  data: 0.0002  max mem: 29573
[23:48:48.609121] Epoch: [0]  [11390/13501]  eta: 0:22:14  lr: 0.000053  closs: 0.3618 (0.6815)  mloss: 0.3618 (0.6815)  time: 0.6327  data: 0.0003  max mem: 29573
[23:48:54.954477] Epoch: [0]  [11400/13501]  eta: 0:22:08  lr: 0.000053  closs: 0.3508 (0.6812)  mloss: 0.3508 (0.6812)  time: 0.6317  data: 0.0003  max mem: 29573
[23:49:01.248361] Epoch: [0]  [11410/13501]  eta: 0:22:02  lr: 0.000053  closs: 0.3494 (0.6809)  mloss: 0.3494 (0.6809)  time: 0.6319  data: 0.0002  max mem: 29573
[23:49:07.596023] Epoch: [0]  [11420/13501]  eta: 0:21:55  lr: 0.000053  closs: 0.3378 (0.6806)  mloss: 0.3378 (0.6806)  time: 0.6320  data: 0.0002  max mem: 29573
[23:49:13.885854] Epoch: [0]  [11430/13501]  eta: 0:21:49  lr: 0.000053  closs: 0.3378 (0.6803)  mloss: 0.3378 (0.6803)  time: 0.6318  data: 0.0002  max mem: 29573
[23:49:20.222019] Epoch: [0]  [11440/13501]  eta: 0:21:43  lr: 0.000053  closs: 0.3384 (0.6800)  mloss: 0.3384 (0.6800)  time: 0.6312  data: 0.0002  max mem: 29573
[23:49:26.509431] Epoch: [0]  [11450/13501]  eta: 0:21:36  lr: 0.000053  closs: 0.3251 (0.6797)  mloss: 0.3251 (0.6797)  time: 0.6311  data: 0.0002  max mem: 29573
[23:49:32.854020] Epoch: [0]  [11460/13501]  eta: 0:21:30  lr: 0.000053  closs: 0.3251 (0.6794)  mloss: 0.3251 (0.6794)  time: 0.6315  data: 0.0002  max mem: 29573
[23:49:39.149813] Epoch: [0]  [11470/13501]  eta: 0:21:24  lr: 0.000053  closs: 0.3464 (0.6791)  mloss: 0.3464 (0.6791)  time: 0.6319  data: 0.0002  max mem: 29573
[23:49:45.505844] Epoch: [0]  [11480/13501]  eta: 0:21:17  lr: 0.000053  closs: 0.3643 (0.6789)  mloss: 0.3643 (0.6789)  time: 0.6325  data: 0.0002  max mem: 29573
[23:49:51.798169] Epoch: [0]  [11490/13501]  eta: 0:21:11  lr: 0.000053  closs: 0.3452 (0.6786)  mloss: 0.3452 (0.6786)  time: 0.6323  data: 0.0002  max mem: 29573
[23:49:58.153709] Epoch: [0]  [11500/13501]  eta: 0:21:05  lr: 0.000053  closs: 0.3264 (0.6783)  mloss: 0.3264 (0.6783)  time: 0.6323  data: 0.0002  max mem: 29573
[23:50:04.445926] Epoch: [0]  [11510/13501]  eta: 0:20:58  lr: 0.000053  closs: 0.3264 (0.6780)  mloss: 0.3264 (0.6780)  time: 0.6323  data: 0.0003  max mem: 29573
[23:50:10.783857] Epoch: [0]  [11520/13501]  eta: 0:20:52  lr: 0.000053  closs: 0.3251 (0.6777)  mloss: 0.3251 (0.6777)  time: 0.6314  data: 0.0003  max mem: 29573
[23:50:17.076530] Epoch: [0]  [11530/13501]  eta: 0:20:46  lr: 0.000053  closs: 0.3273 (0.6774)  mloss: 0.3273 (0.6774)  time: 0.6315  data: 0.0002  max mem: 29573
[23:50:23.425826] Epoch: [0]  [11540/13501]  eta: 0:20:39  lr: 0.000053  closs: 0.3179 (0.6771)  mloss: 0.3179 (0.6771)  time: 0.6320  data: 0.0002  max mem: 29573
[23:50:29.716900] Epoch: [0]  [11550/13501]  eta: 0:20:33  lr: 0.000053  closs: 0.3130 (0.6768)  mloss: 0.3130 (0.6768)  time: 0.6319  data: 0.0002  max mem: 29573
[23:50:36.063390] Epoch: [0]  [11560/13501]  eta: 0:20:27  lr: 0.000054  closs: 0.3160 (0.6765)  mloss: 0.3160 (0.6765)  time: 0.6318  data: 0.0002  max mem: 29573
[23:50:42.356185] Epoch: [0]  [11570/13501]  eta: 0:20:20  lr: 0.000054  closs: 0.3160 (0.6762)  mloss: 0.3160 (0.6762)  time: 0.6319  data: 0.0002  max mem: 29573
[23:50:48.709027] Epoch: [0]  [11580/13501]  eta: 0:20:14  lr: 0.000054  closs: 0.3110 (0.6760)  mloss: 0.3110 (0.6760)  time: 0.6322  data: 0.0002  max mem: 29573
[23:50:55.019874] Epoch: [0]  [11590/13501]  eta: 0:20:08  lr: 0.000054  closs: 0.3225 (0.6757)  mloss: 0.3225 (0.6757)  time: 0.6331  data: 0.0002  max mem: 29573
[23:51:01.354195] Epoch: [0]  [11600/13501]  eta: 0:20:01  lr: 0.000054  closs: 0.3338 (0.6754)  mloss: 0.3338 (0.6754)  time: 0.6322  data: 0.0002  max mem: 29573
[23:51:07.642113] Epoch: [0]  [11610/13501]  eta: 0:19:55  lr: 0.000054  closs: 0.3341 (0.6751)  mloss: 0.3341 (0.6751)  time: 0.6310  data: 0.0002  max mem: 29573
[23:51:13.973688] Epoch: [0]  [11620/13501]  eta: 0:19:49  lr: 0.000054  closs: 0.3346 (0.6748)  mloss: 0.3346 (0.6748)  time: 0.6309  data: 0.0002  max mem: 29573
[23:51:20.264879] Epoch: [0]  [11630/13501]  eta: 0:19:42  lr: 0.000054  closs: 0.3346 (0.6746)  mloss: 0.3346 (0.6746)  time: 0.6311  data: 0.0002  max mem: 29573
[23:51:26.610871] Epoch: [0]  [11640/13501]  eta: 0:19:36  lr: 0.000054  closs: 0.3393 (0.6743)  mloss: 0.3393 (0.6743)  time: 0.6318  data: 0.0002  max mem: 29573
[23:51:32.895122] Epoch: [0]  [11650/13501]  eta: 0:19:30  lr: 0.000054  closs: 0.3230 (0.6740)  mloss: 0.3230 (0.6740)  time: 0.6314  data: 0.0002  max mem: 29573
[23:51:39.235255] Epoch: [0]  [11660/13501]  eta: 0:19:24  lr: 0.000054  closs: 0.3592 (0.6737)  mloss: 0.3592 (0.6737)  time: 0.6311  data: 0.0002  max mem: 29573
[23:51:45.521468] Epoch: [0]  [11670/13501]  eta: 0:19:17  lr: 0.000054  closs: 0.3469 (0.6734)  mloss: 0.3469 (0.6734)  time: 0.6312  data: 0.0002  max mem: 29573
[23:51:51.860384] Epoch: [0]  [11680/13501]  eta: 0:19:11  lr: 0.000054  closs: 0.2922 (0.6731)  mloss: 0.2922 (0.6731)  time: 0.6312  data: 0.0002  max mem: 29573
[23:51:58.172576] Epoch: [0]  [11690/13501]  eta: 0:19:05  lr: 0.000054  closs: 0.2888 (0.6728)  mloss: 0.2888 (0.6728)  time: 0.6325  data: 0.0002  max mem: 29573
[23:52:04.534338] Epoch: [0]  [11700/13501]  eta: 0:18:58  lr: 0.000054  closs: 0.3110 (0.6725)  mloss: 0.3110 (0.6725)  time: 0.6336  data: 0.0002  max mem: 29573
[23:52:10.830569] Epoch: [0]  [11710/13501]  eta: 0:18:52  lr: 0.000054  closs: 0.2798 (0.6722)  mloss: 0.2798 (0.6722)  time: 0.6328  data: 0.0002  max mem: 29573
[23:52:17.180351] Epoch: [0]  [11720/13501]  eta: 0:18:46  lr: 0.000054  closs: 0.3048 (0.6719)  mloss: 0.3048 (0.6719)  time: 0.6322  data: 0.0002  max mem: 29573
[23:52:23.473200] Epoch: [0]  [11730/13501]  eta: 0:18:39  lr: 0.000054  closs: 0.3048 (0.6716)  mloss: 0.3048 (0.6716)  time: 0.6321  data: 0.0002  max mem: 29573
[23:52:29.811184] Epoch: [0]  [11740/13501]  eta: 0:18:33  lr: 0.000054  closs: 0.3649 (0.6714)  mloss: 0.3649 (0.6714)  time: 0.6315  data: 0.0002  max mem: 29573
[23:52:36.101504] Epoch: [0]  [11750/13501]  eta: 0:18:27  lr: 0.000054  closs: 0.4054 (0.6711)  mloss: 0.4054 (0.6711)  time: 0.6313  data: 0.0002  max mem: 29573
[23:52:42.444902] Epoch: [0]  [11760/13501]  eta: 0:18:20  lr: 0.000054  closs: 0.3165 (0.6708)  mloss: 0.3165 (0.6708)  time: 0.6316  data: 0.0002  max mem: 29573
[23:52:48.736508] Epoch: [0]  [11770/13501]  eta: 0:18:14  lr: 0.000054  closs: 0.3165 (0.6705)  mloss: 0.3165 (0.6705)  time: 0.6317  data: 0.0002  max mem: 29573
[23:52:55.079976] Epoch: [0]  [11780/13501]  eta: 0:18:08  lr: 0.000055  closs: 0.3256 (0.6703)  mloss: 0.3256 (0.6703)  time: 0.6317  data: 0.0002  max mem: 29573
[23:53:01.372008] Epoch: [0]  [11790/13501]  eta: 0:18:01  lr: 0.000055  closs: 0.3023 (0.6700)  mloss: 0.3023 (0.6700)  time: 0.6317  data: 0.0002  max mem: 29573
[23:53:07.727836] Epoch: [0]  [11800/13501]  eta: 0:17:55  lr: 0.000055  closs: 0.3027 (0.6697)  mloss: 0.3027 (0.6697)  time: 0.6323  data: 0.0002  max mem: 29573
[23:53:14.013827] Epoch: [0]  [11810/13501]  eta: 0:17:49  lr: 0.000055  closs: 0.3437 (0.6694)  mloss: 0.3437 (0.6694)  time: 0.6320  data: 0.0002  max mem: 29573
[23:53:20.374189] Epoch: [0]  [11820/13501]  eta: 0:17:42  lr: 0.000055  closs: 0.3483 (0.6692)  mloss: 0.3483 (0.6692)  time: 0.6322  data: 0.0002  max mem: 29573
[23:53:26.665281] Epoch: [0]  [11830/13501]  eta: 0:17:36  lr: 0.000055  closs: 0.3369 (0.6689)  mloss: 0.3369 (0.6689)  time: 0.6325  data: 0.0002  max mem: 29573
[23:53:32.999719] Epoch: [0]  [11840/13501]  eta: 0:17:30  lr: 0.000055  closs: 0.3369 (0.6686)  mloss: 0.3369 (0.6686)  time: 0.6312  data: 0.0002  max mem: 29573
[23:53:39.286157] Epoch: [0]  [11850/13501]  eta: 0:17:23  lr: 0.000055  closs: 0.3247 (0.6683)  mloss: 0.3247 (0.6683)  time: 0.6310  data: 0.0002  max mem: 29573
[23:53:45.624069] Epoch: [0]  [11860/13501]  eta: 0:17:17  lr: 0.000055  closs: 0.3247 (0.6681)  mloss: 0.3247 (0.6681)  time: 0.6311  data: 0.0002  max mem: 29573
[23:53:51.921114] Epoch: [0]  [11870/13501]  eta: 0:17:11  lr: 0.000055  closs: 0.3327 (0.6678)  mloss: 0.3327 (0.6678)  time: 0.6317  data: 0.0002  max mem: 29573
[23:53:58.266584] Epoch: [0]  [11880/13501]  eta: 0:17:04  lr: 0.000055  closs: 0.3369 (0.6675)  mloss: 0.3369 (0.6675)  time: 0.6321  data: 0.0002  max mem: 29573
[23:54:04.561280] Epoch: [0]  [11890/13501]  eta: 0:16:58  lr: 0.000055  closs: 0.2995 (0.6672)  mloss: 0.2995 (0.6672)  time: 0.6319  data: 0.0003  max mem: 29573
[23:54:10.924354] Epoch: [0]  [11900/13501]  eta: 0:16:52  lr: 0.000055  closs: 0.3240 (0.6670)  mloss: 0.3240 (0.6670)  time: 0.6328  data: 0.0003  max mem: 29573
[23:54:17.229395] Epoch: [0]  [11910/13501]  eta: 0:16:45  lr: 0.000055  closs: 0.3215 (0.6667)  mloss: 0.3215 (0.6667)  time: 0.6333  data: 0.0002  max mem: 29573
[23:54:23.571017] Epoch: [0]  [11920/13501]  eta: 0:16:39  lr: 0.000055  closs: 0.3215 (0.6664)  mloss: 0.3215 (0.6664)  time: 0.6323  data: 0.0002  max mem: 29573
[23:54:29.863600] Epoch: [0]  [11930/13501]  eta: 0:16:33  lr: 0.000055  closs: 0.3304 (0.6661)  mloss: 0.3304 (0.6661)  time: 0.6316  data: 0.0002  max mem: 29573
[23:54:36.207523] Epoch: [0]  [11940/13501]  eta: 0:16:26  lr: 0.000055  closs: 0.3220 (0.6658)  mloss: 0.3220 (0.6658)  time: 0.6317  data: 0.0002  max mem: 29573
[23:54:42.491979] Epoch: [0]  [11950/13501]  eta: 0:16:20  lr: 0.000055  closs: 0.3350 (0.6655)  mloss: 0.3350 (0.6655)  time: 0.6313  data: 0.0003  max mem: 29573
[23:54:48.826217] Epoch: [0]  [11960/13501]  eta: 0:16:14  lr: 0.000055  closs: 0.3564 (0.6653)  mloss: 0.3564 (0.6653)  time: 0.6309  data: 0.0003  max mem: 29573
[23:54:55.114129] Epoch: [0]  [11970/13501]  eta: 0:16:07  lr: 0.000055  closs: 0.3361 (0.6651)  mloss: 0.3361 (0.6651)  time: 0.6310  data: 0.0002  max mem: 29573
[23:55:01.461925] Epoch: [0]  [11980/13501]  eta: 0:16:01  lr: 0.000055  closs: 0.3283 (0.6648)  mloss: 0.3283 (0.6648)  time: 0.6317  data: 0.0002  max mem: 29573
[23:55:07.756863] Epoch: [0]  [11990/13501]  eta: 0:15:55  lr: 0.000055  closs: 0.3329 (0.6646)  mloss: 0.3329 (0.6646)  time: 0.6321  data: 0.0002  max mem: 29573
[23:55:14.093339] Epoch: [0]  [12000/13501]  eta: 0:15:49  lr: 0.000056  closs: 0.3322 (0.6643)  mloss: 0.3322 (0.6643)  time: 0.6315  data: 0.0002  max mem: 29573
[23:55:20.420244] Epoch: [0]  [12010/13501]  eta: 0:15:42  lr: 0.000056  closs: 0.3097 (0.6640)  mloss: 0.3097 (0.6640)  time: 0.6331  data: 0.0002  max mem: 29573
[23:55:26.767075] Epoch: [0]  [12020/13501]  eta: 0:15:36  lr: 0.000056  closs: 0.3771 (0.6638)  mloss: 0.3771 (0.6638)  time: 0.6336  data: 0.0002  max mem: 29573
[23:55:33.050516] Epoch: [0]  [12030/13501]  eta: 0:15:30  lr: 0.000056  closs: 0.3144 (0.6635)  mloss: 0.3144 (0.6635)  time: 0.6314  data: 0.0002  max mem: 29573
[23:55:39.396030] Epoch: [0]  [12040/13501]  eta: 0:15:23  lr: 0.000056  closs: 0.3179 (0.6632)  mloss: 0.3179 (0.6632)  time: 0.6314  data: 0.0002  max mem: 29573
[23:55:45.687373] Epoch: [0]  [12050/13501]  eta: 0:15:17  lr: 0.000056  closs: 0.2827 (0.6629)  mloss: 0.2827 (0.6629)  time: 0.6318  data: 0.0002  max mem: 29573
[23:55:52.034246] Epoch: [0]  [12060/13501]  eta: 0:15:11  lr: 0.000056  closs: 0.2817 (0.6626)  mloss: 0.2817 (0.6626)  time: 0.6318  data: 0.0003  max mem: 29573
[23:55:58.320730] Epoch: [0]  [12070/13501]  eta: 0:15:04  lr: 0.000056  closs: 0.3166 (0.6623)  mloss: 0.3166 (0.6623)  time: 0.6316  data: 0.0002  max mem: 29573
[23:56:04.654733] Epoch: [0]  [12080/13501]  eta: 0:14:58  lr: 0.000056  closs: 0.3582 (0.6621)  mloss: 0.3582 (0.6621)  time: 0.6309  data: 0.0002  max mem: 29573
[23:56:10.942684] Epoch: [0]  [12090/13501]  eta: 0:14:52  lr: 0.000056  closs: 0.3100 (0.6618)  mloss: 0.3100 (0.6618)  time: 0.6310  data: 0.0003  max mem: 29573
[23:56:17.319371] Epoch: [0]  [12100/13501]  eta: 0:14:45  lr: 0.000056  closs: 0.3382 (0.6616)  mloss: 0.3382 (0.6616)  time: 0.6332  data: 0.0003  max mem: 29573
[23:56:23.618451] Epoch: [0]  [12110/13501]  eta: 0:14:39  lr: 0.000056  closs: 0.3257 (0.6613)  mloss: 0.3257 (0.6613)  time: 0.6337  data: 0.0002  max mem: 29573
[23:56:29.979854] Epoch: [0]  [12120/13501]  eta: 0:14:33  lr: 0.000056  closs: 0.3257 (0.6611)  mloss: 0.3257 (0.6611)  time: 0.6329  data: 0.0002  max mem: 29573
[23:56:36.270807] Epoch: [0]  [12130/13501]  eta: 0:14:26  lr: 0.000056  closs: 0.3811 (0.6608)  mloss: 0.3811 (0.6608)  time: 0.6325  data: 0.0002  max mem: 29573
[23:56:42.606783] Epoch: [0]  [12140/13501]  eta: 0:14:20  lr: 0.000056  closs: 0.3440 (0.6606)  mloss: 0.3440 (0.6606)  time: 0.6313  data: 0.0002  max mem: 29573
[23:56:48.896699] Epoch: [0]  [12150/13501]  eta: 0:14:14  lr: 0.000056  closs: 0.3228 (0.6603)  mloss: 0.3228 (0.6603)  time: 0.6312  data: 0.0002  max mem: 29573
[23:56:55.233051] Epoch: [0]  [12160/13501]  eta: 0:14:07  lr: 0.000056  closs: 0.3197 (0.6600)  mloss: 0.3197 (0.6600)  time: 0.6312  data: 0.0002  max mem: 29573
[23:57:01.523430] Epoch: [0]  [12170/13501]  eta: 0:14:01  lr: 0.000056  closs: 0.3524 (0.6598)  mloss: 0.3524 (0.6598)  time: 0.6313  data: 0.0002  max mem: 29573
[23:57:07.854198] Epoch: [0]  [12180/13501]  eta: 0:13:55  lr: 0.000056  closs: 0.3345 (0.6595)  mloss: 0.3345 (0.6595)  time: 0.6310  data: 0.0002  max mem: 29573
[23:57:14.141030] Epoch: [0]  [12190/13501]  eta: 0:13:48  lr: 0.000056  closs: 0.3345 (0.6593)  mloss: 0.3345 (0.6593)  time: 0.6308  data: 0.0002  max mem: 29573
[23:57:20.483316] Epoch: [0]  [12200/13501]  eta: 0:13:42  lr: 0.000056  closs: 0.3641 (0.6590)  mloss: 0.3641 (0.6590)  time: 0.6314  data: 0.0002  max mem: 29573
[23:57:26.770409] Epoch: [0]  [12210/13501]  eta: 0:13:36  lr: 0.000057  closs: 0.3641 (0.6588)  mloss: 0.3641 (0.6588)  time: 0.6314  data: 0.0002  max mem: 29573
[23:57:33.121428] Epoch: [0]  [12220/13501]  eta: 0:13:29  lr: 0.000057  closs: 0.3499 (0.6585)  mloss: 0.3499 (0.6585)  time: 0.6318  data: 0.0002  max mem: 29573
[23:57:39.420382] Epoch: [0]  [12230/13501]  eta: 0:13:23  lr: 0.000057  closs: 0.3510 (0.6583)  mloss: 0.3510 (0.6583)  time: 0.6324  data: 0.0002  max mem: 29573
[23:57:45.767542] Epoch: [0]  [12240/13501]  eta: 0:13:17  lr: 0.000057  closs: 0.3246 (0.6580)  mloss: 0.3246 (0.6580)  time: 0.6322  data: 0.0002  max mem: 29573
[23:57:52.119477] Epoch: [0]  [12250/13501]  eta: 0:13:10  lr: 0.000057  closs: 0.3052 (0.6577)  mloss: 0.3052 (0.6577)  time: 0.6349  data: 0.0002  max mem: 29573
[23:57:58.464551] Epoch: [0]  [12260/13501]  eta: 0:13:04  lr: 0.000057  closs: 0.3281 (0.6575)  mloss: 0.3281 (0.6575)  time: 0.6348  data: 0.0002  max mem: 29573
[23:58:04.759575] Epoch: [0]  [12270/13501]  eta: 0:12:58  lr: 0.000057  closs: 0.3250 (0.6572)  mloss: 0.3250 (0.6572)  time: 0.6319  data: 0.0002  max mem: 29573
[23:58:11.098208] Epoch: [0]  [12280/13501]  eta: 0:12:51  lr: 0.000057  closs: 0.3300 (0.6570)  mloss: 0.3300 (0.6570)  time: 0.6316  data: 0.0002  max mem: 29573
[23:58:17.389447] Epoch: [0]  [12290/13501]  eta: 0:12:45  lr: 0.000057  closs: 0.3198 (0.6567)  mloss: 0.3198 (0.6567)  time: 0.6314  data: 0.0002  max mem: 29573
[23:58:23.731453] Epoch: [0]  [12300/13501]  eta: 0:12:39  lr: 0.000057  closs: 0.3324 (0.6565)  mloss: 0.3324 (0.6565)  time: 0.6316  data: 0.0003  max mem: 29573
[23:58:30.022458] Epoch: [0]  [12310/13501]  eta: 0:12:33  lr: 0.000057  closs: 0.3506 (0.6562)  mloss: 0.3506 (0.6562)  time: 0.6316  data: 0.0003  max mem: 29573
[23:58:36.371469] Epoch: [0]  [12320/13501]  eta: 0:12:26  lr: 0.000057  closs: 0.3428 (0.6560)  mloss: 0.3428 (0.6560)  time: 0.6319  data: 0.0002  max mem: 29573
[23:58:42.681619] Epoch: [0]  [12330/13501]  eta: 0:12:20  lr: 0.000057  closs: 0.3428 (0.6557)  mloss: 0.3428 (0.6557)  time: 0.6329  data: 0.0002  max mem: 29573
[23:58:49.030950] Epoch: [0]  [12340/13501]  eta: 0:12:14  lr: 0.000057  closs: 0.3647 (0.6555)  mloss: 0.3647 (0.6555)  time: 0.6329  data: 0.0002  max mem: 29573
[23:58:55.314285] Epoch: [0]  [12350/13501]  eta: 0:12:07  lr: 0.000057  closs: 0.3647 (0.6553)  mloss: 0.3647 (0.6553)  time: 0.6316  data: 0.0002  max mem: 29573
[23:59:01.646900] Epoch: [0]  [12360/13501]  eta: 0:12:01  lr: 0.000057  closs: 0.3239 (0.6550)  mloss: 0.3239 (0.6550)  time: 0.6307  data: 0.0002  max mem: 29573
[23:59:07.944818] Epoch: [0]  [12370/13501]  eta: 0:11:55  lr: 0.000057  closs: 0.3149 (0.6548)  mloss: 0.3149 (0.6548)  time: 0.6315  data: 0.0002  max mem: 29573
[23:59:14.285860] Epoch: [0]  [12380/13501]  eta: 0:11:48  lr: 0.000057  closs: 0.3232 (0.6545)  mloss: 0.3232 (0.6545)  time: 0.6319  data: 0.0002  max mem: 29573
[23:59:20.588200] Epoch: [0]  [12390/13501]  eta: 0:11:42  lr: 0.000057  closs: 0.3176 (0.6543)  mloss: 0.3176 (0.6543)  time: 0.6321  data: 0.0002  max mem: 29573
[23:59:26.934770] Epoch: [0]  [12400/13501]  eta: 0:11:36  lr: 0.000057  closs: 0.3122 (0.6540)  mloss: 0.3122 (0.6540)  time: 0.6324  data: 0.0002  max mem: 29573
[23:59:33.230407] Epoch: [0]  [12410/13501]  eta: 0:11:29  lr: 0.000057  closs: 0.3042 (0.6538)  mloss: 0.3042 (0.6538)  time: 0.6320  data: 0.0002  max mem: 29573
[23:59:39.578844] Epoch: [0]  [12420/13501]  eta: 0:11:23  lr: 0.000057  closs: 0.3046 (0.6535)  mloss: 0.3046 (0.6535)  time: 0.6321  data: 0.0002  max mem: 29573
[23:59:45.884365] Epoch: [0]  [12430/13501]  eta: 0:11:17  lr: 0.000058  closs: 0.3145 (0.6533)  mloss: 0.3145 (0.6533)  time: 0.6326  data: 0.0002  max mem: 29573
[23:59:52.243658] Epoch: [0]  [12440/13501]  eta: 0:11:10  lr: 0.000058  closs: 0.3345 (0.6530)  mloss: 0.3345 (0.6530)  time: 0.6332  data: 0.0002  max mem: 29573
[23:59:58.529311] Epoch: [0]  [12450/13501]  eta: 0:11:04  lr: 0.000058  closs: 0.3345 (0.6528)  mloss: 0.3345 (0.6528)  time: 0.6322  data: 0.0002  max mem: 29573
[00:00:04.865220] Epoch: [0]  [12460/13501]  eta: 0:10:58  lr: 0.000058  closs: 0.3737 (0.6526)  mloss: 0.3737 (0.6526)  time: 0.6310  data: 0.0002  max mem: 29573
[00:00:11.164639] Epoch: [0]  [12470/13501]  eta: 0:10:51  lr: 0.000058  closs: 0.3773 (0.6523)  mloss: 0.3773 (0.6523)  time: 0.6317  data: 0.0002  max mem: 29573
[00:00:17.499800] Epoch: [0]  [12480/13501]  eta: 0:10:45  lr: 0.000058  closs: 0.3310 (0.6521)  mloss: 0.3310 (0.6521)  time: 0.6317  data: 0.0002  max mem: 29573
[00:00:23.791422] Epoch: [0]  [12490/13501]  eta: 0:10:39  lr: 0.000058  closs: 0.3815 (0.6519)  mloss: 0.3815 (0.6519)  time: 0.6313  data: 0.0002  max mem: 29573
[00:00:30.135464] Epoch: [0]  [12500/13501]  eta: 0:10:32  lr: 0.000058  closs: 0.3833 (0.6516)  mloss: 0.3833 (0.6516)  time: 0.6317  data: 0.0002  max mem: 29573
[00:00:36.422101] Epoch: [0]  [12510/13501]  eta: 0:10:26  lr: 0.000058  closs: 0.3494 (0.6514)  mloss: 0.3494 (0.6514)  time: 0.6315  data: 0.0002  max mem: 29573
[00:00:42.762008] Epoch: [0]  [12520/13501]  eta: 0:10:20  lr: 0.000058  closs: 0.3252 (0.6511)  mloss: 0.3252 (0.6511)  time: 0.6312  data: 0.0002  max mem: 29573
[00:00:49.046515] Epoch: [0]  [12530/13501]  eta: 0:10:13  lr: 0.000058  closs: 0.3066 (0.6509)  mloss: 0.3066 (0.6509)  time: 0.6311  data: 0.0002  max mem: 29573
[00:00:55.410729] Epoch: [0]  [12540/13501]  eta: 0:10:07  lr: 0.000058  closs: 0.3334 (0.6506)  mloss: 0.3334 (0.6506)  time: 0.6324  data: 0.0002  max mem: 29573
[00:01:01.707621] Epoch: [0]  [12550/13501]  eta: 0:10:01  lr: 0.000058  closs: 0.3381 (0.6504)  mloss: 0.3381 (0.6504)  time: 0.6330  data: 0.0002  max mem: 29573
[00:01:08.045667] Epoch: [0]  [12560/13501]  eta: 0:09:54  lr: 0.000058  closs: 0.3381 (0.6502)  mloss: 0.3381 (0.6502)  time: 0.6317  data: 0.0002  max mem: 29573
[00:01:14.331586] Epoch: [0]  [12570/13501]  eta: 0:09:48  lr: 0.000058  closs: 0.3246 (0.6499)  mloss: 0.3246 (0.6499)  time: 0.6311  data: 0.0002  max mem: 29573
[00:01:20.678202] Epoch: [0]  [12580/13501]  eta: 0:09:42  lr: 0.000058  closs: 0.3003 (0.6497)  mloss: 0.3003 (0.6497)  time: 0.6316  data: 0.0002  max mem: 29573
[00:01:26.972471] Epoch: [0]  [12590/13501]  eta: 0:09:35  lr: 0.000058  closs: 0.2893 (0.6494)  mloss: 0.2893 (0.6494)  time: 0.6320  data: 0.0002  max mem: 29573
[00:01:33.321231] Epoch: [0]  [12600/13501]  eta: 0:09:29  lr: 0.000058  closs: 0.2973 (0.6492)  mloss: 0.2973 (0.6492)  time: 0.6321  data: 0.0002  max mem: 29573
[00:01:39.612179] Epoch: [0]  [12610/13501]  eta: 0:09:23  lr: 0.000058  closs: 0.3378 (0.6490)  mloss: 0.3378 (0.6490)  time: 0.6319  data: 0.0002  max mem: 29573
[00:01:45.955890] Epoch: [0]  [12620/13501]  eta: 0:09:17  lr: 0.000058  closs: 0.3378 (0.6487)  mloss: 0.3378 (0.6487)  time: 0.6317  data: 0.0002  max mem: 29573
[00:01:52.244729] Epoch: [0]  [12630/13501]  eta: 0:09:10  lr: 0.000058  closs: 0.3443 (0.6485)  mloss: 0.3443 (0.6485)  time: 0.6316  data: 0.0002  max mem: 29573
[00:01:58.590852] Epoch: [0]  [12640/13501]  eta: 0:09:04  lr: 0.000059  closs: 0.3509 (0.6483)  mloss: 0.3509 (0.6483)  time: 0.6317  data: 0.0002  max mem: 29573
[00:02:04.899889] Epoch: [0]  [12650/13501]  eta: 0:08:58  lr: 0.000059  closs: 0.3465 (0.6480)  mloss: 0.3465 (0.6480)  time: 0.6327  data: 0.0002  max mem: 29573
[00:02:11.241773] Epoch: [0]  [12660/13501]  eta: 0:08:51  lr: 0.000059  closs: 0.3241 (0.6478)  mloss: 0.3241 (0.6478)  time: 0.6325  data: 0.0002  max mem: 29573
[00:02:17.525538] Epoch: [0]  [12670/13501]  eta: 0:08:45  lr: 0.000059  closs: 0.3574 (0.6476)  mloss: 0.3574 (0.6476)  time: 0.6312  data: 0.0002  max mem: 29573
[00:02:23.867193] Epoch: [0]  [12680/13501]  eta: 0:08:39  lr: 0.000059  closs: 0.3337 (0.6474)  mloss: 0.3337 (0.6474)  time: 0.6312  data: 0.0002  max mem: 29573
[00:02:30.149522] Epoch: [0]  [12690/13501]  eta: 0:08:32  lr: 0.000059  closs: 0.3100 (0.6471)  mloss: 0.3100 (0.6471)  time: 0.6311  data: 0.0002  max mem: 29573
[00:02:36.491059] Epoch: [0]  [12700/13501]  eta: 0:08:26  lr: 0.000059  closs: 0.3026 (0.6468)  mloss: 0.3026 (0.6468)  time: 0.6311  data: 0.0002  max mem: 29573
[00:02:42.781913] Epoch: [0]  [12710/13501]  eta: 0:08:20  lr: 0.000059  closs: 0.3032 (0.6466)  mloss: 0.3032 (0.6466)  time: 0.6315  data: 0.0002  max mem: 29573
[00:02:49.128044] Epoch: [0]  [12720/13501]  eta: 0:08:13  lr: 0.000059  closs: 0.3227 (0.6463)  mloss: 0.3227 (0.6463)  time: 0.6318  data: 0.0003  max mem: 29573
[00:02:55.412787] Epoch: [0]  [12730/13501]  eta: 0:08:07  lr: 0.000059  closs: 0.3576 (0.6461)  mloss: 0.3576 (0.6461)  time: 0.6315  data: 0.0003  max mem: 29573
[00:03:01.760963] Epoch: [0]  [12740/13501]  eta: 0:08:01  lr: 0.000059  closs: 0.3621 (0.6459)  mloss: 0.3621 (0.6459)  time: 0.6316  data: 0.0002  max mem: 29573
[00:03:08.063083] Epoch: [0]  [12750/13501]  eta: 0:07:54  lr: 0.000059  closs: 0.3549 (0.6456)  mloss: 0.3549 (0.6456)  time: 0.6324  data: 0.0002  max mem: 29573
[00:03:14.417533] Epoch: [0]  [12760/13501]  eta: 0:07:48  lr: 0.000059  closs: 0.3086 (0.6454)  mloss: 0.3086 (0.6454)  time: 0.6328  data: 0.0002  max mem: 29573
[00:03:20.720196] Epoch: [0]  [12770/13501]  eta: 0:07:42  lr: 0.000059  closs: 0.3195 (0.6452)  mloss: 0.3195 (0.6452)  time: 0.6328  data: 0.0003  max mem: 29573
[00:03:27.061787] Epoch: [0]  [12780/13501]  eta: 0:07:35  lr: 0.000059  closs: 0.3302 (0.6449)  mloss: 0.3302 (0.6449)  time: 0.6321  data: 0.0003  max mem: 29573
[00:03:33.352245] Epoch: [0]  [12790/13501]  eta: 0:07:29  lr: 0.000059  closs: 0.3375 (0.6447)  mloss: 0.3375 (0.6447)  time: 0.6315  data: 0.0002  max mem: 29573
[00:03:39.694337] Epoch: [0]  [12800/13501]  eta: 0:07:23  lr: 0.000059  closs: 0.3409 (0.6445)  mloss: 0.3409 (0.6445)  time: 0.6315  data: 0.0002  max mem: 29573
[00:03:45.990225] Epoch: [0]  [12810/13501]  eta: 0:07:16  lr: 0.000059  closs: 0.3179 (0.6443)  mloss: 0.3179 (0.6443)  time: 0.6318  data: 0.0002  max mem: 29573
[00:03:52.327562] Epoch: [0]  [12820/13501]  eta: 0:07:10  lr: 0.000059  closs: 0.3215 (0.6441)  mloss: 0.3215 (0.6441)  time: 0.6316  data: 0.0002  max mem: 29573
[00:03:58.616773] Epoch: [0]  [12830/13501]  eta: 0:07:04  lr: 0.000059  closs: 0.3141 (0.6438)  mloss: 0.3141 (0.6438)  time: 0.6312  data: 0.0002  max mem: 29573
[00:04:04.964801] Epoch: [0]  [12840/13501]  eta: 0:06:57  lr: 0.000059  closs: 0.2803 (0.6436)  mloss: 0.2803 (0.6436)  time: 0.6318  data: 0.0002  max mem: 29573
[00:04:11.249865] Epoch: [0]  [12850/13501]  eta: 0:06:51  lr: 0.000059  closs: 0.3032 (0.6433)  mloss: 0.3032 (0.6433)  time: 0.6316  data: 0.0002  max mem: 29573
[00:04:17.605203] Epoch: [0]  [12860/13501]  eta: 0:06:45  lr: 0.000060  closs: 0.3227 (0.6431)  mloss: 0.3227 (0.6431)  time: 0.6319  data: 0.0002  max mem: 29573
[00:04:23.895534] Epoch: [0]  [12870/13501]  eta: 0:06:38  lr: 0.000060  closs: 0.2957 (0.6428)  mloss: 0.2957 (0.6428)  time: 0.6322  data: 0.0003  max mem: 29573
[00:04:30.241978] Epoch: [0]  [12880/13501]  eta: 0:06:32  lr: 0.000060  closs: 0.3220 (0.6426)  mloss: 0.3220 (0.6426)  time: 0.6317  data: 0.0003  max mem: 29573
[00:04:36.534748] Epoch: [0]  [12890/13501]  eta: 0:06:26  lr: 0.000060  closs: 0.3220 (0.6423)  mloss: 0.3220 (0.6423)  time: 0.6319  data: 0.0002  max mem: 29573
[00:04:42.891393] Epoch: [0]  [12900/13501]  eta: 0:06:19  lr: 0.000060  closs: 0.3451 (0.6421)  mloss: 0.3451 (0.6421)  time: 0.6324  data: 0.0002  max mem: 29573
[00:04:49.178427] Epoch: [0]  [12910/13501]  eta: 0:06:13  lr: 0.000060  closs: 0.3415 (0.6419)  mloss: 0.3415 (0.6419)  time: 0.6321  data: 0.0002  max mem: 29573
[00:04:55.513495] Epoch: [0]  [12920/13501]  eta: 0:06:07  lr: 0.000060  closs: 0.3378 (0.6417)  mloss: 0.3378 (0.6417)  time: 0.6310  data: 0.0002  max mem: 29573
[00:05:01.802941] Epoch: [0]  [12930/13501]  eta: 0:06:01  lr: 0.000060  closs: 0.3128 (0.6414)  mloss: 0.3128 (0.6414)  time: 0.6312  data: 0.0002  max mem: 29573
[00:05:08.148132] Epoch: [0]  [12940/13501]  eta: 0:05:54  lr: 0.000060  closs: 0.3213 (0.6412)  mloss: 0.3213 (0.6412)  time: 0.6317  data: 0.0002  max mem: 29573
[00:05:14.441993] Epoch: [0]  [12950/13501]  eta: 0:05:48  lr: 0.000060  closs: 0.3166 (0.6410)  mloss: 0.3166 (0.6410)  time: 0.6319  data: 0.0002  max mem: 29573
[00:05:20.794277] Epoch: [0]  [12960/13501]  eta: 0:05:42  lr: 0.000060  closs: 0.3202 (0.6407)  mloss: 0.3202 (0.6407)  time: 0.6322  data: 0.0002  max mem: 29573
[00:05:27.106756] Epoch: [0]  [12970/13501]  eta: 0:05:35  lr: 0.000060  closs: 0.3242 (0.6405)  mloss: 0.3242 (0.6405)  time: 0.6332  data: 0.0002  max mem: 29573
[00:05:33.451875] Epoch: [0]  [12980/13501]  eta: 0:05:29  lr: 0.000060  closs: 0.3243 (0.6403)  mloss: 0.3243 (0.6403)  time: 0.6328  data: 0.0002  max mem: 29573
[00:05:39.735267] Epoch: [0]  [12990/13501]  eta: 0:05:23  lr: 0.000060  closs: 0.3627 (0.6401)  mloss: 0.3627 (0.6401)  time: 0.6313  data: 0.0002  max mem: 29573
[00:05:46.083399] Epoch: [0]  [13000/13501]  eta: 0:05:16  lr: 0.000060  closs: 0.3366 (0.6398)  mloss: 0.3366 (0.6398)  time: 0.6315  data: 0.0002  max mem: 29573
[00:05:52.380018] Epoch: [0]  [13010/13501]  eta: 0:05:10  lr: 0.000060  closs: 0.3430 (0.6396)  mloss: 0.3430 (0.6396)  time: 0.6322  data: 0.0002  max mem: 29573
[00:05:58.719039] Epoch: [0]  [13020/13501]  eta: 0:05:04  lr: 0.000060  closs: 0.3721 (0.6394)  mloss: 0.3721 (0.6394)  time: 0.6317  data: 0.0002  max mem: 29573
[00:06:05.003435] Epoch: [0]  [13030/13501]  eta: 0:04:57  lr: 0.000060  closs: 0.3458 (0.6392)  mloss: 0.3458 (0.6392)  time: 0.6311  data: 0.0003  max mem: 29573
[00:06:11.340951] Epoch: [0]  [13040/13501]  eta: 0:04:51  lr: 0.000060  closs: 0.3118 (0.6390)  mloss: 0.3118 (0.6390)  time: 0.6310  data: 0.0003  max mem: 29573
[00:06:17.635452] Epoch: [0]  [13050/13501]  eta: 0:04:45  lr: 0.000060  closs: 0.3287 (0.6387)  mloss: 0.3287 (0.6387)  time: 0.6315  data: 0.0002  max mem: 29573
[00:06:23.972143] Epoch: [0]  [13060/13501]  eta: 0:04:38  lr: 0.000060  closs: 0.3263 (0.6385)  mloss: 0.3263 (0.6385)  time: 0.6315  data: 0.0002  max mem: 29573
[00:06:30.267385] Epoch: [0]  [13070/13501]  eta: 0:04:32  lr: 0.000060  closs: 0.3092 (0.6383)  mloss: 0.3092 (0.6383)  time: 0.6315  data: 0.0002  max mem: 29573
[00:06:36.626760] Epoch: [0]  [13080/13501]  eta: 0:04:26  lr: 0.000061  closs: 0.3594 (0.6381)  mloss: 0.3594 (0.6381)  time: 0.6327  data: 0.0002  max mem: 29573
[00:06:42.921353] Epoch: [0]  [13090/13501]  eta: 0:04:19  lr: 0.000061  closs: 0.3410 (0.6379)  mloss: 0.3410 (0.6379)  time: 0.6326  data: 0.0002  max mem: 29573
[00:06:49.258446] Epoch: [0]  [13100/13501]  eta: 0:04:13  lr: 0.000061  closs: 0.3280 (0.6376)  mloss: 0.3280 (0.6376)  time: 0.6315  data: 0.0002  max mem: 29573
[00:06:55.549473] Epoch: [0]  [13110/13501]  eta: 0:04:07  lr: 0.000061  closs: 0.3599 (0.6374)  mloss: 0.3599 (0.6374)  time: 0.6313  data: 0.0002  max mem: 29573
[00:07:01.891940] Epoch: [0]  [13120/13501]  eta: 0:04:00  lr: 0.000061  closs: 0.3579 (0.6372)  mloss: 0.3579 (0.6372)  time: 0.6316  data: 0.0002  max mem: 29573
[00:07:08.184777] Epoch: [0]  [13130/13501]  eta: 0:03:54  lr: 0.000061  closs: 0.3579 (0.6370)  mloss: 0.3579 (0.6370)  time: 0.6317  data: 0.0002  max mem: 29573
[00:07:14.531810] Epoch: [0]  [13140/13501]  eta: 0:03:48  lr: 0.000061  closs: 0.3009 (0.6367)  mloss: 0.3009 (0.6367)  time: 0.6319  data: 0.0002  max mem: 29573
[00:07:20.821174] Epoch: [0]  [13150/13501]  eta: 0:03:41  lr: 0.000061  closs: 0.3032 (0.6365)  mloss: 0.3032 (0.6365)  time: 0.6317  data: 0.0002  max mem: 29573
[00:07:27.166036] Epoch: [0]  [13160/13501]  eta: 0:03:35  lr: 0.000061  closs: 0.3339 (0.6363)  mloss: 0.3339 (0.6363)  time: 0.6316  data: 0.0002  max mem: 29573
[00:07:33.448724] Epoch: [0]  [13170/13501]  eta: 0:03:29  lr: 0.000061  closs: 0.3173 (0.6360)  mloss: 0.3173 (0.6360)  time: 0.6313  data: 0.0002  max mem: 29573
[00:07:39.815349] Epoch: [0]  [13180/13501]  eta: 0:03:22  lr: 0.000061  closs: 0.3173 (0.6358)  mloss: 0.3173 (0.6358)  time: 0.6324  data: 0.0002  max mem: 29573
[00:07:46.116193] Epoch: [0]  [13190/13501]  eta: 0:03:16  lr: 0.000061  closs: 0.3195 (0.6356)  mloss: 0.3195 (0.6356)  time: 0.6333  data: 0.0002  max mem: 29573
[00:07:52.459558] Epoch: [0]  [13200/13501]  eta: 0:03:10  lr: 0.000061  closs: 0.3200 (0.6353)  mloss: 0.3200 (0.6353)  time: 0.6321  data: 0.0002  max mem: 29573
[00:07:58.746204] Epoch: [0]  [13210/13501]  eta: 0:03:03  lr: 0.000061  closs: 0.3011 (0.6351)  mloss: 0.3011 (0.6351)  time: 0.6314  data: 0.0002  max mem: 29573
[00:08:05.087879] Epoch: [0]  [13220/13501]  eta: 0:02:57  lr: 0.000061  closs: 0.3000 (0.6349)  mloss: 0.3000 (0.6349)  time: 0.6313  data: 0.0002  max mem: 29573
[00:08:11.375599] Epoch: [0]  [13230/13501]  eta: 0:02:51  lr: 0.000061  closs: 0.3722 (0.6347)  mloss: 0.3722 (0.6347)  time: 0.6314  data: 0.0002  max mem: 29573
[00:08:17.730553] Epoch: [0]  [13240/13501]  eta: 0:02:45  lr: 0.000061  closs: 0.3651 (0.6345)  mloss: 0.3651 (0.6345)  time: 0.6321  data: 0.0002  max mem: 29573
[00:08:24.017974] Epoch: [0]  [13250/13501]  eta: 0:02:38  lr: 0.000061  closs: 0.2817 (0.6342)  mloss: 0.2817 (0.6342)  time: 0.6320  data: 0.0002  max mem: 29573
[00:08:30.360410] Epoch: [0]  [13260/13501]  eta: 0:02:32  lr: 0.000061  closs: 0.3203 (0.6340)  mloss: 0.3203 (0.6340)  time: 0.6314  data: 0.0002  max mem: 29573
[00:08:36.640472] Epoch: [0]  [13270/13501]  eta: 0:02:26  lr: 0.000061  closs: 0.3431 (0.6338)  mloss: 0.3431 (0.6338)  time: 0.6310  data: 0.0002  max mem: 29573
[00:08:42.991291] Epoch: [0]  [13280/13501]  eta: 0:02:19  lr: 0.000061  closs: 0.3729 (0.6336)  mloss: 0.3729 (0.6336)  time: 0.6315  data: 0.0002  max mem: 29573
[00:08:49.297878] Epoch: [0]  [13290/13501]  eta: 0:02:13  lr: 0.000062  closs: 0.3853 (0.6334)  mloss: 0.3853 (0.6334)  time: 0.6328  data: 0.0002  max mem: 29573
[00:08:55.668050] Epoch: [0]  [13300/13501]  eta: 0:02:07  lr: 0.000062  closs: 0.3296 (0.6332)  mloss: 0.3296 (0.6332)  time: 0.6338  data: 0.0002  max mem: 29573
[00:09:01.954310] Epoch: [0]  [13310/13501]  eta: 0:02:00  lr: 0.000062  closs: 0.3136 (0.6330)  mloss: 0.3136 (0.6330)  time: 0.6327  data: 0.0002  max mem: 29573
[00:09:08.296326] Epoch: [0]  [13320/13501]  eta: 0:01:54  lr: 0.000062  closs: 0.3136 (0.6327)  mloss: 0.3136 (0.6327)  time: 0.6313  data: 0.0002  max mem: 29573
[00:09:14.597420] Epoch: [0]  [13330/13501]  eta: 0:01:48  lr: 0.000062  closs: 0.3203 (0.6325)  mloss: 0.3203 (0.6325)  time: 0.6321  data: 0.0002  max mem: 29573
[00:09:20.936499] Epoch: [0]  [13340/13501]  eta: 0:01:41  lr: 0.000062  closs: 0.3581 (0.6323)  mloss: 0.3581 (0.6323)  time: 0.6319  data: 0.0002  max mem: 29573
[00:09:27.225635] Epoch: [0]  [13350/13501]  eta: 0:01:35  lr: 0.000062  closs: 0.3330 (0.6321)  mloss: 0.3330 (0.6321)  time: 0.6313  data: 0.0002  max mem: 29573
[00:09:33.562431] Epoch: [0]  [13360/13501]  eta: 0:01:29  lr: 0.000062  closs: 0.3354 (0.6318)  mloss: 0.3354 (0.6318)  time: 0.6312  data: 0.0002  max mem: 29573
[00:09:39.851367] Epoch: [0]  [13370/13501]  eta: 0:01:22  lr: 0.000062  closs: 0.3356 (0.6316)  mloss: 0.3356 (0.6316)  time: 0.6312  data: 0.0002  max mem: 29573
[00:09:46.193152] Epoch: [0]  [13380/13501]  eta: 0:01:16  lr: 0.000062  closs: 0.2998 (0.6314)  mloss: 0.2998 (0.6314)  time: 0.6315  data: 0.0002  max mem: 29573
[00:09:52.518381] Epoch: [0]  [13390/13501]  eta: 0:01:10  lr: 0.000062  closs: 0.3157 (0.6312)  mloss: 0.3157 (0.6312)  time: 0.6333  data: 0.0008  max mem: 29573
[00:09:58.913100] Epoch: [0]  [13400/13501]  eta: 0:01:03  lr: 0.000062  closs: 0.3465 (0.6310)  mloss: 0.3465 (0.6310)  time: 0.6359  data: 0.0009  max mem: 29573
[00:10:05.226009] Epoch: [0]  [13410/13501]  eta: 0:00:57  lr: 0.000062  closs: 0.3555 (0.6308)  mloss: 0.3555 (0.6308)  time: 0.6353  data: 0.0010  max mem: 29573
[00:10:11.571936] Epoch: [0]  [13420/13501]  eta: 0:00:51  lr: 0.000062  closs: 0.3378 (0.6306)  mloss: 0.3378 (0.6306)  time: 0.6329  data: 0.0009  max mem: 29573
[00:10:17.871315] Epoch: [0]  [13430/13501]  eta: 0:00:44  lr: 0.000062  closs: 0.3055 (0.6304)  mloss: 0.3055 (0.6304)  time: 0.6322  data: 0.0004  max mem: 29573
[00:10:24.214321] Epoch: [0]  [13440/13501]  eta: 0:00:38  lr: 0.000062  closs: 0.3097 (0.6302)  mloss: 0.3097 (0.6302)  time: 0.6320  data: 0.0003  max mem: 29573
[00:10:30.496835] Epoch: [0]  [13450/13501]  eta: 0:00:32  lr: 0.000062  closs: 0.3466 (0.6300)  mloss: 0.3466 (0.6300)  time: 0.6312  data: 0.0002  max mem: 29573
[00:10:36.864513] Epoch: [0]  [13460/13501]  eta: 0:00:25  lr: 0.000062  closs: 0.3775 (0.6298)  mloss: 0.3775 (0.6298)  time: 0.6324  data: 0.0002  max mem: 29573
[00:10:43.190964] Epoch: [0]  [13470/13501]  eta: 0:00:19  lr: 0.000062  closs: 0.3982 (0.6296)  mloss: 0.3982 (0.6296)  time: 0.6346  data: 0.0005  max mem: 29573
[00:10:49.555380] Epoch: [0]  [13480/13501]  eta: 0:00:13  lr: 0.000062  closs: 0.3898 (0.6294)  mloss: 0.3898 (0.6294)  time: 0.6345  data: 0.0008  max mem: 29573
[00:10:55.860732] Epoch: [0]  [13490/13501]  eta: 0:00:06  lr: 0.000062  closs: 0.3961 (0.6293)  mloss: 0.3961 (0.6293)  time: 0.6334  data: 0.0006  max mem: 29573
[00:11:02.210369] Epoch: [0]  [13500/13501]  eta: 0:00:00  lr: 0.000062  closs: 0.3865 (0.6291)  mloss: 0.3865 (0.6291)  time: 0.6327  data: 0.0002  max mem: 29573
[00:11:02.407246] Epoch: [0] Total time: 2:22:17 (0.6323 s / it)
[00:11:02.407957] Averaged stats: lr: 0.000062  closs: 0.3865 (0.6291)  mloss: 0.3865 (0.6291)
[00:11:03.143434] log_dir: ./output
[00:11:04.674357] Epoch: [1]  [    0/13501]  eta: 5:44:12  lr: 0.000063  closs: 0.3716 (0.3716)  mloss: 0.3716 (0.3716)  time: 1.5297  data: 0.8882  max mem: 29573
[00:11:10.958865] Epoch: [1]  [   10/13501]  eta: 2:39:42  lr: 0.000062  closs: 0.3286 (0.3259)  mloss: 0.3286 (0.3259)  time: 0.7103  data: 0.0809  max mem: 29573
[00:11:17.307860] Epoch: [1]  [   20/13501]  eta: 2:31:31  lr: 0.000062  closs: 0.3286 (0.3448)  mloss: 0.3286 (0.3448)  time: 0.6316  data: 0.0002  max mem: 29573
[00:11:23.587645] Epoch: [1]  [   30/13501]  eta: 2:28:02  lr: 0.000062  closs: 0.3527 (0.3503)  mloss: 0.3527 (0.3503)  time: 0.6314  data: 0.0002  max mem: 29573
[00:11:29.921262] Epoch: [1]  [   40/13501]  eta: 2:26:30  lr: 0.000062  closs: 0.3518 (0.3571)  mloss: 0.3518 (0.3571)  time: 0.6306  data: 0.0002  max mem: 29573
[00:11:36.207165] Epoch: [1]  [   50/13501]  eta: 2:25:19  lr: 0.000062  closs: 0.3337 (0.3500)  mloss: 0.3337 (0.3500)  time: 0.6309  data: 0.0002  max mem: 29573
[00:11:42.548746] Epoch: [1]  [   60/13501]  eta: 2:24:41  lr: 0.000062  closs: 0.3292 (0.3459)  mloss: 0.3292 (0.3459)  time: 0.6313  data: 0.0002  max mem: 29573
[00:11:48.836390] Epoch: [1]  [   70/13501]  eta: 2:24:02  lr: 0.000062  closs: 0.2986 (0.3413)  mloss: 0.2986 (0.3413)  time: 0.6314  data: 0.0002  max mem: 29573
[00:11:55.175649] Epoch: [1]  [   80/13501]  eta: 2:23:40  lr: 0.000062  closs: 0.3133 (0.3457)  mloss: 0.3133 (0.3457)  time: 0.6313  data: 0.0002  max mem: 29573
[00:12:01.462692] Epoch: [1]  [   90/13501]  eta: 2:23:13  lr: 0.000062  closs: 0.3461 (0.3449)  mloss: 0.3461 (0.3449)  time: 0.6312  data: 0.0002  max mem: 29573
[00:12:07.825406] Epoch: [1]  [  100/13501]  eta: 2:23:01  lr: 0.000062  closs: 0.3461 (0.3444)  mloss: 0.3461 (0.3444)  time: 0.6324  data: 0.0002  max mem: 29573
[00:12:09.835957] /work/u8915687/big-superb/big-superb-train-data/SpeechDetection_Aishell1Train/train/BAC009S0118W0478.wav
[00:12:14.116834] Epoch: [1]  [  110/13501]  eta: 2:22:41  lr: 0.000062  closs: 0.3480 (0.3455)  mloss: 0.3480 (0.3455)  time: 0.6326  data: 0.0002  max mem: 29573
[00:12:20.464541] Epoch: [1]  [  120/13501]  eta: 2:22:29  lr: 0.000062  closs: 0.3313 (0.3432)  mloss: 0.3313 (0.3432)  time: 0.6319  data: 0.0002  max mem: 29573
[00:12:26.750608] Epoch: [1]  [  130/13501]  eta: 2:22:12  lr: 0.000062  closs: 0.3195 (0.3432)  mloss: 0.3195 (0.3432)  time: 0.6316  data: 0.0002  max mem: 29573
[00:12:33.087219] Epoch: [1]  [  140/13501]  eta: 2:22:01  lr: 0.000062  closs: 0.3195 (0.3415)  mloss: 0.3195 (0.3415)  time: 0.6311  data: 0.0002  max mem: 29573
[00:12:39.374977] Epoch: [1]  [  150/13501]  eta: 2:21:47  lr: 0.000062  closs: 0.3183 (0.3420)  mloss: 0.3183 (0.3420)  time: 0.6311  data: 0.0002  max mem: 29573
[00:12:45.711765] Epoch: [1]  [  160/13501]  eta: 2:21:38  lr: 0.000062  closs: 0.2925 (0.3401)  mloss: 0.2925 (0.3401)  time: 0.6312  data: 0.0002  max mem: 29573
[00:12:52.003511] Epoch: [1]  [  170/13501]  eta: 2:21:25  lr: 0.000062  closs: 0.3142 (0.3408)  mloss: 0.3142 (0.3408)  time: 0.6314  data: 0.0002  max mem: 29573
[00:12:58.342362] Epoch: [1]  [  180/13501]  eta: 2:21:17  lr: 0.000062  closs: 0.3601 (0.3434)  mloss: 0.3601 (0.3434)  time: 0.6315  data: 0.0002  max mem: 29573
[00:13:04.634433] Epoch: [1]  [  190/13501]  eta: 2:21:05  lr: 0.000062  closs: 0.3685 (0.3437)  mloss: 0.3685 (0.3437)  time: 0.6315  data: 0.0002  max mem: 29573
[00:13:10.980384] Epoch: [1]  [  200/13501]  eta: 2:20:58  lr: 0.000062  closs: 0.3246 (0.3422)  mloss: 0.3246 (0.3422)  time: 0.6318  data: 0.0002  max mem: 29573
[00:13:17.298526] Epoch: [1]  [  210/13501]  eta: 2:20:49  lr: 0.000062  closs: 0.3281 (0.3434)  mloss: 0.3281 (0.3434)  time: 0.6331  data: 0.0002  max mem: 29573
[00:13:23.635853] Epoch: [1]  [  220/13501]  eta: 2:20:41  lr: 0.000062  closs: 0.3835 (0.3455)  mloss: 0.3835 (0.3455)  time: 0.6327  data: 0.0002  max mem: 29573
[00:13:29.920131] Epoch: [1]  [  230/13501]  eta: 2:20:31  lr: 0.000062  closs: 0.3215 (0.3428)  mloss: 0.3215 (0.3428)  time: 0.6310  data: 0.0002  max mem: 29573
[00:13:36.261772] Epoch: [1]  [  240/13501]  eta: 2:20:24  lr: 0.000062  closs: 0.2865 (0.3418)  mloss: 0.2865 (0.3418)  time: 0.6312  data: 0.0002  max mem: 29573
[00:13:42.548562] Epoch: [1]  [  250/13501]  eta: 2:20:14  lr: 0.000062  closs: 0.3232 (0.3424)  mloss: 0.3232 (0.3424)  time: 0.6313  data: 0.0002  max mem: 29573
[00:13:48.888057] Epoch: [1]  [  260/13501]  eta: 2:20:07  lr: 0.000062  closs: 0.3053 (0.3399)  mloss: 0.3053 (0.3399)  time: 0.6312  data: 0.0002  max mem: 29573
WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 11058 closing signal SIGINT
Traceback (most recent call last):
  File "/home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM/bigsuperb_finetune2.py", line 231, in <module>
    main(args)
  File "/home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM/bigsuperb_finetune2.py", line 199, in main
    train_stats = train_one_epoch(
  File "/home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM/engine_finetune.py", line 44, in train_one_epoch
    c_loss, m_loss = model(examples, labels, imgs)
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM/llama/llama_adapter.py", line 206, in forward
    h = layer(h, 0, freqs_cis, mask, visual_proj + prefix_query[prefix_index])
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM/llama/llama.py", line 273, in forward
    h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask, prompt)
  File "/home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM/llama/llama.py", line 191, in forward
    output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 237, in launch_agent
    result = agent.run()
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 11009 got signal: 2
