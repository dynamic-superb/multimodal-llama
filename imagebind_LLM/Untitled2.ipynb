{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56ff67f",
   "metadata": {},
   "source": [
    "# Inference LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a1f31a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]33m\u001b[33m\n",
      "Get:5 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [848 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [108 kB]m\u001b[33m\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB][33m\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [823 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [977 kB]m\u001b[33m\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB][0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]33m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1092 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [865 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1231 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [25.6 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [49.4 kB]\n",
      "Fetched 26.3 MB in 5s (5037 kB/s)33m                         \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "85 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  fontconfig fontconfig-config fonts-dejavu-core i965-va-driver\n",
      "  intel-media-va-driver libaacs0 libaom3 libass9 libasyncns0 libavc1394-0\n",
      "  libavcodec58 libavdevice58 libavfilter7 libavformat58 libavutil56 libbdplus0\n",
      "  libbluray2 libbs2b0 libcaca0 libcairo-gobject2 libcairo2 libcdio-cdda2\n",
      "  libcdio-paranoia2 libcdio19 libchromaprint1 libcodec2-1.0 libdatrie1\n",
      "  libdav1d5 libdc1394-25 libdecor-0-0 libdecor-0-plugin-1-cairo libdeflate0\n",
      "  libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2 libdrm-radeon1\n",
      "  libdrm2 libelf1 libflite1 libfontconfig1 libfreetype6 libfribidi0 libgbm1\n",
      "  libgdk-pixbuf-2.0-0 libgdk-pixbuf2.0-bin libgdk-pixbuf2.0-common libgl1\n",
      "  libgl1-amber-dri libgl1-mesa-dri libglapi-mesa libglvnd0 libglx-mesa0\n",
      "  libglx0 libgme0 libgraphite2-3 libharfbuzz0b libicu70 libiec61883-0\n",
      "  libigdgmm12 libjack-jackd2-0 libjbig0 liblilv-0-0 libllvm15 libmfx1\n",
      "  libmp3lame0 libmpg123-0 libmysofa1 libnorm1 libopenal-data libopenal1\n",
      "  libopenjp2-7 libopenmpt0 libpango-1.0-0 libpangocairo-1.0-0\n",
      "  libpangoft2-1.0-0 libpciaccess0 libpgm-5.3-0 libpixman-1-0 libpocketsphinx3\n",
      "  libpostproc55 libpulse0 librabbitmq4 libraw1394-11 librsvg2-2\n",
      "  librsvg2-common librubberband2 libsamplerate0 libsdl2-2.0-0\n",
      "  libsensors-config libsensors5 libserd-0-0 libshine3 libslang2 libsndio7.0\n",
      "  libsord-0-0 libsoxr0 libspeex1 libsphinxbase3 libsratom-0-0 libsrt1.4-gnutls\n",
      "  libssh-gcrypt-4 libswresample3 libswscale5 libthai-data libthai0 libtheora0\n",
      "  libtiff5 libtwolame0 libudfread0 libusb-1.0-0 libva-drm2 libva-x11-2 libva2\n",
      "  libvdpau1 libvidstab1.1 libvpx7 libwayland-client0 libwayland-cursor0\n",
      "  libwayland-egl1 libwayland-server0 libwebp7 libwebpmux3 libx11-xcb1\n",
      "  libx264-163 libx265-199 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0\n",
      "  libxcb-present0 libxcb-render0 libxcb-shape0 libxcb-shm0 libxcb-sync1\n",
      "  libxcb-xfixes0 libxcursor1 libxfixes3 libxi6 libxinerama1 libxkbcommon0\n",
      "  libxml2 libxrandr2 libxrender1 libxshmfence1 libxss1 libxv1 libxvidcore4\n",
      "  libxxf86vm1 libzimg2 libzmq5 libzvbi-common libzvbi0 mesa-va-drivers\n",
      "  mesa-vdpau-drivers ocl-icd-libopencl1 pocketsphinx-en-us shared-mime-info\n",
      "  va-driver-all vdpau-driver-all x11-common xkb-data\n",
      "Suggested packages:\n",
      "  ffmpeg-doc i965-va-driver-shaders libcuda1 libnvcuvid1 libnvidia-encode1\n",
      "  libbluray-bdj jackd2 libportaudio2 pciutils pulseaudio libraw1394-doc\n",
      "  librsvg2-bin xdg-utils lm-sensors serdi sndiod sordi speex opencl-icd\n",
      "  libvdpau-va-gl1\n",
      "The following NEW packages will be installed:\n",
      "  ffmpeg fontconfig fontconfig-config fonts-dejavu-core i965-va-driver\n",
      "  intel-media-va-driver libaacs0 libaom3 libass9 libasyncns0 libavc1394-0\n",
      "  libavcodec58 libavdevice58 libavfilter7 libavformat58 libavutil56 libbdplus0\n",
      "  libbluray2 libbs2b0 libcaca0 libcairo-gobject2 libcairo2 libcdio-cdda2\n",
      "  libcdio-paranoia2 libcdio19 libchromaprint1 libcodec2-1.0 libdatrie1\n",
      "  libdav1d5 libdc1394-25 libdecor-0-0 libdecor-0-plugin-1-cairo libdeflate0\n",
      "  libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2 libdrm-radeon1\n",
      "  libdrm2 libelf1 libflite1 libfontconfig1 libfreetype6 libfribidi0 libgbm1\n",
      "  libgdk-pixbuf-2.0-0 libgdk-pixbuf2.0-bin libgdk-pixbuf2.0-common libgl1\n",
      "  libgl1-amber-dri libgl1-mesa-dri libglapi-mesa libglvnd0 libglx-mesa0\n",
      "  libglx0 libgme0 libgraphite2-3 libharfbuzz0b libicu70 libiec61883-0\n",
      "  libigdgmm12 libjack-jackd2-0 libjbig0 liblilv-0-0 libllvm15 libmfx1\n",
      "  libmp3lame0 libmpg123-0 libmysofa1 libnorm1 libopenal-data libopenal1\n",
      "  libopenjp2-7 libopenmpt0 libpango-1.0-0 libpangocairo-1.0-0\n",
      "  libpangoft2-1.0-0 libpciaccess0 libpgm-5.3-0 libpixman-1-0 libpocketsphinx3\n",
      "  libpostproc55 libpulse0 librabbitmq4 libraw1394-11 librsvg2-2\n",
      "  librsvg2-common librubberband2 libsamplerate0 libsdl2-2.0-0\n",
      "  libsensors-config libsensors5 libserd-0-0 libshine3 libslang2 libsndio7.0\n",
      "  libsord-0-0 libsoxr0 libspeex1 libsphinxbase3 libsratom-0-0 libsrt1.4-gnutls\n",
      "  libssh-gcrypt-4 libswresample3 libswscale5 libthai-data libthai0 libtheora0\n",
      "  libtiff5 libtwolame0 libudfread0 libusb-1.0-0 libva-drm2 libva-x11-2 libva2\n",
      "  libvdpau1 libvidstab1.1 libvpx7 libwayland-client0 libwayland-cursor0\n",
      "  libwayland-egl1 libwayland-server0 libwebp7 libwebpmux3 libx11-xcb1\n",
      "  libx264-163 libx265-199 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0\n",
      "  libxcb-present0 libxcb-render0 libxcb-shape0 libxcb-shm0 libxcb-sync1\n",
      "  libxcb-xfixes0 libxcursor1 libxfixes3 libxi6 libxinerama1 libxkbcommon0\n",
      "  libxml2 libxrandr2 libxrender1 libxshmfence1 libxss1 libxv1 libxvidcore4\n",
      "  libxxf86vm1 libzimg2 libzmq5 libzvbi-common libzvbi0 mesa-va-drivers\n",
      "  mesa-vdpau-drivers ocl-icd-libopencl1 pocketsphinx-en-us shared-mime-info\n",
      "  va-driver-all vdpau-driver-all x11-common xkb-data\n",
      "0 upgraded, 162 newly installed, 0 to remove and 85 not upgraded.\n",
      "Need to get 146 MB of archives.\n",
      "After this operation, 454 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libelf1 amd64 0.186-1build1 [51.0 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libfribidi0 amd64 1.0.8-2ubuntu3.1 [26.1 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libicu70 amd64 70.1-2 [10.6 MB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libslang2 amd64 2.3.2-5build4 [468 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libxml2 amd64 2.9.13+dfsg-1ubuntu0.3 [763 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 shared-mime-info amd64 2.1-2 [454 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xkb-data all 2.33-1 [394 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-common all 2.4.113-2~ubuntu0.22.04.1 [5450 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm2 amd64 2.4.113-2~ubuntu0.22.04.1 [38.1 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libusb-1.0-0 amd64 2:1.0.25-1ubuntu2 [52.7 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaom3 amd64 3.3.0-1 [1748 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libva2 amd64 2.14.0-1 [65.0 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmfx1 amd64 22.3.0-1 [3105 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libva-drm2 amd64 2.14.0-1 [7502 B]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfixes3 amd64 1:6.0.0-1 [11.7 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libva-x11-2 amd64 2.14.0-1 [12.6 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvdpau1 amd64 1.4-3build2 [27.0 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ocl-icd-libopencl1 amd64 2.2.14-3 [39.1 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavutil56 amd64 7:4.4.2-0ubuntu0.22.04.1 [290 kB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libfreetype6 amd64 2.11.1+dfsg-1ubuntu0.2 [389 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1041 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 fontconfig-config all 2.13.1-4.2ubuntu5 [29.1 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontconfig1 amd64 2.13.1-4.2ubuntu5 [131 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpixman-1-0 amd64 0.40.0-1ubuntu0.22.04.1 [264 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render0 amd64 1.14-3ubuntu3 [16.4 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-shm0 amd64 1.14-3ubuntu3 [5780 B]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxrender1 amd64 1:0.9.10-1build4 [19.7 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcairo2 amd64 1.16.0-5ubuntu2 [628 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcodec2-1.0 amd64 1.0.1-3 [8435 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libdav1d5 amd64 0.9.2-1 [463 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmp3lame0 amd64 3.100-3build2 [141 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopenjp2-7 amd64 2.4.0-6 [158 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcairo-gobject2 amd64 1.16.0-5ubuntu2 [19.4 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf2.0-common all 2.42.8+dfsg-1ubuntu0.2 [5530 B]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdeflate0 amd64 1.10-2 [70.9 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libjbig0 amd64 2.1-3.1ubuntu0.22.04.1 [29.2 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwebp7 amd64 1.2.2-2ubuntu0.22.04.1 [206 kB]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtiff5 amd64 4.3.0-6ubuntu0.4 [183 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf-2.0-0 amd64 2.42.8+dfsg-1ubuntu0.2 [148 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 fontconfig amd64 2.13.1-4.2ubuntu5 [177 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgraphite2-3 amd64 1.3.14-1build2 [71.3 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libharfbuzz0b amd64 2.7.4-1ubuntu3.1 [352 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu jammy/main amd64 libthai-data all 0.1.29-1build1 [162 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdatrie1 amd64 0.2.13-2 [19.9 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 libthai0 amd64 0.1.29-1build1 [19.2 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpango-1.0-0 amd64 1.50.6+ds-2ubuntu1 [230 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpangoft2-1.0-0 amd64 1.50.6+ds-2ubuntu1 [54.0 kB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpangocairo-1.0-0 amd64 1.50.6+ds-2ubuntu1 [39.8 kB]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-2 amd64 2.52.5+dfsg-3ubuntu0.2 [2974 kB]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libshine3 amd64 3.1.1-2 [23.2 kB]\n",
      "Get:51 http://archive.ubuntu.com/ubuntu jammy/main amd64 libspeex1 amd64 1.2~rc1.2-1.1ubuntu3 [57.9 kB]\n",
      "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsoxr0 amd64 0.1.3-4build2 [79.8 kB]\n",
      "Get:53 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libswresample3 amd64 7:4.4.2-0ubuntu0.22.04.1 [62.2 kB]\n",
      "Get:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtheora0 amd64 1.1.1+dfsg.1-15ubuntu4 [209 kB]\n",
      "Get:55 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtwolame0 amd64 0.4.0-2build2 [52.5 kB]\n",
      "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvpx7 amd64 1.11.0-2ubuntu2 [1078 kB]\n",
      "Get:57 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwebpmux3 amd64 1.2.2-2ubuntu0.22.04.1 [20.5 kB]\n",
      "Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libx264-163 amd64 2:0.163.3060+git5db6aa6-2build1 [591 kB]\n",
      "Get:59 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libx265-199 amd64 3.5-2 [1170 kB]\n",
      "Get:60 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libxvidcore4 amd64 2:1.3.7-1 [201 kB]\n",
      "Get:61 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzvbi-common all 0.2.35-19 [35.5 kB]\n",
      "Get:62 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzvbi0 amd64 0.2.35-19 [262 kB]\n",
      "Get:63 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavcodec58 amd64 7:4.4.2-0ubuntu0.22.04.1 [5567 kB]\n",
      "Get:64 http://archive.ubuntu.com/ubuntu jammy/main amd64 libraw1394-11 amd64 2.1.2-2build2 [27.0 kB]\n",
      "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 libavc1394-0 amd64 0.5.4-5build2 [17.0 kB]\n",
      "Get:66 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libass9 amd64 1:0.15.2-1 [97.5 kB]\n",
      "Get:67 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libudfread0 amd64 1.1.2-1 [16.2 kB]\n",
      "Get:68 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbluray2 amd64 1:1.3.1-1 [159 kB]\n",
      "Get:69 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libchromaprint1 amd64 1.5.1-2 [28.4 kB]\n",
      "Get:70 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgme0 amd64 0.6.3-2 [127 kB]\n",
      "Get:71 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmpg123-0 amd64 1.29.3-1build1 [172 kB]\n",
      "Get:72 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenmpt0 amd64 0.6.1-1 [592 kB]\n",
      "Get:73 http://archive.ubuntu.com/ubuntu jammy/main amd64 librabbitmq4 amd64 0.10.0-1ubuntu2 [39.3 kB]\n",
      "Get:74 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsrt1.4-gnutls amd64 1.4.4-4 [309 kB]\n",
      "Get:75 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libssh-gcrypt-4 amd64 0.9.6-2ubuntu0.22.04.1 [222 kB]\n",
      "Get:76 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libnorm1 amd64 1.5.9+dfsg-2 [221 kB]\n",
      "Get:77 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpgm-5.3-0 amd64 5.3.128~dfsg-2 [161 kB]\n",
      "Get:78 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzmq5 amd64 4.3.4-2 [256 kB]\n",
      "Get:79 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavformat58 amd64 7:4.4.2-0ubuntu0.22.04.1 [1103 kB]\n",
      "Get:80 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbs2b0 amd64 3.1.0+dfsg-2.2build1 [10.2 kB]\n",
      "Get:81 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libflite1 amd64 2.2-3 [13.7 MB]\n",
      "Get:82 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libserd-0-0 amd64 0.30.10-2 [40.8 kB]\n",
      "Get:83 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsord-0-0 amd64 0.16.8-2 [21.2 kB]\n",
      "Get:84 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsratom-0-0 amd64 0.6.8-1 [17.0 kB]\n",
      "Get:85 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblilv-0-0 amd64 0.24.12-2 [42.8 kB]\n",
      "Get:86 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmysofa1 amd64 1.2.1~dfsg0-1 [1157 kB]\n",
      "Get:87 http://archive.ubuntu.com/ubuntu jammy/main amd64 libasyncns0 amd64 0.8-6build2 [12.8 kB]\n",
      "Get:88 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libx11-xcb1 amd64 2:1.7.5-1ubuntu0.2 [7800 B]\n",
      "Get:89 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpulse0 amd64 1:15.99.1+dfsg1-1ubuntu2.1 [297 kB]\n",
      "Get:90 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsphinxbase3 amd64 0.8+5prealpha+1-13build1 [126 kB]\n",
      "Get:91 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpocketsphinx3 amd64 0.8.0+real5prealpha+1-14ubuntu1 [132 kB]\n",
      "Get:92 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libpostproc55 amd64 7:4.4.2-0ubuntu0.22.04.1 [60.1 kB]\n",
      "Get:93 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsamplerate0 amd64 0.2.2-1build1 [1359 kB]\n",
      "Get:94 http://archive.ubuntu.com/ubuntu jammy/universe amd64 librubberband2 amd64 2.0.0-2 [90.0 kB]\n",
      "Get:95 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libswscale5 amd64 7:4.4.2-0ubuntu0.22.04.1 [180 kB]\n",
      "Get:96 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libvidstab1.1 amd64 1.1.0-2 [35.0 kB]\n",
      "Get:97 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzimg2 amd64 3.0.3+ds1-1 [241 kB]\n",
      "Get:98 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavfilter7 amd64 7:4.4.2-0ubuntu0.22.04.1 [1496 kB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:99 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcaca0 amd64 0.99.beta19-2.2ubuntu4 [224 kB]\n",
      "Get:100 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcdio19 amd64 2.1.0-3build1 [63.3 kB]\n",
      "Get:101 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcdio-cdda2 amd64 10.2+2.0.0-1build3 [16.7 kB]\n",
      "Get:102 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcdio-paranoia2 amd64 10.2+2.0.0-1build3 [15.9 kB]\n",
      "Get:103 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libdc1394-25 amd64 2.2.6-4 [88.8 kB]\n",
      "Get:104 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd0 amd64 1.4.0-1 [73.6 kB]\n",
      "Get:105 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglapi-mesa amd64 22.2.5-0ubuntu0.1~22.04.3 [35.5 kB]\n",
      "Get:106 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-dri2-0 amd64 1.14-3ubuntu3 [7206 B]\n",
      "Get:107 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-dri3-0 amd64 1.14-3ubuntu3 [6968 B]\n",
      "Get:108 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-glx0 amd64 1.14-3ubuntu3 [25.9 kB]\n",
      "Get:109 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-present0 amd64 1.14-3ubuntu3 [5734 B]\n",
      "Get:110 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-sync1 amd64 1.14-3ubuntu3 [9416 B]\n",
      "Get:111 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xfixes0 amd64 1.14-3ubuntu3 [9996 B]\n",
      "Get:112 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxshmfence1 amd64 1.3-1build4 [5394 B]\n",
      "Get:113 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86vm1 amd64 1:1.1.4-1build3 [10.4 kB]\n",
      "Get:114 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-amdgpu1 amd64 2.4.113-2~ubuntu0.22.04.1 [19.9 kB]\n",
      "Get:115 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-nouveau2 amd64 2.4.113-2~ubuntu0.22.04.1 [17.5 kB]\n",
      "Get:116 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-radeon1 amd64 2.4.113-2~ubuntu0.22.04.1 [21.6 kB]\n",
      "Get:117 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libllvm15 amd64 1:15.0.7-0ubuntu0.22.04.3 [25.4 MB]\n",
      "Get:118 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsensors-config all 1:3.6.0-7ubuntu1 [5274 B]33m\u001b[33m\n",
      "Get:119 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsensors5 amd64 1:3.6.0-7ubuntu1 [26.3 kB]\n",
      "Get:120 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dri amd64 22.2.5-0ubuntu0.1~22.04.3 [7511 kB]\n",
      "Get:121 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglx-mesa0 amd64 22.2.5-0ubuntu0.1~22.04.3 [158 kB]\n",
      "Get:122 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx0 amd64 1.4.0-1 [41.0 kB]\n",
      "Get:123 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl1 amd64 1.4.0-1 [110 kB]m\n",
      "Get:124 http://archive.ubuntu.com/ubuntu jammy/main amd64 libiec61883-0 amd64 1.2.0-4build3 [25.9 kB]\n",
      "Get:125 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjack-jackd2-0 amd64 1.9.20~dfsg-1 [293 kB]\n",
      "Get:126 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenal-data all 1:1.19.1-2build3 [164 kB]\n",
      "Get:127 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsndio7.0 amd64 1.8.1-1.1 [29.3 kB]\n",
      "Get:128 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenal1 amd64 1:1.19.1-2build3 [535 kB]\n",
      "Get:129 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-client0 amd64 1.20.0-1ubuntu0.1 [25.9 kB]\n",
      "Get:130 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdecor-0-0 amd64 0.1.0-3build1 [15.1 kB]\n",
      "Get:131 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-server0 amd64 1.20.0-1ubuntu0.1 [34.3 kB]\n",
      "Get:132 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgbm1 amd64 22.2.5-0ubuntu0.1~22.04.3 [33.1 kB]\n",
      "Get:133 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-cursor0 amd64 1.20.0-1ubuntu0.1 [10.7 kB]\n",
      "Get:134 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-egl1 amd64 1.20.0-1ubuntu0.1 [5582 B]\n",
      "Get:135 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcursor1 amd64 1:1.2.0-2build4 [20.9 kB]\n",
      "Get:136 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxi6 amd64 2:1.8-1build1 [32.6 kB]\n",
      "Get:137 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxinerama1 amd64 2:1.1.4-3 [7382 B]\n",
      "Get:138 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon0 amd64 1.4.0-1 [125 kB]\n",
      "Get:139 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxrandr2 amd64 2:1.5.2-1build1 [20.4 kB]\n",
      "Get:140 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-common all 1:7.7+23ubuntu2 [23.4 kB]\n",
      "Get:141 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxss1 amd64 1:1.2.3-1build2 [8476 B]\n",
      "Get:142 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsdl2-2.0-0 amd64 2.0.20+dfsg-2ubuntu1.22.04.1 [582 kB]\n",
      "Get:143 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-shape0 amd64 1.14-3ubuntu3 [6158 B]\n",
      "Get:144 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxv1 amd64 2:1.0.11-1build2 [11.2 kB]\n",
      "Get:145 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavdevice58 amd64 7:4.4.2-0ubuntu0.22.04.1 [87.5 kB]\n",
      "Get:146 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 ffmpeg amd64 7:4.4.2-0ubuntu0.22.04.1 [1696 kB]\n",
      "Get:147 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libigdgmm12 amd64 22.1.2+ds1-1 [139 kB]\n",
      "Get:148 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 intel-media-va-driver amd64 22.3.1+dfsg1-1ubuntu2 [2283 kB]\n",
      "Get:149 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaacs0 amd64 0.11.1-1 [64.1 kB]\n",
      "Get:150 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbdplus0 amd64 0.2.0-1 [52.2 kB]\n",
      "Get:151 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdecor-0-plugin-1-cairo amd64 0.1.0-3build1 [20.4 kB]\n",
      "Get:152 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpciaccess0 amd64 0.16-3 [19.1 kB]\n",
      "Get:153 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-intel1 amd64 2.4.113-2~ubuntu0.22.04.1 [66.7 kB]\n",
      "Get:154 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf2.0-bin amd64 2.42.8+dfsg-1ubuntu0.2 [14.2 kB]\n",
      "Get:155 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl1-amber-dri amd64 21.3.7-0ubuntu1 [4433 kB]\n",
      "Get:156 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
      "Get:157 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 mesa-va-drivers amd64 22.2.5-0ubuntu0.1~22.04.3 [3409 kB]\n",
      "Get:158 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vdpau-drivers amd64 22.2.5-0ubuntu0.1~22.04.3 [3313 kB]\n",
      "Get:159 http://archive.ubuntu.com/ubuntu jammy/universe amd64 i965-va-driver amd64 2.4.1+dfsg1-1 [302 kB]\n",
      "Get:160 http://archive.ubuntu.com/ubuntu jammy/universe amd64 va-driver-all amd64 2.14.0-1 [3984 B]\n",
      "Get:161 http://archive.ubuntu.com/ubuntu jammy/main amd64 vdpau-driver-all amd64 1.4-3build2 [4510 B]\n",
      "Get:162 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pocketsphinx-en-us all 0.8.0+real5prealpha+1-14ubuntu1 [27.6 MB]\n",
      "Fetched 146 MB in 20s (7175 kB/s)                                              \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 162.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Extracting templates from packages: 100%\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libelf1:amd64.\n",
      "(Reading database ... 28925 files and directories currently installed.)\n",
      "Preparing to unpack .../000-libelf1_0.186-1build1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libelf1:amd64 (0.186-1build1) ...\n",
      "Selecting previously unselected package libfribidi0:amd64.\n",
      "Preparing to unpack .../001-libfribidi0_1.0.8-2ubuntu3.1_amd64.deb ...\n",
      "Unpacking libfribidi0:amd64 (1.0.8-2ubuntu3.1) ...\n",
      "Selecting previously unselected package libicu70:amd64.\n",
      "Preparing to unpack .../002-libicu70_70.1-2_amd64.deb ...\n",
      "Unpacking libicu70:amd64 (70.1-2) ...\n",
      "Selecting previously unselected package libslang2:amd64.\n",
      "Preparing to unpack .../003-libslang2_2.3.2-5build4_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  1%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libslang2:amd64 (2.3.2-5build4) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libxml2:amd64.\n",
      "Preparing to unpack .../004-libxml2_2.9.13+dfsg-1ubuntu0.3_amd64.deb ...\n",
      "Unpacking libxml2:amd64 (2.9.13+dfsg-1ubuntu0.3) ...\n",
      "Selecting previously unselected package shared-mime-info.\n",
      "Preparing to unpack .../005-shared-mime-info_2.1-2_amd64.deb ...\n",
      "Unpacking shared-mime-info (2.1-2) ...\n",
      "Selecting previously unselected package xkb-data.\n",
      "Preparing to unpack .../006-xkb-data_2.33-1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking xkb-data (2.33-1) ...\n",
      "Selecting previously unselected package libdrm-common.\n",
      "Preparing to unpack .../007-libdrm-common_2.4.113-2~ubuntu0.22.04.1_all.deb ...\n",
      "Unpacking libdrm-common (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libdrm2:amd64.\n",
      "Preparing to unpack .../008-libdrm2_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libusb-1.0-0:amd64.\n",
      "Preparing to unpack .../009-libusb-1.0-0_2%3a1.0.25-1ubuntu2_amd64.deb ...\n",
      "Unpacking libusb-1.0-0:amd64 (2:1.0.25-1ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  3%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Selecting previously unselected package libaom3:amd64.\n",
      "Preparing to unpack .../010-libaom3_3.3.0-1_amd64.deb ...\n",
      "Unpacking libaom3:amd64 (3.3.0-1) ...\n",
      "Selecting previously unselected package libva2:amd64.\n",
      "Preparing to unpack .../011-libva2_2.14.0-1_amd64.deb ...\n",
      "Unpacking libva2:amd64 (2.14.0-1) ...\n",
      "Selecting previously unselected package libmfx1:amd64.\n",
      "Preparing to unpack .../012-libmfx1_22.3.0-1_amd64.deb ...\n",
      "Unpacking libmfx1:amd64 (22.3.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Selecting previously unselected package libva-drm2:amd64.\n",
      "Preparing to unpack .../013-libva-drm2_2.14.0-1_amd64.deb ...\n",
      "Unpacking libva-drm2:amd64 (2.14.0-1) ...\n",
      "Selecting previously unselected package libxfixes3:amd64.\n",
      "Preparing to unpack .../014-libxfixes3_1%3a6.0.0-1_amd64.deb ...\n",
      "Unpacking libxfixes3:amd64 (1:6.0.0-1) ...\n",
      "Selecting previously unselected package libva-x11-2:amd64.\n",
      "Preparing to unpack .../015-libva-x11-2_2.14.0-1_amd64.deb ...\n",
      "Unpacking libva-x11-2:amd64 (2.14.0-1) ...\n",
      "Selecting previously unselected package libvdpau1:amd64.\n",
      "Preparing to unpack .../016-libvdpau1_1.4-3build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [##........................................................] \u001b8Unpacking libvdpau1:amd64 (1.4-3build2) ...\n",
      "Selecting previously unselected package ocl-icd-libopencl1:amd64.\n",
      "Preparing to unpack .../017-ocl-icd-libopencl1_2.2.14-3_amd64.deb ...\n",
      "Unpacking ocl-icd-libopencl1:amd64 (2.2.14-3) ...\n",
      "Selecting previously unselected package libavutil56:amd64.\n",
      "Preparing to unpack .../018-libavutil56_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libavutil56:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libfreetype6:amd64.\n",
      "Preparing to unpack .../019-libfreetype6_2.11.1+dfsg-1ubuntu0.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libfreetype6:amd64 (2.11.1+dfsg-1ubuntu0.2) ...\n",
      "Selecting previously unselected package fonts-dejavu-core.\n",
      "Preparing to unpack .../020-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
      "Selecting previously unselected package fontconfig-config.\n",
      "Preparing to unpack .../021-fontconfig-config_2.13.1-4.2ubuntu5_all.deb ...\n",
      "Unpacking fontconfig-config (2.13.1-4.2ubuntu5) ...\n",
      "Selecting previously unselected package libfontconfig1:amd64.\n",
      "Preparing to unpack .../022-libfontconfig1_2.13.1-4.2ubuntu5_amd64.deb ...\n",
      "Unpacking libfontconfig1:amd64 (2.13.1-4.2ubuntu5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package libpixman-1-0:amd64.\n",
      "Preparing to unpack .../023-libpixman-1-0_0.40.0-1ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libpixman-1-0:amd64 (0.40.0-1ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libxcb-render0:amd64.\n",
      "Preparing to unpack .../024-libxcb-render0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-render0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-shm0:amd64.\n",
      "Preparing to unpack .../025-libxcb-shm0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-shm0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Selecting previously unselected package libxrender1:amd64.\n",
      "Preparing to unpack .../026-libxrender1_1%3a0.9.10-1build4_amd64.deb ...\n",
      "Unpacking libxrender1:amd64 (1:0.9.10-1build4) ...\n",
      "Selecting previously unselected package libcairo2:amd64.\n",
      "Preparing to unpack .../027-libcairo2_1.16.0-5ubuntu2_amd64.deb ...\n",
      "Unpacking libcairo2:amd64 (1.16.0-5ubuntu2) ...\n",
      "Selecting previously unselected package libcodec2-1.0:amd64.\n",
      "Preparing to unpack .../028-libcodec2-1.0_1.0.1-3_amd64.deb ...\n",
      "Unpacking libcodec2-1.0:amd64 (1.0.1-3) ...\n",
      "Selecting previously unselected package libdav1d5:amd64.\n",
      "Preparing to unpack .../029-libdav1d5_0.9.2-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking libdav1d5:amd64 (0.9.2-1) ...\n",
      "Selecting previously unselected package libmp3lame0:amd64.\n",
      "Preparing to unpack .../030-libmp3lame0_3.100-3build2_amd64.deb ...\n",
      "Unpacking libmp3lame0:amd64 (3.100-3build2) ...\n",
      "Selecting previously unselected package libopenjp2-7:amd64.\n",
      "Preparing to unpack .../031-libopenjp2-7_2.4.0-6_amd64.deb ...\n",
      "Unpacking libopenjp2-7:amd64 (2.4.0-6) ...\n",
      "Selecting previously unselected package libcairo-gobject2:amd64.\n",
      "Preparing to unpack .../032-libcairo-gobject2_1.16.0-5ubuntu2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking libcairo-gobject2:amd64 (1.16.0-5ubuntu2) ...\n",
      "Selecting previously unselected package libgdk-pixbuf2.0-common.\n",
      "Preparing to unpack .../033-libgdk-pixbuf2.0-common_2.42.8+dfsg-1ubuntu0.2_all.deb ...\n",
      "Unpacking libgdk-pixbuf2.0-common (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libdeflate0:amd64.\n",
      "Preparing to unpack .../034-libdeflate0_1.10-2_amd64.deb ...\n",
      "Unpacking libdeflate0:amd64 (1.10-2) ...\n",
      "Selecting previously unselected package libjbig0:amd64.\n",
      "Preparing to unpack .../035-libjbig0_2.1-3.1ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libjbig0:amd64 (2.1-3.1ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package libwebp7:amd64.\n",
      "Preparing to unpack .../036-libwebp7_1.2.2-2ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libwebp7:amd64 (1.2.2-2ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libtiff5:amd64.\n",
      "Preparing to unpack .../037-libtiff5_4.3.0-6ubuntu0.4_amd64.deb ...\n",
      "Unpacking libtiff5:amd64 (4.3.0-6ubuntu0.4) ...\n",
      "Selecting previously unselected package libgdk-pixbuf-2.0-0:amd64.\n",
      "Preparing to unpack .../038-libgdk-pixbuf-2.0-0_2.42.8+dfsg-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [######....................................................] \u001b8Selecting previously unselected package fontconfig.\n",
      "Preparing to unpack .../039-fontconfig_2.13.1-4.2ubuntu5_amd64.deb ...\n",
      "Unpacking fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Selecting previously unselected package libgraphite2-3:amd64.\n",
      "Preparing to unpack .../040-libgraphite2-3_1.3.14-1build2_amd64.deb ...\n",
      "Unpacking libgraphite2-3:amd64 (1.3.14-1build2) ...\n",
      "Selecting previously unselected package libharfbuzz0b:amd64.\n",
      "Preparing to unpack .../041-libharfbuzz0b_2.7.4-1ubuntu3.1_amd64.deb ...\n",
      "Unpacking libharfbuzz0b:amd64 (2.7.4-1ubuntu3.1) ...\n",
      "Selecting previously unselected package libthai-data.\n",
      "Preparing to unpack .../042-libthai-data_0.1.29-1build1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Unpacking libthai-data (0.1.29-1build1) ...\n",
      "Selecting previously unselected package libdatrie1:amd64.\n",
      "Preparing to unpack .../043-libdatrie1_0.2.13-2_amd64.deb ...\n",
      "Unpacking libdatrie1:amd64 (0.2.13-2) ...\n",
      "Selecting previously unselected package libthai0:amd64.\n",
      "Preparing to unpack .../044-libthai0_0.1.29-1build1_amd64.deb ...\n",
      "Unpacking libthai0:amd64 (0.1.29-1build1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libpango-1.0-0:amd64.\n",
      "Preparing to unpack .../045-libpango-1.0-0_1.50.6+ds-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Unpacking libpango-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Selecting previously unselected package libpangoft2-1.0-0:amd64.\n",
      "Preparing to unpack .../046-libpangoft2-1.0-0_1.50.6+ds-2ubuntu1_amd64.deb ...\n",
      "Unpacking libpangoft2-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Selecting previously unselected package libpangocairo-1.0-0:amd64.\n",
      "Preparing to unpack .../047-libpangocairo-1.0-0_1.50.6+ds-2ubuntu1_amd64.deb ...\n",
      "Unpacking libpangocairo-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Selecting previously unselected package librsvg2-2:amd64.\n",
      "Preparing to unpack .../048-librsvg2-2_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking librsvg2-2:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libshine3:amd64.\n",
      "Preparing to unpack .../049-libshine3_3.1.1-2_amd64.deb ...\n",
      "Unpacking libshine3:amd64 (3.1.1-2) ...\n",
      "Selecting previously unselected package libspeex1:amd64.\n",
      "Preparing to unpack .../050-libspeex1_1.2~rc1.2-1.1ubuntu3_amd64.deb ...\n",
      "Unpacking libspeex1:amd64 (1.2~rc1.2-1.1ubuntu3) ...\n",
      "Selecting previously unselected package libsoxr0:amd64.\n",
      "Preparing to unpack .../051-libsoxr0_0.1.3-4build2_amd64.deb ...\n",
      "Unpacking libsoxr0:amd64 (0.1.3-4build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package libswresample3:amd64.\n",
      "Preparing to unpack .../052-libswresample3_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libswresample3:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libtheora0:amd64.\n",
      "Preparing to unpack .../053-libtheora0_1.1.1+dfsg.1-15ubuntu4_amd64.deb ...\n",
      "Unpacking libtheora0:amd64 (1.1.1+dfsg.1-15ubuntu4) ...\n",
      "Selecting previously unselected package libtwolame0:amd64.\n",
      "Preparing to unpack .../054-libtwolame0_0.4.0-2build2_amd64.deb ...\n",
      "Unpacking libtwolame0:amd64 (0.4.0-2build2) ...\n",
      "Selecting previously unselected package libvpx7:amd64.\n",
      "Preparing to unpack .../055-libvpx7_1.11.0-2ubuntu2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking libvpx7:amd64 (1.11.0-2ubuntu2) ...\n",
      "Selecting previously unselected package libwebpmux3:amd64.\n",
      "Preparing to unpack .../056-libwebpmux3_1.2.2-2ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libwebpmux3:amd64 (1.2.2-2ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libx264-163:amd64.\n",
      "Preparing to unpack .../057-libx264-163_2%3a0.163.3060+git5db6aa6-2build1_amd64.deb ...\n",
      "Unpacking libx264-163:amd64 (2:0.163.3060+git5db6aa6-2build1) ...\n",
      "Selecting previously unselected package libx265-199:amd64.\n",
      "Preparing to unpack .../058-libx265-199_3.5-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking libx265-199:amd64 (3.5-2) ...\n",
      "Selecting previously unselected package libxvidcore4:amd64.\n",
      "Preparing to unpack .../059-libxvidcore4_2%3a1.3.7-1_amd64.deb ...\n",
      "Unpacking libxvidcore4:amd64 (2:1.3.7-1) ...\n",
      "Selecting previously unselected package libzvbi-common.\n",
      "Preparing to unpack .../060-libzvbi-common_0.2.35-19_all.deb ...\n",
      "Unpacking libzvbi-common (0.2.35-19) ...\n",
      "Selecting previously unselected package libzvbi0:amd64.\n",
      "Preparing to unpack .../061-libzvbi0_0.2.35-19_amd64.deb ...\n",
      "Unpacking libzvbi0:amd64 (0.2.35-19) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libavcodec58:amd64.\n",
      "Preparing to unpack .../062-libavcodec58_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libavcodec58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libraw1394-11:amd64.\n",
      "Preparing to unpack .../063-libraw1394-11_2.1.2-2build2_amd64.deb ...\n",
      "Unpacking libraw1394-11:amd64 (2.1.2-2build2) ...\n",
      "Selecting previously unselected package libavc1394-0:amd64.\n",
      "Preparing to unpack .../064-libavc1394-0_0.5.4-5build2_amd64.deb ...\n",
      "Unpacking libavc1394-0:amd64 (0.5.4-5build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libass9:amd64.\n",
      "Preparing to unpack .../065-libass9_1%3a0.15.2-1_amd64.deb ...\n",
      "Unpacking libass9:amd64 (1:0.15.2-1) ...\n",
      "Selecting previously unselected package libudfread0:amd64.\n",
      "Preparing to unpack .../066-libudfread0_1.1.2-1_amd64.deb ...\n",
      "Unpacking libudfread0:amd64 (1.1.2-1) ...\n",
      "Selecting previously unselected package libbluray2:amd64.\n",
      "Preparing to unpack .../067-libbluray2_1%3a1.3.1-1_amd64.deb ...\n",
      "Unpacking libbluray2:amd64 (1:1.3.1-1) ...\n",
      "Selecting previously unselected package libchromaprint1:amd64.\n",
      "Preparing to unpack .../068-libchromaprint1_1.5.1-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking libchromaprint1:amd64 (1.5.1-2) ...\n",
      "Selecting previously unselected package libgme0:amd64.\n",
      "Preparing to unpack .../069-libgme0_0.6.3-2_amd64.deb ...\n",
      "Unpacking libgme0:amd64 (0.6.3-2) ...\n",
      "Selecting previously unselected package libmpg123-0:amd64.\n",
      "Preparing to unpack .../070-libmpg123-0_1.29.3-1build1_amd64.deb ...\n",
      "Unpacking libmpg123-0:amd64 (1.29.3-1build1) ...\n",
      "Selecting previously unselected package libopenmpt0:amd64.\n",
      "Preparing to unpack .../071-libopenmpt0_0.6.1-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Unpacking libopenmpt0:amd64 (0.6.1-1) ...\n",
      "Selecting previously unselected package librabbitmq4:amd64.\n",
      "Preparing to unpack .../072-librabbitmq4_0.10.0-1ubuntu2_amd64.deb ...\n",
      "Unpacking librabbitmq4:amd64 (0.10.0-1ubuntu2) ...\n",
      "Selecting previously unselected package libsrt1.4-gnutls:amd64.\n",
      "Preparing to unpack .../073-libsrt1.4-gnutls_1.4.4-4_amd64.deb ...\n",
      "Unpacking libsrt1.4-gnutls:amd64 (1.4.4-4) ...\n",
      "Selecting previously unselected package libssh-gcrypt-4:amd64.\n",
      "Preparing to unpack .../074-libssh-gcrypt-4_0.9.6-2ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libssh-gcrypt-4:amd64 (0.9.6-2ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libnorm1:amd64.\n",
      "Preparing to unpack .../075-libnorm1_1.5.9+dfsg-2_amd64.deb ...\n",
      "Unpacking libnorm1:amd64 (1.5.9+dfsg-2) ...\n",
      "Selecting previously unselected package libpgm-5.3-0:amd64.\n",
      "Preparing to unpack .../076-libpgm-5.3-0_5.3.128~dfsg-2_amd64.deb ...\n",
      "Unpacking libpgm-5.3-0:amd64 (5.3.128~dfsg-2) ...\n",
      "Selecting previously unselected package libzmq5:amd64.\n",
      "Preparing to unpack .../077-libzmq5_4.3.4-2_amd64.deb ...\n",
      "Unpacking libzmq5:amd64 (4.3.4-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Selecting previously unselected package libavformat58:amd64.\n",
      "Preparing to unpack .../078-libavformat58_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libavformat58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libbs2b0:amd64.\n",
      "Preparing to unpack .../079-libbs2b0_3.1.0+dfsg-2.2build1_amd64.deb ...\n",
      "Unpacking libbs2b0:amd64 (3.1.0+dfsg-2.2build1) ...\n",
      "Selecting previously unselected package libflite1:amd64.\n",
      "Preparing to unpack .../080-libflite1_2.2-3_amd64.deb ...\n",
      "Unpacking libflite1:amd64 (2.2-3) ...\n",
      "Selecting previously unselected package libserd-0-0:amd64.\n",
      "Preparing to unpack .../081-libserd-0-0_0.30.10-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libserd-0-0:amd64 (0.30.10-2) ...\n",
      "Selecting previously unselected package libsord-0-0:amd64.\n",
      "Preparing to unpack .../082-libsord-0-0_0.16.8-2_amd64.deb ...\n",
      "Unpacking libsord-0-0:amd64 (0.16.8-2) ...\n",
      "Selecting previously unselected package libsratom-0-0:amd64.\n",
      "Preparing to unpack .../083-libsratom-0-0_0.6.8-1_amd64.deb ...\n",
      "Unpacking libsratom-0-0:amd64 (0.6.8-1) ...\n",
      "Selecting previously unselected package liblilv-0-0:amd64.\n",
      "Preparing to unpack .../084-liblilv-0-0_0.24.12-2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking liblilv-0-0:amd64 (0.24.12-2) ...\n",
      "Selecting previously unselected package libmysofa1:amd64.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to unpack .../085-libmysofa1_1.2.1~dfsg0-1_amd64.deb ...\n",
      "Unpacking libmysofa1:amd64 (1.2.1~dfsg0-1) ...\n",
      "Selecting previously unselected package libasyncns0:amd64.\n",
      "Preparing to unpack .../086-libasyncns0_0.8-6build2_amd64.deb ...\n",
      "Unpacking libasyncns0:amd64 (0.8-6build2) ...\n",
      "Selecting previously unselected package libx11-xcb1:amd64.\n",
      "Preparing to unpack .../087-libx11-xcb1_2%3a1.7.5-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libx11-xcb1:amd64 (2:1.7.5-1ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Selecting previously unselected package libpulse0:amd64.\n",
      "Preparing to unpack .../088-libpulse0_1%3a15.99.1+dfsg1-1ubuntu2.1_amd64.deb ...\n",
      "Unpacking libpulse0:amd64 (1:15.99.1+dfsg1-1ubuntu2.1) ...\n",
      "Selecting previously unselected package libsphinxbase3:amd64.\n",
      "Preparing to unpack .../089-libsphinxbase3_0.8+5prealpha+1-13build1_amd64.deb ...\n",
      "Unpacking libsphinxbase3:amd64 (0.8+5prealpha+1-13build1) ...\n",
      "Selecting previously unselected package libpocketsphinx3:amd64.\n",
      "Preparing to unpack .../090-libpocketsphinx3_0.8.0+real5prealpha+1-14ubuntu1_amd64.deb ...\n",
      "Unpacking libpocketsphinx3:amd64 (0.8.0+real5prealpha+1-14ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Selecting previously unselected package libpostproc55:amd64.\n",
      "Preparing to unpack .../091-libpostproc55_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libpostproc55:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libsamplerate0:amd64.\n",
      "Preparing to unpack .../092-libsamplerate0_0.2.2-1build1_amd64.deb ...\n",
      "Unpacking libsamplerate0:amd64 (0.2.2-1build1) ...\n",
      "Selecting previously unselected package librubberband2:amd64.\n",
      "Preparing to unpack .../093-librubberband2_2.0.0-2_amd64.deb ...\n",
      "Unpacking librubberband2:amd64 (2.0.0-2) ...\n",
      "Selecting previously unselected package libswscale5:amd64.\n",
      "Preparing to unpack .../094-libswscale5_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Unpacking libswscale5:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libvidstab1.1:amd64.\n",
      "Preparing to unpack .../095-libvidstab1.1_1.1.0-2_amd64.deb ...\n",
      "Unpacking libvidstab1.1:amd64 (1.1.0-2) ...\n",
      "Selecting previously unselected package libzimg2:amd64.\n",
      "Preparing to unpack .../096-libzimg2_3.0.3+ds1-1_amd64.deb ...\n",
      "Unpacking libzimg2:amd64 (3.0.3+ds1-1) ...\n",
      "Selecting previously unselected package libavfilter7:amd64.\n",
      "Preparing to unpack .../097-libavfilter7_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libavfilter7:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libcaca0:amd64.\n",
      "Preparing to unpack .../098-libcaca0_0.99.beta19-2.2ubuntu4_amd64.deb ...\n",
      "Unpacking libcaca0:amd64 (0.99.beta19-2.2ubuntu4) ...\n",
      "Selecting previously unselected package libcdio19:amd64.\n",
      "Preparing to unpack .../099-libcdio19_2.1.0-3build1_amd64.deb ...\n",
      "Unpacking libcdio19:amd64 (2.1.0-3build1) ...\n",
      "Selecting previously unselected package libcdio-cdda2:amd64.\n",
      "Preparing to unpack .../100-libcdio-cdda2_10.2+2.0.0-1build3_amd64.deb ...\n",
      "Unpacking libcdio-cdda2:amd64 (10.2+2.0.0-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libcdio-paranoia2:amd64.\n",
      "Preparing to unpack .../101-libcdio-paranoia2_10.2+2.0.0-1build3_amd64.deb ...\n",
      "Unpacking libcdio-paranoia2:amd64 (10.2+2.0.0-1build3) ...\n",
      "Selecting previously unselected package libdc1394-25:amd64.\n",
      "Preparing to unpack .../102-libdc1394-25_2.2.6-4_amd64.deb ...\n",
      "Unpacking libdc1394-25:amd64 (2.2.6-4) ...\n",
      "Selecting previously unselected package libglvnd0:amd64.\n",
      "Preparing to unpack .../103-libglvnd0_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglvnd0:amd64 (1.4.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Selecting previously unselected package libglapi-mesa:amd64.\n",
      "Preparing to unpack .../104-libglapi-mesa_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking libglapi-mesa:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package libxcb-dri2-0:amd64.\n",
      "Preparing to unpack .../105-libxcb-dri2-0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-dri2-0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-dri3-0:amd64.\n",
      "Preparing to unpack .../106-libxcb-dri3-0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-dri3-0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-glx0:amd64.\n",
      "Preparing to unpack .../107-libxcb-glx0_1.14-3ubuntu3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libxcb-glx0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-present0:amd64.\n",
      "Preparing to unpack .../108-libxcb-present0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-present0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-sync1:amd64.\n",
      "Preparing to unpack .../109-libxcb-sync1_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-sync1:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-xfixes0:amd64.\n",
      "Preparing to unpack .../110-libxcb-xfixes0_1.14-3ubuntu3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking libxcb-xfixes0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxshmfence1:amd64.\n",
      "Preparing to unpack .../111-libxshmfence1_1.3-1build4_amd64.deb ...\n",
      "Unpacking libxshmfence1:amd64 (1.3-1build4) ...\n",
      "Selecting previously unselected package libxxf86vm1:amd64.\n",
      "Preparing to unpack .../112-libxxf86vm1_1%3a1.1.4-1build3_amd64.deb ...\n",
      "Unpacking libxxf86vm1:amd64 (1:1.1.4-1build3) ...\n",
      "Selecting previously unselected package libdrm-amdgpu1:amd64.\n",
      "Preparing to unpack .../113-libdrm-amdgpu1_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-amdgpu1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libdrm-nouveau2:amd64.\n",
      "Preparing to unpack .../114-libdrm-nouveau2_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-nouveau2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libdrm-radeon1:amd64.\n",
      "Preparing to unpack .../115-libdrm-radeon1_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-radeon1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libllvm15:amd64.\n",
      "Preparing to unpack .../116-libllvm15_1%3a15.0.7-0ubuntu0.22.04.3_amd64.deb ...\n",
      "Unpacking libllvm15:amd64 (1:15.0.7-0ubuntu0.22.04.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libsensors-config.\n",
      "Preparing to unpack .../117-libsensors-config_1%3a3.6.0-7ubuntu1_all.deb ...\n",
      "Unpacking libsensors-config (1:3.6.0-7ubuntu1) ...\n",
      "Selecting previously unselected package libsensors5:amd64.\n",
      "Preparing to unpack .../118-libsensors5_1%3a3.6.0-7ubuntu1_amd64.deb ...\n",
      "Unpacking libsensors5:amd64 (1:3.6.0-7ubuntu1) ...\n",
      "Selecting previously unselected package libgl1-mesa-dri:amd64.\n",
      "Preparing to unpack .../119-libgl1-mesa-dri_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking libgl1-mesa-dri:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package libglx-mesa0:amd64.\n",
      "Preparing to unpack .../120-libglx-mesa0_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Unpacking libglx-mesa0:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package libglx0:amd64.\n",
      "Preparing to unpack .../121-libglx0_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglx0:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgl1:amd64.\n",
      "Preparing to unpack .../122-libgl1_1.4.0-1_amd64.deb ...\n",
      "Unpacking libgl1:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libiec61883-0:amd64.\n",
      "Preparing to unpack .../123-libiec61883-0_1.2.0-4build3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Unpacking libiec61883-0:amd64 (1.2.0-4build3) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libjack-jackd2-0:amd64.\n",
      "Preparing to unpack .../124-libjack-jackd2-0_1.9.20~dfsg-1_amd64.deb ...\n",
      "Unpacking libjack-jackd2-0:amd64 (1.9.20~dfsg-1) ...\n",
      "Selecting previously unselected package libopenal-data.\n",
      "Preparing to unpack .../125-libopenal-data_1%3a1.19.1-2build3_all.deb ...\n",
      "Unpacking libopenal-data (1:1.19.1-2build3) ...\n",
      "Selecting previously unselected package libsndio7.0:amd64.\n",
      "Preparing to unpack .../126-libsndio7.0_1.8.1-1.1_amd64.deb ...\n",
      "Unpacking libsndio7.0:amd64 (1.8.1-1.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libopenal1:amd64.\n",
      "Preparing to unpack .../127-libopenal1_1%3a1.19.1-2build3_amd64.deb ...\n",
      "Unpacking libopenal1:amd64 (1:1.19.1-2build3) ...\n",
      "Selecting previously unselected package libwayland-client0:amd64.\n",
      "Preparing to unpack .../128-libwayland-client0_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libwayland-client0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libdecor-0-0:amd64.\n",
      "Preparing to unpack .../129-libdecor-0-0_0.1.0-3build1_amd64.deb ...\n",
      "Unpacking libdecor-0-0:amd64 (0.1.0-3build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package libwayland-server0:amd64.\n",
      "Preparing to unpack .../130-libwayland-server0_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libwayland-server0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libgbm1:amd64.\n",
      "Preparing to unpack .../131-libgbm1_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking libgbm1:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package libwayland-cursor0:amd64.\n",
      "Preparing to unpack .../132-libwayland-cursor0_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libwayland-cursor0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libwayland-egl1:amd64.\n",
      "Preparing to unpack .../133-libwayland-egl1_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Unpacking libwayland-egl1:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libxcursor1:amd64.\n",
      "Preparing to unpack .../134-libxcursor1_1%3a1.2.0-2build4_amd64.deb ...\n",
      "Unpacking libxcursor1:amd64 (1:1.2.0-2build4) ...\n",
      "Selecting previously unselected package libxi6:amd64.\n",
      "Preparing to unpack .../135-libxi6_2%3a1.8-1build1_amd64.deb ...\n",
      "Unpacking libxi6:amd64 (2:1.8-1build1) ...\n",
      "Selecting previously unselected package libxinerama1:amd64.\n",
      "Preparing to unpack .../136-libxinerama1_2%3a1.1.4-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Unpacking libxinerama1:amd64 (2:1.1.4-3) ...\n",
      "Selecting previously unselected package libxkbcommon0:amd64.\n",
      "Preparing to unpack .../137-libxkbcommon0_1.4.0-1_amd64.deb ...\n",
      "Unpacking libxkbcommon0:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libxrandr2:amd64.\n",
      "Preparing to unpack .../138-libxrandr2_2%3a1.5.2-1build1_amd64.deb ...\n",
      "Unpacking libxrandr2:amd64 (2:1.5.2-1build1) ...\n",
      "Selecting previously unselected package x11-common.\n",
      "Preparing to unpack .../139-x11-common_1%3a7.7+23ubuntu2_all.deb ...\n",
      "Unpacking x11-common (1:7.7+23ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libxss1:amd64.\n",
      "Preparing to unpack .../140-libxss1_1%3a1.2.3-1build2_amd64.deb ...\n",
      "Unpacking libxss1:amd64 (1:1.2.3-1build2) ...\n",
      "Selecting previously unselected package libsdl2-2.0-0:amd64.\n",
      "Preparing to unpack .../141-libsdl2-2.0-0_2.0.20+dfsg-2ubuntu1.22.04.1_amd64.deb ...\n",
      "Unpacking libsdl2-2.0-0:amd64 (2.0.20+dfsg-2ubuntu1.22.04.1) ...\n",
      "Selecting previously unselected package libxcb-shape0:amd64.\n",
      "Preparing to unpack .../142-libxcb-shape0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-shape0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libxv1:amd64.\n",
      "Preparing to unpack .../143-libxv1_2%3a1.0.11-1build2_amd64.deb ...\n",
      "Unpacking libxv1:amd64 (2:1.0.11-1build2) ...\n",
      "Selecting previously unselected package libavdevice58:amd64.\n",
      "Preparing to unpack .../144-libavdevice58_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libavdevice58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package ffmpeg.\n",
      "Preparing to unpack .../145-ffmpeg_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking ffmpeg (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libigdgmm12:amd64.\n",
      "Preparing to unpack .../146-libigdgmm12_22.1.2+ds1-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking libigdgmm12:amd64 (22.1.2+ds1-1) ...\n",
      "Selecting previously unselected package intel-media-va-driver:amd64.\n",
      "Preparing to unpack .../147-intel-media-va-driver_22.3.1+dfsg1-1ubuntu2_amd64.deb ...\n",
      "Unpacking intel-media-va-driver:amd64 (22.3.1+dfsg1-1ubuntu2) ...\n",
      "Selecting previously unselected package libaacs0:amd64.\n",
      "Preparing to unpack .../148-libaacs0_0.11.1-1_amd64.deb ...\n",
      "Unpacking libaacs0:amd64 (0.11.1-1) ...\n",
      "Selecting previously unselected package libbdplus0:amd64.\n",
      "Preparing to unpack .../149-libbdplus0_0.2.0-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking libbdplus0:amd64 (0.2.0-1) ...\n",
      "Selecting previously unselected package libdecor-0-plugin-1-cairo:amd64.\n",
      "Preparing to unpack .../150-libdecor-0-plugin-1-cairo_0.1.0-3build1_amd64.deb ...\n",
      "Unpacking libdecor-0-plugin-1-cairo:amd64 (0.1.0-3build1) ...\n",
      "Selecting previously unselected package libpciaccess0:amd64.\n",
      "Preparing to unpack .../151-libpciaccess0_0.16-3_amd64.deb ...\n",
      "Unpacking libpciaccess0:amd64 (0.16-3) ...\n",
      "Selecting previously unselected package libdrm-intel1:amd64.\n",
      "Preparing to unpack .../152-libdrm-intel1_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-intel1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Selecting previously unselected package libgdk-pixbuf2.0-bin.\n",
      "Preparing to unpack .../153-libgdk-pixbuf2.0-bin_2.42.8+dfsg-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libgdk-pixbuf2.0-bin (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libgl1-amber-dri:amd64.\n",
      "Preparing to unpack .../154-libgl1-amber-dri_21.3.7-0ubuntu1_amd64.deb ...\n",
      "Unpacking libgl1-amber-dri:amd64 (21.3.7-0ubuntu1) ...\n",
      "Selecting previously unselected package librsvg2-common:amd64.\n",
      "Preparing to unpack .../155-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Selecting previously unselected package mesa-va-drivers:amd64.\n",
      "Preparing to unpack .../156-mesa-va-drivers_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking mesa-va-drivers:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package mesa-vdpau-drivers:amd64.\n",
      "Preparing to unpack .../157-mesa-vdpau-drivers_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking mesa-vdpau-drivers:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package i965-va-driver:amd64.\n",
      "Preparing to unpack .../158-i965-va-driver_2.4.1+dfsg1-1_amd64.deb ...\n",
      "Unpacking i965-va-driver:amd64 (2.4.1+dfsg1-1) ...\n",
      "Selecting previously unselected package va-driver-all:amd64.\n",
      "Preparing to unpack .../159-va-driver-all_2.14.0-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Unpacking va-driver-all:amd64 (2.14.0-1) ...\n",
      "Selecting previously unselected package vdpau-driver-all:amd64.\n",
      "Preparing to unpack .../160-vdpau-driver-all_1.4-3build2_amd64.deb ...\n",
      "Unpacking vdpau-driver-all:amd64 (1.4-3build2) ...\n",
      "Selecting previously unselected package pocketsphinx-en-us.\n",
      "Preparing to unpack .../161-pocketsphinx-en-us_0.8.0+real5prealpha+1-14ubuntu1_all.deb ...\n",
      "Unpacking pocketsphinx-en-us (0.8.0+real5prealpha+1-14ubuntu1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up libgme0:amd64 (0.6.3-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libssh-gcrypt-4:amd64 (0.9.6-2ubuntu0.22.04.1) ...\n",
      "Setting up libgraphite2-3:amd64 (1.3.14-1build2) ...\n",
      "Setting up libsrt1.4-gnutls:amd64 (1.4.4-4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libxcb-dri3-0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libpixman-1-0:amd64 (0.40.0-1ubuntu0.22.04.1) ...\n",
      "Setting up libudfread0:amd64 (1.1.2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libwayland-server0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Setting up libaom3:amd64 (3.3.0-1) ...\n",
      "Setting up libx11-xcb1:amd64 (2:1.7.5-1ubuntu0.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libpciaccess0:amd64 (0.16-3) ...\n",
      "Setting up librabbitmq4:amd64 (0.10.0-1ubuntu2) ...\n",
      "Setting up libraw1394-11:amd64 (2.1.2-2build2) ...\n",
      "Setting up libcodec2-1.0:amd64 (1.0.1-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libmpg123-0:amd64 (1.29.3-1build1) ...\n",
      "Setting up libxcb-xfixes0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libspeex1:amd64 (1.2~rc1.2-1.1ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libshine3:amd64 (3.1.1-2) ...\n",
      "Setting up libxi6:amd64 (2:1.8-1build1) ...\n",
      "Setting up libtwolame0:amd64 (0.4.0-2build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libxrender1:amd64 (1:0.9.10-1build4) ...\n",
      "Setting up libdatrie1:amd64 (0.2.13-2) ...\n",
      "Setting up libxcb-render0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libsoxr0:amd64 (0.1.3-4build2) ...\n",
      "Setting up libglvnd0:amd64 (1.4.0-1) ...\n",
      "Setting up libpgm-5.3-0:amd64 (5.3.128~dfsg-2) ...\n",
      "Setting up libxcb-glx0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libgdk-pixbuf2.0-common (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Setting up libnorm1:amd64 (1.5.9+dfsg-2) ...\n",
      "Setting up libmysofa1:amd64 (1.2.1~dfsg0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libxcb-shape0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up x11-common (1:7.7+23ubuntu2) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
      "debconf: falling back to frontend: Readline\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up libsensors-config (1:3.6.0-7ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libdeflate0:amd64 (1.10-2) ...\n",
      "Setting up xkb-data (2.33-1) ...\n",
      "Setting up libxcb-shm0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libigdgmm12:amd64 (22.1.2+ds1-1) ...\n",
      "Setting up libcdio19:amd64 (2.1.0-3build1) ...\n",
      "Setting up libxvidcore4:amd64 (2:1.3.7-1) ...\n",
      "Setting up libjbig0:amd64 (2.1-3.1ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libxxf86vm1:amd64 (1:1.1.4-1build3) ...\n",
      "Setting up libxcb-present0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libslang2:amd64 (2.3.2-5build4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libva2:amd64 (2.14.0-1) ...\n",
      "Setting up libfreetype6:amd64 (2.11.1+dfsg-1ubuntu0.2) ...\n",
      "Setting up libxfixes3:amd64 (1:6.0.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libxcb-sync1:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libx264-163:amd64 (2:0.163.3060+git5db6aa6-2build1) ...\n",
      "Setting up libfribidi0:amd64 (1.0.8-2ubuntu3.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libxinerama1:amd64 (2:1.1.4-3) ...\n",
      "Setting up intel-media-va-driver:amd64 (22.3.1+dfsg1-1ubuntu2) ...\n",
      "Setting up libxv1:amd64 (2:1.0.11-1build2) ...\n",
      "Setting up libxrandr2:amd64 (2:1.5.2-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up fonts-dejavu-core (2.37-2build1) ...\n",
      "Setting up libsensors5:amd64 (1:3.6.0-7ubuntu1) ...\n",
      "Setting up libaacs0:amd64 (0.11.1-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up pocketsphinx-en-us (0.8.0+real5prealpha+1-14ubuntu1) ...\n",
      "Setting up libglapi-mesa:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libx265-199:amd64 (3.5-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libwebp7:amd64 (1.2.2-2ubuntu0.22.04.1) ...\n",
      "Setting up libsndio7.0:amd64 (1.8.1-1.1) ...\n",
      "Setting up libxcb-dri2-0:amd64 (1.14-3ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libbdplus0:amd64 (0.2.0-1) ...\n",
      "Setting up libvidstab1.1:amd64 (1.1.0-2) ...\n",
      "Setting up libflite1:amd64 (2.2-3) ...\n",
      "Setting up ocl-icd-libopencl1:amd64 (2.2.14-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libasyncns0:amd64 (0.8-6build2) ...\n",
      "Setting up libxshmfence1:amd64 (1.3-1build4) ...\n",
      "Setting up libvdpau1:amd64 (1.4-3build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libbs2b0:amd64 (3.1.0+dfsg-2.2build1) ...\n",
      "Setting up libzimg2:amd64 (3.0.3+ds1-1) ...\n",
      "Setting up libopenjp2-7:amd64 (2.4.0-6) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libharfbuzz0b:amd64 (2.7.4-1ubuntu3.1) ...\n",
      "Setting up libopenal-data (1:1.19.1-2build3) ...\n",
      "Setting up libthai-data (0.1.29-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libvpx7:amd64 (1.11.0-2ubuntu2) ...\n",
      "Setting up libtiff5:amd64 (4.3.0-6ubuntu0.4) ...\n",
      "Setting up libwayland-egl1:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Setting up libxss1:amd64 (1:1.2.3-1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libusb-1.0-0:amd64 (2:1.0.25-1ubuntu2) ...\n",
      "Setting up libdav1d5:amd64 (0.9.2-1) ...\n",
      "Setting up libmfx1:amd64 (22.3.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libsamplerate0:amd64 (0.2.2-1build1) ...\n",
      "Setting up libwebpmux3:amd64 (1.2.2-2ubuntu0.22.04.1) ...\n",
      "Setting up libdrm-common (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libelf1:amd64 (0.186-1build1) ...\n",
      "Setting up libopenmpt0:amd64 (0.6.1-1) ...\n",
      "Setting up libzvbi-common (0.2.35-19) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libmp3lame0:amd64 (3.100-3build2) ...\n",
      "Setting up libicu70:amd64 (70.1-2) ...\n",
      "Setting up libiec61883-0:amd64 (1.2.0-4build3) ...\n",
      "Setting up libserd-0-0:amd64 (0.30.10-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libxkbcommon0:amd64 (1.4.0-1) ...\n",
      "Setting up libwayland-client0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Setting up libavc1394-0:amd64 (0.5.4-5build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up libzvbi0:amd64 (0.2.35-19) ...\n",
      "Setting up libzmq5:amd64 (4.3.4-2) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up libcaca0:amd64 (0.99.beta19-2.2ubuntu4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libpulse0:amd64 (1:15.99.1+dfsg1-1ubuntu2.1) ...\n",
      "Setting up libcdio-cdda2:amd64 (10.2+2.0.0-1build3) ...\n",
      "Setting up fontconfig-config (2.13.1-4.2ubuntu5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libcdio-paranoia2:amd64 (10.2+2.0.0-1build3) ...\n",
      "Setting up libxcursor1:amd64 (1:1.2.0-2build4) ...\n",
      "Setting up libopenal1:amd64 (1:1.19.1-2build3) ...\n",
      "Setting up libthai0:amd64 (0.1.29-1build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libdc1394-25:amd64 (2.2.6-4) ...\n",
      "Setting up librubberband2:amd64 (2.0.0-2) ...\n",
      "Setting up libjack-jackd2-0:amd64 (1.9.20~dfsg-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libdrm2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libva-drm2:amd64 (2.14.0-1) ...\n",
      "Setting up libsord-0-0:amd64 (0.16.8-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libwayland-cursor0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Setting up libsratom-0-0:amd64 (0.6.8-1) ...\n",
      "Setting up libdecor-0-0:amd64 (0.1.0-3build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up libfontconfig1:amd64 (2.13.1-4.2ubuntu5) ...\n",
      "Setting up libva-x11-2:amd64 (2.14.0-1) ...\n",
      "Setting up liblilv-0-0:amd64 (0.24.12-2) ...\n",
      "Setting up libxml2:amd64 (2.9.13+dfsg-1ubuntu0.3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up libdrm-amdgpu1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Regenerating fonts cache... done.\n",
      "Setting up libdrm-nouveau2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libgbm1:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libsphinxbase3:amd64 (0.8+5prealpha+1-13build1) ...\n",
      "Setting up libdrm-radeon1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libpango-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Setting up libdrm-intel1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libcairo2:amd64 (1.16.0-5ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libavutil56:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libpocketsphinx3:amd64 (0.8.0+real5prealpha+1-14ubuntu1) ...\n",
      "Setting up libass9:amd64 (1:0.15.2-1) ...\n",
      "Setting up shared-mime-info (2.1-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libpostproc55:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libllvm15:amd64 (1:15.0.7-0ubuntu0.22.04.3) ...\n",
      "Setting up libtheora0:amd64 (1.1.1+dfsg.1-15ubuntu4) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Setting up libswscale5:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libcairo-gobject2:amd64 (1.16.0-5ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up mesa-va-drivers:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libpangoft2-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Setting up libbluray2:amd64 (1:1.3.1-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up libsdl2-2.0-0:amd64 (2.0.20+dfsg-2ubuntu1.22.04.1) ...\n",
      "Setting up i965-va-driver:amd64 (2.4.1+dfsg1-1) ...\n",
      "Setting up libpangocairo-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Setting up libgl1-amber-dri:amd64 (21.3.7-0ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up mesa-vdpau-drivers:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libgl1-mesa-dri:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libswresample3:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up librsvg2-2:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "Setting up va-driver-all:amd64 (2.14.0-1) ...\n",
      "Setting up libdecor-0-plugin-1-cairo:amd64 (0.1.0-3build1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "Setting up vdpau-driver-all:amd64 (1.4-3build2) ...\n",
      "Setting up libgdk-pixbuf2.0-bin (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Setting up libavcodec58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 97%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up libchromaprint1:amd64 (1.5.1-2) ...\n",
      "Setting up libglx-mesa0:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libglx0:amd64 (1.4.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8Setting up libavformat58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libgl1:amd64 (1.4.0-1) ...\n",
      "Setting up libavfilter7:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libavdevice58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 99%]\u001b[49m\u001b[39m [#########################################################.] \u001b8Setting up ffmpeg (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf59750c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from llama.llama_adapter import LLaMA_adapter\n",
    "import util.misc as misc\n",
    "from datasets import load_from_disk, load_dataset, Dataset, disable_caching\n",
    "from llama import Tokenizer\n",
    "import llama\n",
    "from ImageBind.data import my_load_and_transform_audio_data\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655cde96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA-Adapter from /home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exp/first/checkpoint-3.pth\n",
      "model args: ModelArgs(dim=4096, n_layers=32, n_heads=32, vocab_size=-1, multiple_of=256, norm_eps=1e-06, max_batch_size=16, max_seq_len=512, w_bias=True, w_lora=True, lora_rank=16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLaMA_adapter(\n",
       "  (image_bind): ImageBindModel(\n",
       "    (modality_preprocessors): ModuleDict(\n",
       "      (vision): RGBDTPreprocessor(\n",
       "        (cls_token): tensor((1, 1, 1280), requires_grad=False)\n",
       "        \n",
       "        (rgbt_stem): PatchEmbedGeneric(\n",
       "          (proj): Sequential(\n",
       "            (0): PadIm2Video()\n",
       "            (1): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "          (pos_embed): tensor((1, 257, 1280), requires_grad=False)\n",
       "          \n",
       "        )\n",
       "      )\n",
       "      (text): TextPreprocessor(\n",
       "        (pos_embed): tensor((1, 77, 1024), requires_grad=False)\n",
       "        (mask): tensor((77, 77), requires_grad=False)\n",
       "        \n",
       "        (token_embedding): Embedding(49408, 1024)\n",
       "      )\n",
       "      (audio): AudioPreprocessor(\n",
       "        (cls_token): tensor((1, 1, 768), requires_grad=False)\n",
       "        \n",
       "        (rgbt_stem): PatchEmbedGeneric(\n",
       "          (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10), bias=False)\n",
       "          (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "          (pos_embed): tensor((1, 229, 768), requires_grad=False)\n",
       "          \n",
       "        )\n",
       "      )\n",
       "      (depth): RGBDTPreprocessor(\n",
       "        (cls_token): tensor((1, 1, 384), requires_grad=False)\n",
       "        \n",
       "        (depth_stem): PatchEmbedGeneric(\n",
       "          (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "          (norm_layer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "          (pos_embed): tensor((1, 197, 384), requires_grad=False)\n",
       "          \n",
       "        )\n",
       "      )\n",
       "      (thermal): ThermalPreprocessor(\n",
       "        (cls_token): tensor((1, 1, 768), requires_grad=False)\n",
       "        \n",
       "        (rgbt_stem): PatchEmbedGeneric(\n",
       "          (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "          (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "          (pos_embed): tensor((1, 197, 768), requires_grad=False)\n",
       "          \n",
       "        )\n",
       "      )\n",
       "      (imu): IMUPreprocessor(\n",
       "        (pos_embed): tensor((1, 251, 512), requires_grad=False)\n",
       "        (cls_token): tensor((1, 1, 512), requires_grad=False)\n",
       "        \n",
       "        (imu_stem): PatchEmbedGeneric(\n",
       "          (proj): Linear(in_features=48, out_features=512, bias=False)\n",
       "          (norm_layer): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (modality_trunks): ModuleDict(\n",
       "      (vision): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (12): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (13): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (14): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (15): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (16): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (17): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (18): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (19): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (20): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (21): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (22): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (23): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (24): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (25): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (26): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (27): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (28): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (29): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (30): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (31): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "      (text): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (12): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (13): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (14): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (15): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (16): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (17): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (18): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (19): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (20): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (21): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (22): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (23): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "      (audio): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.009)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.018)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.027)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.036)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.045)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.055)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.064)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.073)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.082)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.091)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "      (depth): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "      (thermal): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "      (imu): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.140)\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.280)\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.420)\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.560)\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.700)\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "    )\n",
       "    (modality_heads): ModuleDict(\n",
       "      (vision): Sequential(\n",
       "        (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): SelectElement()\n",
       "        (2): Linear(in_features=1280, out_features=1024, bias=False)\n",
       "      )\n",
       "      (text): SelectEOSAndProject(\n",
       "        (proj): Sequential(\n",
       "          (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (audio): Sequential(\n",
       "        (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): SelectElement()\n",
       "        (2): Linear(in_features=768, out_features=1024, bias=False)\n",
       "      )\n",
       "      (depth): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): SelectElement()\n",
       "        (2): Linear(in_features=384, out_features=1024, bias=False)\n",
       "      )\n",
       "      (thermal): Sequential(\n",
       "        (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): SelectElement()\n",
       "        (2): Linear(in_features=768, out_features=1024, bias=False)\n",
       "      )\n",
       "      (imu): Sequential(\n",
       "        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): SelectElement()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      )\n",
       "      (point): Sequential(\n",
       "        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (modality_postprocessors): ModuleDict(\n",
       "      (vision): Normalize()\n",
       "      (text): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n",
       "      )\n",
       "      (audio): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=20.0,learnable=False, max_logit_scale=100)\n",
       "      )\n",
       "      (depth): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
       "      )\n",
       "      (thermal): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=10.0,learnable=False, max_logit_scale=100)\n",
       "      )\n",
       "      (imu): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
       "      )\n",
       "      (point): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=1.0,learnable=False, max_logit_scale=100)\n",
       "      )\n",
       "    )\n",
       "    (point_trunk): PointTransformerBind(\n",
       "      (point_encoder): PointTransformer(\n",
       "        (group_divider): Group()\n",
       "        (encoder): Encoder(\n",
       "          (first_conv): Sequential(\n",
       "            (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (second_conv): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (reduce_dim): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (pos_embed): Sequential(\n",
       "          (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=384, bias=True)\n",
       "        )\n",
       "        (blocks): TransformerEncoder(\n",
       "          (blocks): ModuleList(\n",
       "            (0): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.018)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.027)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.036)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.045)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.055)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.064)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.073)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.082)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.091)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (image_bind_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (image_bind_norm_1): RMSNorm()\n",
       "  (image_bind_f1_1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (image_bind_f2_1): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "  (image_bind_f3_1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (image_bind_norm_2): RMSNorm()\n",
       "  (image_bind_f1_2): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (image_bind_f2_2): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "  (image_bind_f3_2): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (image_bind_norm_3): RMSNorm()\n",
       "  (image_bind_f1_3): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (image_bind_f2_3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "  (image_bind_f3_3): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (llama): Transformer(\n",
       "    (tok_embeddings): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (6): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (7): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (8): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (9): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (10): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (11): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (12): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (13): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (14): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (15): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (16): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (17): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (18): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (19): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (20): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (21): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (22): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (23): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (24): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (25): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (26): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (27): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (28): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (29): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (30): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (31): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "    (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       "  (prefix_query): Embedding(32, 4096)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_dir = \"/home/u8915687/lab/big-superb/Macaw-LLM2/weights/llama/\"\n",
    "pretrained_path = \"/home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exp/first/checkpoint-3.pth\"\n",
    "\n",
    "model = llama.load(pretrained_path, llama_dir, knn=True, max_batch_size=16)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f060140",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(model_path=\"/home/u8915687/lab/big-superb/Macaw-LLM2/weights/llama_7B/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8353487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e37947a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def forward_inference(self, visual_feats, tokens, start_pos: int):\n",
    "    _bsz, seqlen = tokens.shape\n",
    "    h = self.llama.tok_embeddings(tokens)\n",
    "    freqs_cis = self.llama.freqs_cis.to(h.device)\n",
    "    freqs_cis = freqs_cis[start_pos:start_pos + seqlen]\n",
    "    mask = None\n",
    "    mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=h.device)\n",
    "    mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "\n",
    "    for layer in self.llama.layers[:-1 * self.query_layer]:\n",
    "        h = layer(h, start_pos, freqs_cis, mask)\n",
    "    prefix_query = self.prefix_query.weight.reshape(\n",
    "        self.query_layer, 1, 4096).unsqueeze(1)\n",
    "    prefix_index = 0\n",
    "    \n",
    "    visual_proj = []\n",
    "    for i in range(_bsz):\n",
    "        visual_proj.append(\n",
    "            visual_feats[i, :, i, :]\n",
    "        )\n",
    "    visual_proj = torch.stack(visual_proj)\n",
    "#     print(visual_proj)\n",
    "        # B, 1, D\n",
    "    for layer in self.llama.layers[-1 * self.query_layer:]:\n",
    "        h = layer(h, start_pos, freqs_cis, mask, visual_proj + prefix_query[prefix_index].repeat(_bsz, 1, 1))\n",
    "        prefix_index = prefix_index + 1\n",
    "\n",
    "    h = self.llama.norm(h)\n",
    "    output = self.llama.output(h[:, -1, :])\n",
    "\n",
    "    return output.float()\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def my_generate(\n",
    "        self,\n",
    "        inputs,\n",
    "        prompts,\n",
    "        max_gen_len: int = 256,\n",
    "        temperature: float = 0.1,\n",
    "        top_p: float = 0.75,\n",
    "        cache_size=10,\n",
    "        cache_t=20,\n",
    "        cache_weight=0.5\n",
    "):\n",
    "    bsz = len(prompts)\n",
    "    params = self.llama.params\n",
    "    assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        visual_query = self.forward_visual(inputs, cache_size, cache_t, cache_weight)\n",
    "\n",
    "    if isinstance(prompts[0], str):\n",
    "        prompts = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "\n",
    "    min_prompt_size = min([len(t) for t in prompts])\n",
    "    max_prompt_size = max([len(t) for t in prompts])\n",
    "\n",
    "    total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n",
    "\n",
    "    tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()\n",
    "    \n",
    "    reach_eos = torch.zeros(bsz, dtype=torch.bool)\n",
    "    for k, t in enumerate(prompts):\n",
    "        tokens[k, : len(t)] = torch.tensor(t).cuda().long()\n",
    "    input_text_mask = tokens != self.tokenizer.pad_id\n",
    "    start_pos = min_prompt_size\n",
    "    prev_pos = 0\n",
    "    for cur_pos in range(start_pos, total_len):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = forward_inference(model, visual_query, tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "            next_token = sample_top_p(probs, top_p)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "        next_token = next_token.reshape(-1)\n",
    "\n",
    "        next_token = torch.where(\n",
    "            input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "        )\n",
    "        tokens[:, cur_pos] = next_token\n",
    "        # trick: early stop if bsz==1\n",
    "        \n",
    "\n",
    "        for i in range(bsz):\n",
    "            if next_token[i] == self.tokenizer.eos_id:\n",
    "                reach_eos[i] = True\n",
    "        if (bsz == 1 and next_token[0] == self.tokenizer.eos_id) or (all(reach_eos)):\n",
    "            break\n",
    "        \n",
    "    \n",
    "        prev_pos = cur_pos\n",
    "\n",
    "    decoded = []\n",
    "    for i, t in enumerate(tokens.tolist()):\n",
    "\n",
    "        # cut to max gen len\n",
    "        t = t[len(prompts[i]): len(prompts[i]) + max_gen_len]\n",
    "        # cut to eos tok if any\n",
    "        try:\n",
    "            t = t[: t.index(self.tokenizer.eos_id)]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        decoded.append(self.tokenizer.decode(t))\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5a977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(b):\n",
    "    batch = {}\n",
    "    \n",
    "    audios = []\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    for data in b:\n",
    "        audio = my_load_and_transform_audio_data(\n",
    "                        torch.tensor(data[\"audio\"][\"array\"], dtype=torch.float32\n",
    "                    ).unsqueeze(0))[0]\n",
    "        audios.append(audio)\n",
    "        text = data.get(\"text\").lower() if data.get(\"text\") else None\n",
    "        instruction = data[\"instruction\"].lower()\n",
    "        \n",
    "        prompts.append(llama.format_prompt(instruction, text))\n",
    "        labels.append(data[\"label\"])\n",
    "    \n",
    "    batch[\"audio\"] = torch.stack(audios)\n",
    "    batch[\"prompts\"] = prompts\n",
    "    batch[\"instructions\"] = [d[\"instruction\"] for d in b]\n",
    "    batch[\"labels\"] = [d[\"label\"] for d in b]\n",
    "    return batch\n",
    "\n",
    "DATA_PATH = Path(\"/work/u8915687/big-superb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2d457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e3c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ebfcbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\n",
    "#     'SpeechBigBench/NoiseDetectionspeech_LJSpeechMusan',\n",
    "    'SpeechBigBench/AccentClassification_AccentdbExtended', 'SpeechBigBench/BirdSoundDetection_Warblrb10k', 'SpeechBigBench/ChordClassification_AcousticGuitarAndPiano', 'SpeechBigBench/Deeply_Parent_Child_Vocal_Interaction', 'SpeechBigBench/DialogueActClassification_DailyTalk', 'SpeechBigBench/DialogueEmotionClassification_DailyTalk', 'SpeechBigBench/EmotionRecognition_MultimodalEmotionlinesDataset', 'SpeechBigBench/EnhancementDetection_LibrittsTestCleanWham', 'SpeechBigBench/EnvironmentalSoundClassification_AnimalsESC50', 'SpeechBigBench/EnvironmentalSoundClassification_ExteriorAndUrbanNoisesESC50', 'SpeechBigBench/EnvironmentalSoundClassification_HumanAndNonSpeechSoundsESC50', 'SpeechBigBench/EnvironmentalSoundClassification_InteriorAndDomesticSoundsESC50', 'SpeechBigBench/EnvironmentalSoundClassification_NaturalSoundscapesAndWaterSoundsESC50', 'SpeechBigBench/HowFarAreYou_3DSpeaker', 'SpeechBigBench/IntentClassification_FluentSpeechCommands', 'SpeechBigBench/Korean_Read_Speech_Corpus', 'SpeechBigBench/LanguageIdentification_VoxForge', 'SpeechBigBench/NoiseDetectiongaussian_LJSpeechMusan', 'SpeechBigBench/NoiseDetectiongaussian_VCTKMusan', 'SpeechBigBench/NoiseDetectionmusic_LJSpeechMusan', 'SpeechBigBench/NoiseDetectionmusic_VCTKMusan', 'SpeechBigBench/NoiseDetectionnoise_LJSpeechMusan', 'SpeechBigBench/NoiseDetectionnoise_VCTKMusan', 'SpeechBigBench/NoiseDetectionspeech_VCTKMusan', 'SpeechBigBench/NoiseSNRLevelPredictiongaussian_VCTKMusan', 'SpeechBigBench/NoiseSNRLevelPredictionmusic_VCTKMusan', 'SpeechBigBench/NoiseSNRLevelPredictionnoise_VCTKMusan', 'SpeechBigBench/NoiseSNRLevelPredictionspeech_VCTKMusan', 'SpeechBigBench/Nonverbal_Vocalization', 'SpeechBigBench/PronounciationEvaluationAccuracy_Speechocean762', 'SpeechBigBench/PronounciationEvaluationFluency_Speechocean762', 'SpeechBigBench/PronounciationEvaluationOverall_Speechocean762', 'SpeechBigBench/PronounciationEvaluationProsodic_Speechocean762', 'SpeechBigBench/ReverberationDetectionlargeroom_LJSpeechRirsNoises', 'SpeechBigBench/ReverberationDetectionlargeroom_VCTKRirsNoises', 'SpeechBigBench/ReverberationDetectionmediumroom_LJSpeechRirsNoises', 'SpeechBigBench/ReverberationDetectionmediumroom_VCTKRirsNoises', 'SpeechBigBench/ReverberationDetectionsmallroom_LJSpeechRirsNoises', 'SpeechBigBench/ReverberationDetectionsmallroom_VCTKRirsNoises', 'SpeechBigBench/SarcasmDetection_Mustard', 'SpeechBigBench/SpeakerCounting_LibriTTSTestClean', 'SpeechBigBench/SpeechCommandRecognition_GoogleSpeechCommandsV1', 'SpeechBigBench/SpeechDetection_LJSpeech', 'SpeechBigBench/SpeechDetection_LibriSpeechTestClean', 'SpeechBigBench/SpeechDetection_LibriSpeechTestOther', 'SpeechBigBench/SpeechTextMatching_LJSpeech', 'SpeechBigBench/SpeechTextMatching_LibriSpeechTestClean', 'SpeechBigBench/SpeechTextMatching_LibriSpeechTestOther', 'SpeechBigBench/SpokenTermDetection_LJSpeech', 'SpeechBigBench/SpokenTermDetection_LibriSpeechTestClean', 'SpeechBigBench/SpokenTermDetection_LibriSpeechTestOther', 'SpeechBigBench/SpoofDetection_ASVspoof2015', 'SpeechBigBench/SpoofDetection_ASVspoof2017', 'SpeechBigBench/StressDetection_MIRSD', 'SpeechBigBench/arabic_speech_corpus', 'SpeechBigBench/speech_commands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eca7b4ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechBigBench/AccentClassification_AccentdbExtended\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 88/1083 [01:57<22:05,  1.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[1;32m     55\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: [batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     57\u001b[0m     }\n\u001b[0;32m---> 58\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmy_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r,l, ins \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m     69\u001b[0m         r \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 73\u001b[0m, in \u001b[0;36mmy_generate\u001b[0;34m(self, inputs, prompts, max_gen_len, temperature, top_p, cache_size, cache_t, cache_weight)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_pos, total_len):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 73\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[43mforward_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcur_pos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     75\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits \u001b[38;5;241m/\u001b[39m temperature, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m, in \u001b[0;36mforward_inference\u001b[0;34m(self, visual_feats, tokens, start_pos)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     print(visual_proj)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# B, 1, D\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_layer:]:\n\u001b[0;32m---> 27\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_proj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprefix_query\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprefix_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_bsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m         prefix_index \u001b[38;5;241m=\u001b[39m prefix_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     30\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllama\u001b[38;5;241m.\u001b[39mnorm(h)\n",
      "File \u001b[0;32m~/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/lab/big-superb/LLaMA-Adapter/imagebind_LLM/llama/llama.py:273\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, start_pos, freqs_cis, mask, prompt)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, start_pos: \u001b[38;5;28mint\u001b[39m, freqs_cis: torch\u001b[38;5;241m.\u001b[39mTensor, mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor], prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 273\u001b[0m     h \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, start_pos, freqs_cis, mask, prompt)\n\u001b[1;32m    274\u001b[0m     out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(h))\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/lab/big-superb/LLaMA-Adapter/imagebind_LLM/llama/llama.py:41\u001b[0m, in \u001b[0;36mRMSNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 41\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype_as(x)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\n",
      "File \u001b[0;32m~/lab/big-superb/LLaMA-Adapter/imagebind_LLM/llama/llama.py:38\u001b[0m, in \u001b[0;36mRMSNorm._norm\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DATA_PATH = Path(\"/work/u8915687/big-superb\")\n",
    "ROOT_PATH = Path(\"/home/u8915687/lab/big-superb/LLaMA-Adapter/imagebind_LLM\")\n",
    "EXP_PATH = ROOT_PATH/ \"exp/first\"\n",
    "RESULT_PATH = EXP_PATH/\"results3_test\"\n",
    "RESULT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for dataset_name in all_datasets:\n",
    "    print(dataset_name)\n",
    "    task_name = dataset_name.split(\"/\")[-1]\n",
    "    \n",
    "    dataset = load_from_disk(\n",
    "        DATA_PATH/dataset_name\n",
    "    )\n",
    "    \n",
    "    if not dataset.get(\"test\"):\n",
    "        print(f\"Error {dataset_name}\")\n",
    "        continue\n",
    "        \n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset[\"test\"],\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    correct = 0\n",
    "    count = 0\n",
    "    predictions = []\n",
    "    label_count = {}\n",
    "    for batch in tqdm(data_loader):\n",
    "        inputs = {\n",
    "            \"Audio\": [batch[\"audio\"].to(\"cuda\"), 1]\n",
    "        }\n",
    "        results = my_generate(\n",
    "            model,\n",
    "            inputs,\n",
    "            batch[\"prompts\"],\n",
    "            max_gen_len=150,\n",
    "            temperature=0,\n",
    "            top_p=0\n",
    "        )\n",
    "\n",
    "        \n",
    "        for r,l, ins in zip(results, batch[\"labels\"], batch[\"instructions\"]):\n",
    "            r = r.strip()\n",
    "            if r == l:\n",
    "                correct += 1\n",
    "            count += 1\n",
    "            \n",
    "            label_count[l] = label_count.get(l, 0) + 1\n",
    "            predictions.append({\n",
    "                \"pred\": r,\n",
    "                \"label\": l,\n",
    "                \"instruction\": ins\n",
    "            })\n",
    "    print(correct, count)\n",
    "    print(f\"{c} / {len(predictions)}\")\n",
    "    json.dump(\n",
    "        {\n",
    "            \"predictions\": predictions,\n",
    "            \"label_count\": label_count,\n",
    "            \"accuracy\": correct / len(predictions)\n",
    "        },\n",
    "        (RESULT_PATH/f\"{task_name}.json\").open(\"w\"), indent=2\n",
    "    )\n",
    "    print(\"=\"*100)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4874cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8781a98",
   "metadata": {},
   "source": [
    "## Single inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f7d0ef3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:23<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine whether the given audio clip contains real speech or not. The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "0 500\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# task = \"BigSuperbPrivate/NoiseDetectionSpeech_VoxcelebMusan\"\n",
    "task = \"SpeechBigBench/NoiseSNRLevelPredictiongaussian_VCTKMusan\"\n",
    "\n",
    "dataset = load_from_disk(\n",
    "    DATA_PATH/task\n",
    ")\n",
    "dataset = dataset[\"test\"].shuffle(42)[:500]\n",
    "dataset = Dataset.from_dict(dataset)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "correct = 0\n",
    "count = 0\n",
    "predictions = []\n",
    "label_count = {}\n",
    "\n",
    "\n",
    "\n",
    "for batch in tqdm(data_loader):\n",
    "    inputs = {\n",
    "        \"Audio\": [batch[\"audio\"].to(\"cuda\"), 1]\n",
    "    }\n",
    "#     prompts = batch[\"prompts\"]\n",
    "#     prompts = [p.replace(\n",
    "#         \"The answer could be clean or noisy.\".lower(), \n",
    "#         \"The answer could be yes or no.\".lower()\n",
    "#     ) for p in batch[\"prompts\"]]\n",
    "    prompts = [llama.format_prompt(\"Determine whether the given audio clip contains real speech or not. The answer could be yes or no.\") for p in batch[\"prompts\"]]\n",
    "\n",
    "    results = my_generate(\n",
    "        model,\n",
    "        inputs,\n",
    "        prompts,\n",
    "        max_gen_len=150,\n",
    "        temperature=0,\n",
    "        top_p=0\n",
    "    )\n",
    "\n",
    "\n",
    "    for r,l, ins in zip(results, batch[\"labels\"], batch[\"instructions\"]):\n",
    "        r = r.strip()\n",
    "        if r == l:\n",
    "            correct += 1\n",
    "        count += 1\n",
    "\n",
    "        label_count[l] = label_count.get(l, 0) + 1\n",
    "        predictions.append({\n",
    "            \"pred\": r,\n",
    "            \"label\": l,\n",
    "            \"instruction\": ins\n",
    "        })\n",
    "\n",
    "# random score\n",
    "\n",
    "        \n",
    "print(prompts[0])\n",
    "print(correct, count)\n",
    "print(correct/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c38c6893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.176\n",
      "[('clean', {'yes': 82, 'no': 3}), ('fifteen', {'yes': 56, 'no': 53}), ('five', {'no': 83, 'yes': 24}), ('ten', {'no': 64, 'yes': 54}), ('zero', {'no': 77, 'yes': 4})]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "matrix = defaultdict(dict)\n",
    "\n",
    "all_labels = [p[\"label\"] for p in predictions]\n",
    "random.seed(42)\n",
    "random_count = 0\n",
    "\n",
    "for p in predictions:\n",
    "    \n",
    "    matrix[p[\"label\"]][p[\"pred\"]] = matrix[p[\"label\"]].get(p[\"pred\"], 0) + 1\n",
    "    if p[\"label\"] == random.choice(all_labels):\n",
    "        random_count+=1\n",
    "print(random_count / len(predictions))\n",
    "print(sorted(matrix.items(), key=lambda x:x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2cba4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_matrix = defaultdict(dict)\n",
    "# for k,v in matrix.items():\n",
    "#     for kk,vv in v.items():\n",
    "#         if vv >= 2:\n",
    "#             new_matrix[k][kk] = vv\n",
    "# matrix = new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0a041b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred\\label | clean | fifteen | five | ten | zero | \n",
      "|---|---|---|---|---|\n",
      "|no | 3 | 53 | 83 | 64 | 77 | \n",
      "|yes | 82 | 56 | 24 | 54 | 4 | \n"
     ]
    }
   ],
   "source": [
    "row = set()\n",
    "column = set()\n",
    "for k,v in matrix.items():\n",
    "    row = row.union({k})\n",
    "    column = column.union(set(v.keys()))\n",
    "    \n",
    "row = list(sorted(row))\n",
    "column = list(sorted(column))\n",
    "\n",
    "print(\"pred\\label\", end=\" | \")\n",
    "for j in range(len(row)):\n",
    "    print(row[j], end=\" | \")\n",
    "print()\n",
    "print(end=\"|\")\n",
    "for j in range(len(row)):\n",
    "    print(\"---\", end=\"|\")\n",
    "print()\n",
    "\n",
    "for i in range(len(column)):\n",
    "    \n",
    "    c = column[i]\n",
    "    print(end=\"|\")\n",
    "    print(c, end=\" | \")\n",
    "        \n",
    "    for j in range(len(row)):\n",
    "        r = row[j]\n",
    "        \n",
    "        print(matrix[r].get(c, 0), end=\" | \")\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751e6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43fb6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f69a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661ff8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5c7f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "audios = []\n",
    "prompts = []\n",
    "s1 = time.time()\n",
    "for i in range(4):\n",
    "    audio = my_load_and_transform_audio_data(torch.tensor(dataset[i][\"audio\"][\"array\"], dtype=torch.float32).unsqueeze(0))[0].unsqueeze(0)\n",
    "    audio = audio.to(\"cuda\")\n",
    "    audios.append(audio)\n",
    "    \n",
    "    prompts.append(llama.format_prompt(dataset[i][\"instruction\"]))\n",
    "\n",
    "    print(dataset[i][\"instruction\"])\n",
    "    print(\"{{\",dataset[i][\"label\"],\"}}\")\n",
    "    inputs = {}\n",
    "    inputs[\"Audio\"] = [audio, 1]\n",
    "    results = my_generate(\n",
    "        model,\n",
    "        inputs,\n",
    "        [prompts[0]],\n",
    "        max_gen_len=150,\n",
    "        temperature=0,\n",
    "        top_p=0\n",
    "    )\n",
    "    print(results[0])\n",
    "s2 = time.time()\n",
    "inputs[\"Audio\"] = [torch.cat(audios, dim=0).to(\"cuda\"), 1]\n",
    "#     inputs[\"Audio\"] = [audio, 1]\n",
    "\n",
    "print(s2-s1)\n",
    "results = my_generate(\n",
    "    model,\n",
    "    inputs,\n",
    "    prompts,\n",
    "    max_gen_len=150,\n",
    "    temperature=0,\n",
    "    top_p=0\n",
    ")\n",
    "print(results)\n",
    "print(time.time()-s2)\n",
    "#     result = results[0].strip()\n",
    "#     print(result)\n",
    "#     print(f'{{{dataset[i][\"label\"]}}}')\n",
    "#     print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06990fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb024df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d7b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b2fdd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = model.forward_visual(\n",
    "    {\"audio\":[torch.cat(audios[0:1], dim=0).to(\"cuda\"), 1]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5e836e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = model.forward_visual(\n",
    "    {\"audio\":[torch.cat(audios[1:2], dim=0).to(\"cuda\"), 1]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "228ca1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-15.7063,   1.7233,  -3.5650,  ...,  -0.9190,  -3.5915,   0.2461]]]],\n",
       "        device='cuda:0'),\n",
       " tensor([[[[-20.4146,   4.7461,   3.2833,  ..., -10.5704,   2.3532,  -3.9585]]]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1, f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d952e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = model.forward_visual(\n",
    "    {\"audio\":[torch.cat(audios[0:4], dim=0).to(\"cuda\"), 1]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "809c29a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-15.7063,   1.7233,  -3.5650,  ...,  -0.9190,  -3.5915,   0.2461]],\n",
       "\n",
       "        [[-20.4146,   4.7461,   3.2833,  ..., -10.5704,   2.3532,  -3.9585]],\n",
       "\n",
       "        [[-12.8722,  -4.6297,   2.0977,  ..., -12.4727,   3.0803,   9.3993]],\n",
       "\n",
       "        [[ -8.4097,  -4.4047,   3.7414,  ...,  -1.2437,   6.6060,   4.4494]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = []\n",
    "for i in range(4):\n",
    "    ff.append(f[i, :, i, :])\n",
    "torch.stack(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072022f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e068b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16124990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b797cf83",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "no\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "no\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "no\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "no\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{clean}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the recording contain recognizable human speech?? The answer could be yes or no.\n",
      "\n",
      "### Response:\n",
      "yes\n",
      "{noisy}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in range(len(dataset)):\n",
    "    batch = dataset[i]\n",
    "    inputs = {}\n",
    "    audio = my_load_and_transform_audio_data(torch.tensor(batch[\"audio\"][\"array\"], dtype=torch.float32).unsqueeze(0))[0].unsqueeze(0)\n",
    "    audio = audio.to(\"cuda\")\n",
    "    inputs[\"Audio\"] = [audio, 1]\n",
    "    \n",
    "    \n",
    "#     prompt = llama.format_prompt(batch[\"instruction\"])\n",
    "    prompt = llama.format_prompt(\"Does the recording contain recognizable human speech?? The answer could be yes or no.\")\n",
    "    print(prompt)\n",
    "    \n",
    "    results = model.generate(\n",
    "        inputs,\n",
    "        [prompt],\n",
    "        max_gen_len=150,\n",
    "        temperature=0,\n",
    "        top_p=0\n",
    "    )\n",
    "    result = results[0].strip()\n",
    "    print(result)\n",
    "    print(f'{{{batch[\"label\"]}}}')\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    predictions.append({\n",
    "        \"pred\": result,\n",
    "        \"label\": batch[\"label\"],\n",
    "        \"instruction\": batch[\"instruction\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0fe96b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 / 30\n",
      "{'clean': 11, 'noisy': 19}\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "label_count = {}\n",
    "for r in predictions:\n",
    "    if r[\"pred\"] == \"yes\":\n",
    "        r[\"pred\"] = \"clean\"\n",
    "    if r[\"pred\"] == \"no\":\n",
    "        r[\"pred\"] = \"noisy\"\n",
    "        \n",
    "    if r[\"pred\"] == r[\"label\"]:\n",
    "        c += 1\n",
    "    \n",
    "    label_count[r[\"label\"]] = label_count.get(r[\"label\"], 0) + 1\n",
    "print(c, \"/\", len(predictions))\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "622e9cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e93b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da11277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690012f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1876fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7befe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Audio\n",
    "\n",
    "d = load_from_disk(\"/work/u8915687/big-superb/BigSuperbPrivate/SpeakerVerification_Tedlium2Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "186f0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.cast_column(\"audio\", Audio(decode=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682d459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagebind_LLM",
   "language": "python",
   "name": "imagebind_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
