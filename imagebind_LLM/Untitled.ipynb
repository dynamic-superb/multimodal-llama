{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae404b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.misc as misc\n",
    "import torch\n",
    "from llama.llama_adapter import LLaMA_adapter\n",
    "import llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97578227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA-Adapter from /home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exp/whisper_debug/checkpoint-1.pth\n",
      "model args: ModelArgs(dim=4096, n_layers=32, n_heads=32, vocab_size=-1, multiple_of=256, norm_eps=1e-06, max_batch_size=16, max_seq_len=512, w_bias=True, w_lora=True, lora_rank=16)\n"
     ]
    }
   ],
   "source": [
    "model = llama.whisper_llama_adapter.load(\"/home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exp/whisper_debug/checkpoint-1.pth\", \"/home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/ckpts/llama\", knn=True, max_batch_size=16)\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c48901cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA-Adapter from /home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exp/whisper_debug/model-1.pth\n",
      "model args: ModelArgs(dim=4096, n_layers=32, n_heads=32, vocab_size=-1, multiple_of=256, norm_eps=1e-06, max_batch_size=16, max_seq_len=512, w_bias=True, w_lora=True, lora_rank=16)\n"
     ]
    }
   ],
   "source": [
    "model2 = llama.whisper_llama_adapter.load(\"/home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exp/whisper_debug/model-1.pth\", \"/home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/ckpts/llama\", knn=True, max_batch_size=16)\n",
    "model2 = model2.to(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a363f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name in model.state_dict().keys():\n",
    "    if not torch.all(model.state_dict()[name] == model2.state_dict()[name]):\n",
    "        print(\n",
    "            name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a355f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc15bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5d65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94545b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109a77eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "Load checkpoint /home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exp/new_train2/checkpoint-3.pth\n"
     ]
    }
   ],
   "source": [
    "misc.load_model(model, \"/home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exp/new_train2/checkpoint-3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_paths = [output_dir / ('model-%s.pth' % epoch_name)]\n",
    "for checkpoint_path in checkpoint_paths:\n",
    "\n",
    "    model_state_dict = model_without_ddp.state_dict()\n",
    "    model_state_dict_with_grad = OrderedDict()\n",
    "    for key, val in model_without_ddp.named_parameters():\n",
    "        if val.requires_grad:\n",
    "            model_state_dict_with_grad[key] = model_state_dict[key]\n",
    "\n",
    "    to_save = {\n",
    "        'model': model_state_dict_with_grad,\n",
    "        # 'optimizer': optimizer.state_dict(),\n",
    "        # 'epoch': epoch,\n",
    "        # 'scaler': loss_scaler.state_dict(),\n",
    "        'args': args,\n",
    "    }\n",
    "\n",
    "    save_on_master(to_save, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ff4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque, OrderedDict\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "model_state_dict_with_grad = OrderedDict()\n",
    "for key, val in model.named_parameters():\n",
    "    if val.requires_grad:\n",
    "        model_state_dict_with_grad[key] = model_state_dict[key]\n",
    "to_save = {\n",
    "        'model': model_state_dict_with_grad,\n",
    "        # 'optimizer': optimizer.state_dict(),\n",
    "        # 'epoch': epoch,\n",
    "        # 'scaler': loss_scaler.state_dict(),\n",
    "        # 'args': args,\n",
    "    }\n",
    "torch.save(to_save, \"/home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exp/new_train2/model-3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6e85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ee4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83480c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e068946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u2619111/.local/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/u2619111/.local/lib/python3.9/site-packages/whisper/timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n"
     ]
    }
   ],
   "source": [
    "# Hank\n",
    "import torch\n",
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import llama.utils\n",
    "from llama import Tokenizer\n",
    "import copy\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import random\n",
    "from pytorchvideo.data.clip_sampling import ConstantClipsPerVideoSampler\n",
    "import torchaudio\n",
    "import whisper\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "class BigSuperbDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, data_path2=None, max_length=128, used_data_split=\"train\", phase=\"train\", audio_input_type=\"imagebind\", allowed_datasets=[]):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.datas = []\n",
    "        self.used_datasets = []\n",
    "        self.used_data_split = used_data_split\n",
    "        self.audio_input_type = audio_input_type\n",
    "        self.allowed_datasets = allowed_datasets\n",
    "        self.phase = phase\n",
    "\n",
    "        for task_path in data_path.iterdir():\n",
    "            if not self.check_allowed_datasets(task_path):\n",
    "                continue\n",
    "            \n",
    "            self.used_datasets.append(task_path.stem)\n",
    "            \n",
    "            for data_split in task_path.iterdir():\n",
    "                if data_split.stem != self.used_data_split:\n",
    "                    continue\n",
    "                \n",
    "                json_data = json.load((data_split/\"metadata.json\").open())\n",
    "                for filename, d in json_data.items():\n",
    "                    \n",
    "                    d[\"file\"] = f\"{data_split}/{filename}\"\n",
    "\n",
    "                    if d.get(\"file2\"):\n",
    "                        filename2 = filename.replace(\".wav\", \"_pair.wav\").replace(\".flac\", \"_pair.flac\")\n",
    "                        d[\"file2\"] = f\"{data_split}/{filename2}\"\n",
    "                        # assert Path(d[\"file2\"]).exists(), d[\"file2\"]\n",
    "                        # if \".\" not in d[\"file2\"]:\n",
    "                        #     d[\"file2\"] = d[\"file2\"] + \".wav\"\n",
    "                        \n",
    "                        # if (data_split/f\"{task_path.stem}_{self.used_data_split}_{d['file2']}\").exists():\n",
    "                        #     d[\"file2\"] = f\"{data_split}/{task_path.stem}_{self.used_data_split}_{d['file2']}\"\n",
    "                        # elif (task_path/\"missing_files\"/d[\"file2\"]).exists():\n",
    "                        #     d[\"file2\"] = str(task_path/\"missing_files\"/d[\"file2\"])\n",
    "                        # else:\n",
    "                        #     assert False, f\"{task_path}\"+ d[\"file2\"]\n",
    "                            \n",
    "                    self.datas.append(d)\n",
    "        # exclude\n",
    "        if data_path2 is not None:\n",
    "            for task_path in data_path2.iterdir():\n",
    "                if not self.check_allowed_datasets(task_path):\n",
    "                    continue\n",
    "                \n",
    "                self.used_datasets.append(task_path.stem)\n",
    "                for data_split in task_path.iterdir():\n",
    "                    if data_split.stem != self.used_data_split:\n",
    "                        continue\n",
    "                    \n",
    "                    json_data = json.load((data_split/\"metadata.json\").open())\n",
    "                    for file_name, d in json_data.items():\n",
    "                        d[\"file\"] = str(data_split/file_name)\n",
    "                                \n",
    "                        self.datas.append(d)\n",
    "\n",
    "\n",
    "        # Audio loader\n",
    "        self.clip_sampler = ConstantClipsPerVideoSampler(\n",
    "            clip_duration=2, clips_per_video=3\n",
    "        )\n",
    "        print(\"Used datasets\", len(self.used_datasets), self.used_datasets)\n",
    "        print(len(self.datas))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.datas[idx]\n",
    "        new_data = {}\n",
    "        \n",
    "        # Text \n",
    "        if self.used_data_split == \"train\":\n",
    "            instruction = data[\"instruction\"].lower()\n",
    "            text = data[\"text\"].lower() if data.get(\"text\") else None\n",
    "            input1 = llama.format_prompt(instruction, text)\n",
    "            input2 = input1 + data[\"label\"]\n",
    "\n",
    "            input1 = torch.tensor(\n",
    "                self.tokenizer.encode(input1, bos=True, eos=False), dtype=torch.long\n",
    "            )\n",
    "            input2 = torch.tensor(self.tokenizer.encode(input2, bos=True, eos=True), dtype=torch.long)\n",
    "            padding = self.max_length - input2.size(0)\n",
    "            if padding > 0:\n",
    "                input2 = torch.cat((input2, torch.zeros(padding, dtype=torch.long) - 1))\n",
    "            else:\n",
    "                input2 = input2[:self.max_length]\n",
    "\n",
    "            labels = copy.deepcopy(input2)\n",
    "            labels[:input1.size(0)] = -1\n",
    "\n",
    "            input2_mask = input2.ge(0)\n",
    "            label_mask = labels.ge(0)\n",
    "            input2[~input2_mask] = 0\n",
    "            labels[~label_mask] = 0\n",
    "\n",
    "            input2_mask = input2_mask.float()\n",
    "            label_mask = label_mask.float()\n",
    "\n",
    "            new_data[\"instruction\"] = data[\"instruction\"]\n",
    "            new_data[\"input_ids\"] = input2\n",
    "            new_data[\"labels\"] = labels\n",
    "            new_data[\"input_mask\"] = input2_mask\n",
    "\n",
    "            # for inference training set\n",
    "            if self.phase == \"inference\":\n",
    "                instruction = data[\"instruction\"].lower()\n",
    "                text = data[\"text\"].lower() if data.get(\"text\") else None\n",
    "                input1 = llama.format_prompt(instruction, text)\n",
    "\n",
    "                new_data[\"prompt\"] = input1\n",
    "                new_data[\"instruction\"] = data[\"instruction\"]\n",
    "                new_data[\"label\"] = data[\"label\"]\n",
    "        elif self.used_data_split == \"test\":\n",
    "            instruction = data[\"instruction\"].lower()\n",
    "            text = data[\"text\"].lower() if data.get(\"text\") else None\n",
    "            input1 = llama.format_prompt(instruction, text)\n",
    "            \n",
    "            new_data[\"prompt\"] = input1\n",
    "            new_data[\"instruction\"] = data[\"instruction\"]\n",
    "            new_data[\"label\"] = data[\"label\"]\n",
    "\n",
    "        # Load Audio\n",
    "        if self.audio_input_type == \"imagebind\":\n",
    "            if data.get(\"file2\"):\n",
    "                audio = self._load_imagebind_audio([data[\"file\"], data[\"file2\"]])\n",
    "            else:\n",
    "                audio = self._load_imagebind_audio([data[\"file\"]])\n",
    "        elif self.audio_input_type == \"whisper\":\n",
    "            if data.get(\"file2\"):\n",
    "                audio = self._load_whisper_audio([data[\"file\"], data[\"file2\"]])\n",
    "            else:\n",
    "                audio = self._load_whisper_audio([data[\"file\"]])\n",
    "        new_data[\"audio\"] = audio\n",
    "        return new_data\n",
    "    \n",
    "    def _load_whisper_audio(self, audio_paths):\n",
    "\n",
    "        waveforms = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform = torch.tensor(whisper.load_audio(audio_path))\n",
    "            if waveform.size(0) == 0:\n",
    "                waveform = torch.zeros([16000*3])\n",
    "                print(audio_path)\n",
    "            \n",
    "            if len(audio_paths) == 2:\n",
    "                # pad_or_trim to 15 seconds for tasks have 2 audio.\n",
    "                waveform = whisper.pad_or_trim(waveform, length=15*16000)\n",
    "            waveforms.append(\n",
    "                waveform\n",
    "            )\n",
    "        audio = torch.cat(waveforms, dim=0)\n",
    "        audio = whisper.pad_or_trim(audio)\n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "        \n",
    "        return mel\n",
    "\n",
    "    \n",
    "    def _load_imagebind_audio(self, \n",
    "            audio_paths,\n",
    "            num_mel_bins=128,\n",
    "            target_length=204,\n",
    "            sample_rate=16000,\n",
    "            clip_duration=2,\n",
    "            clips_per_video=3,\n",
    "            mean=-4.268,\n",
    "            std=9.138\n",
    "        ):\n",
    "\n",
    "        waveforms = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "            if waveform.size(1) == 0:\n",
    "                waveform = torch.zeros([1, 16000*3])\n",
    "                sr = 16000\n",
    "                # logging.warning(f\"An audio is set to zero, {audio_path}\")\n",
    "                print(audio_path)\n",
    "                \n",
    "            if sample_rate != sr:\n",
    "                waveform = torchaudio.functional.resample(\n",
    "                    waveform, orig_freq=sr, new_freq=sample_rate\n",
    "                )\n",
    "\n",
    "            if len(audio_paths) == 2:\n",
    "                if waveform.size(1) < 5*16000:\n",
    "                    waveform = torch.cat(\n",
    "                        [waveform, torch.zeros([1, 5*16000 - waveform.size(1)])], dim=1\n",
    "                    )\n",
    "                else:\n",
    "                    waveform = waveform[:, :5*16000]\n",
    "\n",
    "            waveforms.append(waveform)\n",
    "        waveform = torch.cat(waveforms, dim=1)\n",
    "        \n",
    "            \n",
    "        all_clips_timepoints = self._get_clip_timepoints(\n",
    "            self.clip_sampler, waveform.size(1) / sample_rate\n",
    "        )\n",
    "        all_clips = []\n",
    "        for clip_timepoints in all_clips_timepoints:\n",
    "            waveform_clip = waveform[\n",
    "                :,\n",
    "                int(clip_timepoints[0] * sample_rate) : int(\n",
    "                    clip_timepoints[1] * sample_rate\n",
    "                ),\n",
    "            ]\n",
    "            waveform_melspec = self._def_waveform2melspec(\n",
    "                waveform_clip, sample_rate, num_mel_bins, target_length\n",
    "            )\n",
    "            all_clips.append(waveform_melspec)\n",
    "\n",
    "        normalize = transforms.Normalize(mean=mean, std=std)\n",
    "        all_clips = [normalize(ac) for ac in all_clips]\n",
    "\n",
    "        all_clips = torch.stack(all_clips, dim=0)\n",
    "\n",
    "        return all_clips\n",
    "        \n",
    "    def _get_clip_timepoints(self, clip_sampler, duration):\n",
    "        # Read out all clips in this video\n",
    "        all_clips_timepoints = []\n",
    "        is_last_clip = False\n",
    "        end = 0.0\n",
    "        while not is_last_clip:\n",
    "            start, end, _, _, is_last_clip = clip_sampler(end, duration, annotation=None)\n",
    "            all_clips_timepoints.append((start, end))\n",
    "        return all_clips_timepoints\n",
    "\n",
    "    def _def_waveform2melspec(self, waveform, sample_rate, num_mel_bins, target_length):\n",
    "        # Based on https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/dataloader.py#L102\n",
    "        waveform -= waveform.mean()\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(\n",
    "            waveform,\n",
    "            htk_compat=True,\n",
    "            sample_frequency=sample_rate,\n",
    "            use_energy=False,\n",
    "            window_type=\"hanning\",\n",
    "            num_mel_bins=num_mel_bins,\n",
    "            dither=0.0,\n",
    "            frame_length=25,\n",
    "            frame_shift=10,\n",
    "        )\n",
    "        # Convert to [mel_bins, num_frames] shape\n",
    "        fbank = fbank.transpose(0, 1)\n",
    "        # Pad to target_length\n",
    "        n_frames = fbank.size(1)\n",
    "        p = target_length - n_frames\n",
    "        \n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            fbank = torch.nn.functional.pad(fbank, (0, p), mode=\"constant\", value=0)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[:, 0:target_length]\n",
    "        # Convert to [1, mel_bins, num_frames] shape, essentially like a 1\n",
    "        # channel image\n",
    "        fbank = fbank.unsqueeze(0)\n",
    "        return fbank\n",
    "\n",
    "    def check_allowed_datasets(self, task_path):\n",
    "        for d in self.allowed_datasets:\n",
    "            if task_path.stem.lower() in d.lower():\n",
    "                return True\n",
    "        # print(f\"find {task_path.stem} not in allowed list.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dcc0ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BirdSoundDetection_Warblrb10k', 'ChordClassification_AcousticGuitarAndPiano', 'EnvironmentalSoundClassification_AnimalsESC50', 'EnvironmentalSoundClassification_ExteriorAndUrbanNoisesESC50', 'EnvironmentalSoundClassification_HumanAndNonSpeechSoundsESC50', 'EnvironmentalSoundClassification_InteriorAndDomesticSoundsESC50', 'EnvironmentalSoundClassification_NaturalSoundscapesAndWaterSoundsESC50', 'SpeechDetection_LJSpeech', 'SpeechDetection_LibriSpeechTestClean', 'SpeechDetection_LibriSpeechTestOther', 'SpeechTextMatching_LJSpeech', 'SpeechTextMatching_LibriSpeechTestClean', 'SpeechTextMatching_LibriSpeechTestOther', 'SpokenTermDetection_LJSpeech', 'SpokenTermDetection_LibriSpeechTestClean', 'SpokenTermDetection_LibriSpeechTestOther', 'SpeechCommandRecognition_GoogleSpeechCommandsV1', 'LanguageIdentification_VoxForge', 'EnhancementDetection_LibrittsTestCleanWham', 'NoiseDetectiongaussian_LJSpeechMusan', 'NoiseDetectiongaussian_VCTKMusan', 'NoiseDetectionmusic_LJSpeechMusan', 'NoiseDetectionmusic_VCTKMusan', 'NoiseDetectionnoise_LJSpeechMusan', 'NoiseDetectionnoise_VCTKMusan', 'NoiseDetectionspeech_LJSpeechMusan', 'NoiseDetectionspeech_VCTKMusan', 'NoiseSNRLevelPredictiongaussian_VCTKMusan', 'NoiseSNRLevelPredictionmusic_VCTKMusan', 'NoiseSNRLevelPredictionnoise_VCTKMusan', 'NoiseSNRLevelPredictionspeech_VCTKMusan', 'ReverberationDetectionlargeroom_LJSpeechRirsNoises', 'ReverberationDetectionlargeroom_VCTKRirsNoises', 'ReverberationDetectionmediumroom_LJSpeechRirsNoises', 'ReverberationDetectionmediumroom_VCTKRirsNoises', 'ReverberationDetectionsmallroom_LJSpeechRirsNoises', 'ReverberationDetectionsmallroom_VCTKRirsNoises', 'AccentClassification_AccentdbExtended', 'DialogueEmotionClassification_DailyTalk', 'EmotionRecognition_MultimodalEmotionlinesDataset', 'HowFarAreYou_3DSpeaker', 'StressDetection_MIRSD', 'SpoofDetection_ASVspoof2015', 'SpoofDetection_ASVspoof2017', 'DialogueActClassification_DailyTalk', 'Intent_Classification_FluentSpeechCommands_Action', 'Intent_Classification_FluentSpeechCommands_Location', 'Intent_Classification_FluentSpeechCommands_Object', 'SarcasmDetection_Mustard', 'DialogueActPairing_DailyTalk', 'SpeakerCounting_LibriTTSTestClean', 'SpeakerVerification_LibriSpeechTestClean', 'SpeakerVerification_VCTK']\n",
      "Used datasets 53 ['NoiseSNRLevelPredictiongaussian_VCTKMusan', 'SpoofDetection_ASVspoof2017', 'SpokenTermDetection_LibriSpeechTestOther', 'EnvironmentalSoundClassification_NaturalSoundscapesAndWaterSoundsESC50', 'Intent_Classification_FluentSpeechCommands_Object', 'Intent_Classification_FluentSpeechCommands_Action', 'DialogueActClassification_DailyTalk', 'DialogueEmotionClassification_DailyTalk', 'DialogueActPairing_DailyTalk', 'ReverberationDetectionmediumroom_VCTKRirsNoises', 'NoiseSNRLevelPredictionspeech_VCTKMusan', 'SpeakerVerification_VCTK', 'EnvironmentalSoundClassification_AnimalsESC50', 'EnvironmentalSoundClassification_ExteriorAndUrbanNoisesESC50', 'LanguageIdentification_VoxForge', 'EmotionRecognition_MultimodalEmotionlinesDataset', 'ReverberationDetectionlargeroom_LJSpeechRirsNoises', 'SpeechDetection_LJSpeech', 'BirdSoundDetection_Warblrb10k', 'Intent_Classification_FluentSpeechCommands_Location', 'SpoofDetection_ASVspoof2015', 'SpokenTermDetection_LJSpeech', 'ReverberationDetectionmediumroom_LJSpeechRirsNoises', 'SpeakerVerification_LibriSpeechTestClean', 'AccentClassification_AccentdbExtended', 'SpeechDetection_LibriSpeechTestOther', 'HowFarAreYou_3DSpeaker', 'ChordClassification_AcousticGuitarAndPiano', 'SarcasmDetection_Mustard', 'SpeechTextMatching_LibriSpeechTestOther', 'NoiseDetectiongaussian_VCTKMusan', 'ReverberationDetectionlargeroom_VCTKRirsNoises', 'EnhancementDetection_LibrittsTestCleanWham', 'ReverberationDetectionsmallroom_LJSpeechRirsNoises', 'SpeakerCounting_LibriTTSTestClean', 'EnvironmentalSoundClassification_InteriorAndDomesticSoundsESC50', 'NoiseDetectionmusic_VCTKMusan', 'NoiseDetectiongaussian_LJSpeechMusan', 'StressDetection_MIRSD', 'SpeechTextMatching_LibriSpeechTestClean', 'SpeechDetection_LibriSpeechTestClean', 'ReverberationDetectionsmallroom_VCTKRirsNoises', 'NoiseSNRLevelPredictionnoise_VCTKMusan', 'EnvironmentalSoundClassification_HumanAndNonSpeechSoundsESC50', 'NoiseDetectionnoise_LJSpeechMusan', 'NoiseDetectionmusic_LJSpeechMusan', 'SpokenTermDetection_LibriSpeechTestClean', 'SpeechTextMatching_LJSpeech', 'NoiseSNRLevelPredictionmusic_VCTKMusan', 'NoiseDetectionspeech_LJSpeechMusan', 'SpeechCommandRecognition_GoogleSpeechCommandsV1', 'NoiseDetectionnoise_VCTKMusan', 'NoiseDetectionspeech_VCTKMusan']\n",
      "735826\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "tokenizer = Tokenizer(model_path=\"/home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/ckpts/llama/tokenizer.model\")\n",
    "data_path = Path(\"/home/u2619111/hank/Dataset/big-superb-test-data-renamed\")\n",
    "all_datasets = open(\"data/test_dataset.txt\").read().split(\"\\n\")\n",
    "print(all_datasets)\n",
    "# all_datasets = [\"SpeakerVerification_LibrispeechTrainClean100\"]\n",
    "all_train_dataset = BigSuperbDataset(data_path, tokenizer, data_path2=None, used_data_split=\"test\", audio_input_type=\"whisper\", phase=\"test\", allowed_datasets=all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a415e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(all_train_dataset), 500):\n",
    "    all_train_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff418e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e83b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619edd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "from collections import Counter\n",
    "for task_path in Path(\"/home/u2619111/hank/Dataset/big-superb-test-data-renamed\").iterdir():\n",
    "    datas = json.load((task_path/\"test/metadata.json\").open())\n",
    "    \n",
    "    all_labels = []\n",
    "    for file_name, data in datas.items():\n",
    "        all_labels.append(data[\"label\"])\n",
    "    random.seed(1) # 42, 1 ,87\n",
    "    predictions = []\n",
    "    for file_name, data in datas.items():\n",
    "        predictions.append({\n",
    "            \"pred\": random.choice(all_labels),\n",
    "            \"label\": data[\"label\"],\n",
    "            \"instruction\": data[\"instruction\"]\n",
    "        })\n",
    "\n",
    "    counter = Counter(all_labels)\n",
    "    json.dump({\"label_count\": counter,\"predictions\": predictions},\n",
    "        open(f\"/home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/exp/random/random1/{task_path.stem}.json\", \"w\"), indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagebind_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
