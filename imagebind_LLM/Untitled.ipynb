{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d617a6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]m3m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [798 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [108 kB]m\u001b[33m\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB] \u001b[0m\u001b[33m\u001b[33m\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [957 kB]m\u001b[33m\n",
      "Get:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [842 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.8 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1212 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [857 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1081 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [49.4 kB]33m\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [25.6 kB]\n",
      "Fetched 26.2 MB in 6s (4638 kB/s)33m\u001b[0m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "71 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libxext6 is already the newest version (2:1.3.4-1build1).\n",
      "libxext6 set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  fontconfig fontconfig-config fonts-dejavu-core i965-va-driver\n",
      "  intel-media-va-driver libaacs0 libaom3 libass9 libasyncns0 libavc1394-0\n",
      "  libavcodec58 libavdevice58 libavfilter7 libavformat58 libavutil56 libbdplus0\n",
      "  libbluray2 libbs2b0 libcaca0 libcairo-gobject2 libcairo2 libcdio-cdda2\n",
      "  libcdio-paranoia2 libcdio19 libchromaprint1 libcodec2-1.0 libdatrie1\n",
      "  libdav1d5 libdc1394-25 libdecor-0-0 libdecor-0-plugin-1-cairo libdeflate0\n",
      "  libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2 libdrm-radeon1\n",
      "  libdrm2 libelf1 libflite1 libfontconfig1 libfreetype6 libfribidi0 libgbm1\n",
      "  libgdk-pixbuf-2.0-0 libgdk-pixbuf2.0-bin libgdk-pixbuf2.0-common libgl1\n",
      "  libgl1-amber-dri libgl1-mesa-dri libglapi-mesa libglvnd0 libglx-mesa0\n",
      "  libglx0 libgme0 libgraphite2-3 libharfbuzz0b libice6 libicu70 libiec61883-0\n",
      "  libigdgmm12 libjack-jackd2-0 libjbig0 liblilv-0-0 libllvm15 libmfx1\n",
      "  libmp3lame0 libmpg123-0 libmysofa1 libnorm1 libopenal-data libopenal1\n",
      "  libopenjp2-7 libopenmpt0 libpango-1.0-0 libpangocairo-1.0-0\n",
      "  libpangoft2-1.0-0 libpciaccess0 libpgm-5.3-0 libpixman-1-0 libpocketsphinx3\n",
      "  libpostproc55 libpulse0 librabbitmq4 libraw1394-11 librsvg2-2\n",
      "  librsvg2-common librubberband2 libsamplerate0 libsdl2-2.0-0\n",
      "  libsensors-config libsensors5 libserd-0-0 libshine3 libslang2 libsndio7.0\n",
      "  libsord-0-0 libsoxr0 libspeex1 libsphinxbase3 libsratom-0-0 libsrt1.4-gnutls\n",
      "  libssh-gcrypt-4 libswresample3 libswscale5 libthai-data libthai0 libtheora0\n",
      "  libtiff5 libtwolame0 libudfread0 libusb-1.0-0 libva-drm2 libva-x11-2 libva2\n",
      "  libvdpau1 libvidstab1.1 libvpx7 libwayland-client0 libwayland-cursor0\n",
      "  libwayland-egl1 libwayland-server0 libwebp7 libwebpmux3 libx11-xcb1\n",
      "  libx264-163 libx265-199 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0\n",
      "  libxcb-present0 libxcb-render0 libxcb-shape0 libxcb-shm0 libxcb-sync1\n",
      "  libxcb-xfixes0 libxcursor1 libxfixes3 libxi6 libxinerama1 libxkbcommon0\n",
      "  libxml2 libxrandr2 libxrender1 libxshmfence1 libxss1 libxv1 libxvidcore4\n",
      "  libxxf86vm1 libzimg2 libzmq5 libzvbi-common libzvbi0 mesa-va-drivers\n",
      "  mesa-vdpau-drivers ocl-icd-libopencl1 pocketsphinx-en-us shared-mime-info\n",
      "  va-driver-all vdpau-driver-all x11-common xkb-data\n",
      "Suggested packages:\n",
      "  ffmpeg-doc i965-va-driver-shaders libcuda1 libnvcuvid1 libnvidia-encode1\n",
      "  libbluray-bdj jackd2 libportaudio2 pciutils pulseaudio libraw1394-doc\n",
      "  librsvg2-bin xdg-utils lm-sensors serdi sndiod sordi speex opencl-icd\n",
      "  libvdpau-va-gl1\n",
      "The following NEW packages will be installed:\n",
      "  ffmpeg fontconfig fontconfig-config fonts-dejavu-core i965-va-driver\n",
      "  intel-media-va-driver libaacs0 libaom3 libass9 libasyncns0 libavc1394-0\n",
      "  libavcodec58 libavdevice58 libavfilter7 libavformat58 libavutil56 libbdplus0\n",
      "  libbluray2 libbs2b0 libcaca0 libcairo-gobject2 libcairo2 libcdio-cdda2\n",
      "  libcdio-paranoia2 libcdio19 libchromaprint1 libcodec2-1.0 libdatrie1\n",
      "  libdav1d5 libdc1394-25 libdecor-0-0 libdecor-0-plugin-1-cairo libdeflate0\n",
      "  libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2 libdrm-radeon1\n",
      "  libdrm2 libelf1 libflite1 libfontconfig1 libfreetype6 libfribidi0 libgbm1\n",
      "  libgdk-pixbuf-2.0-0 libgdk-pixbuf2.0-bin libgdk-pixbuf2.0-common libgl1\n",
      "  libgl1-amber-dri libgl1-mesa-dri libglapi-mesa libglvnd0 libglx-mesa0\n",
      "  libglx0 libgme0 libgraphite2-3 libharfbuzz0b libice6 libicu70 libiec61883-0\n",
      "  libigdgmm12 libjack-jackd2-0 libjbig0 liblilv-0-0 libllvm15 libmfx1\n",
      "  libmp3lame0 libmpg123-0 libmysofa1 libnorm1 libopenal-data libopenal1\n",
      "  libopenjp2-7 libopenmpt0 libpango-1.0-0 libpangocairo-1.0-0\n",
      "  libpangoft2-1.0-0 libpciaccess0 libpgm-5.3-0 libpixman-1-0 libpocketsphinx3\n",
      "  libpostproc55 libpulse0 librabbitmq4 libraw1394-11 librsvg2-2\n",
      "  librsvg2-common librubberband2 libsamplerate0 libsdl2-2.0-0\n",
      "  libsensors-config libsensors5 libserd-0-0 libshine3 libslang2 libsm6\n",
      "  libsndio7.0 libsord-0-0 libsoxr0 libspeex1 libsphinxbase3 libsratom-0-0\n",
      "  libsrt1.4-gnutls libssh-gcrypt-4 libswresample3 libswscale5 libthai-data\n",
      "  libthai0 libtheora0 libtiff5 libtwolame0 libudfread0 libusb-1.0-0 libva-drm2\n",
      "  libva-x11-2 libva2 libvdpau1 libvidstab1.1 libvpx7 libwayland-client0\n",
      "  libwayland-cursor0 libwayland-egl1 libwayland-server0 libwebp7 libwebpmux3\n",
      "  libx11-xcb1 libx264-163 libx265-199 libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0\n",
      "  libxcb-present0 libxcb-render0 libxcb-shape0 libxcb-shm0 libxcb-sync1\n",
      "  libxcb-xfixes0 libxcursor1 libxfixes3 libxi6 libxinerama1 libxkbcommon0\n",
      "  libxml2 libxrandr2 libxrender1 libxshmfence1 libxss1 libxv1 libxvidcore4\n",
      "  libxxf86vm1 libzimg2 libzmq5 libzvbi-common libzvbi0 mesa-va-drivers\n",
      "  mesa-vdpau-drivers ocl-icd-libopencl1 pocketsphinx-en-us shared-mime-info\n",
      "  va-driver-all vdpau-driver-all x11-common xkb-data\n",
      "0 upgraded, 164 newly installed, 0 to remove and 71 not upgraded.\n",
      "Need to get 146 MB of archives.\n",
      "After this operation, 455 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libelf1 amd64 0.186-1build1 [51.0 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libfribidi0 amd64 1.0.8-2ubuntu3.1 [26.1 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libicu70 amd64 70.1-2 [10.6 MB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libslang2 amd64 2.3.2-5build4 [468 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libxml2 amd64 2.9.13+dfsg-1ubuntu0.3 [763 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 shared-mime-info amd64 2.1-2 [454 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xkb-data all 2.33-1 [394 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-common all 2.4.113-2~ubuntu0.22.04.1 [5450 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm2 amd64 2.4.113-2~ubuntu0.22.04.1 [38.1 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libusb-1.0-0 amd64 2:1.0.25-1ubuntu2 [52.7 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaom3 amd64 3.3.0-1 [1748 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libva2 amd64 2.14.0-1 [65.0 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmfx1 amd64 22.3.0-1 [3105 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libva-drm2 amd64 2.14.0-1 [7502 B]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfixes3 amd64 1:6.0.0-1 [11.7 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libva-x11-2 amd64 2.14.0-1 [12.6 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvdpau1 amd64 1.4-3build2 [27.0 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ocl-icd-libopencl1 amd64 2.2.14-3 [39.1 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavutil56 amd64 7:4.4.2-0ubuntu0.22.04.1 [290 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libfreetype6 amd64 2.11.1+dfsg-1ubuntu0.2 [389 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1041 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 fontconfig-config all 2.13.1-4.2ubuntu5 [29.1 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontconfig1 amd64 2.13.1-4.2ubuntu5 [131 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpixman-1-0 amd64 0.40.0-1ubuntu0.22.04.1 [264 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render0 amd64 1.14-3ubuntu3 [16.4 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-shm0 amd64 1.14-3ubuntu3 [5780 B]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxrender1 amd64 1:0.9.10-1build4 [19.7 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcairo2 amd64 1.16.0-5ubuntu2 [628 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcodec2-1.0 amd64 1.0.1-3 [8435 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libdav1d5 amd64 0.9.2-1 [463 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmp3lame0 amd64 3.100-3build2 [141 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopenjp2-7 amd64 2.4.0-6 [158 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcairo-gobject2 amd64 1.16.0-5ubuntu2 [19.4 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf2.0-common all 2.42.8+dfsg-1ubuntu0.2 [5530 B]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdeflate0 amd64 1.10-2 [70.9 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libjbig0 amd64 2.1-3.1ubuntu0.22.04.1 [29.2 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwebp7 amd64 1.2.2-2ubuntu0.22.04.1 [206 kB]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtiff5 amd64 4.3.0-6ubuntu0.4 [183 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf-2.0-0 amd64 2.42.8+dfsg-1ubuntu0.2 [148 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 fontconfig amd64 2.13.1-4.2ubuntu5 [177 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgraphite2-3 amd64 1.3.14-1build2 [71.3 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libharfbuzz0b amd64 2.7.4-1ubuntu3.1 [352 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu jammy/main amd64 libthai-data all 0.1.29-1build1 [162 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdatrie1 amd64 0.2.13-2 [19.9 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 libthai0 amd64 0.1.29-1build1 [19.2 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpango-1.0-0 amd64 1.50.6+ds-2ubuntu1 [230 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpangoft2-1.0-0 amd64 1.50.6+ds-2ubuntu1 [54.0 kB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpangocairo-1.0-0 amd64 1.50.6+ds-2ubuntu1 [39.8 kB]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu jammy/main amd64 librsvg2-2 amd64 2.52.5+dfsg-3 [3020 kB]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libshine3 amd64 3.1.1-2 [23.2 kB]\n",
      "Get:51 http://archive.ubuntu.com/ubuntu jammy/main amd64 libspeex1 amd64 1.2~rc1.2-1.1ubuntu3 [57.9 kB]\n",
      "Get:52 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsoxr0 amd64 0.1.3-4build2 [79.8 kB]\n",
      "Get:53 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libswresample3 amd64 7:4.4.2-0ubuntu0.22.04.1 [62.2 kB]\n",
      "Get:54 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtheora0 amd64 1.1.1+dfsg.1-15ubuntu4 [209 kB]\n",
      "Get:55 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtwolame0 amd64 0.4.0-2build2 [52.5 kB]\n",
      "Get:56 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvpx7 amd64 1.11.0-2ubuntu2 [1078 kB]\n",
      "Get:57 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwebpmux3 amd64 1.2.2-2ubuntu0.22.04.1 [20.5 kB]\n",
      "Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libx264-163 amd64 2:0.163.3060+git5db6aa6-2build1 [591 kB]\n",
      "Get:59 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libx265-199 amd64 3.5-2 [1170 kB]\n",
      "Get:60 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libxvidcore4 amd64 2:1.3.7-1 [201 kB]\n",
      "Get:61 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzvbi-common all 0.2.35-19 [35.5 kB]\n",
      "Get:62 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzvbi0 amd64 0.2.35-19 [262 kB]\n",
      "Get:63 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavcodec58 amd64 7:4.4.2-0ubuntu0.22.04.1 [5567 kB]\n",
      "Get:64 http://archive.ubuntu.com/ubuntu jammy/main amd64 libraw1394-11 amd64 2.1.2-2build2 [27.0 kB]\n",
      "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 libavc1394-0 amd64 0.5.4-5build2 [17.0 kB]\n",
      "Get:66 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libass9 amd64 1:0.15.2-1 [97.5 kB]\n",
      "Get:67 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libudfread0 amd64 1.1.2-1 [16.2 kB]\n",
      "Get:68 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbluray2 amd64 1:1.3.1-1 [159 kB]\n",
      "Get:69 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libchromaprint1 amd64 1.5.1-2 [28.4 kB]\n",
      "Get:70 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgme0 amd64 0.6.3-2 [127 kB]\n",
      "Get:71 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmpg123-0 amd64 1.29.3-1build1 [172 kB]\n",
      "Get:72 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenmpt0 amd64 0.6.1-1 [592 kB]\n",
      "Get:73 http://archive.ubuntu.com/ubuntu jammy/main amd64 librabbitmq4 amd64 0.10.0-1ubuntu2 [39.3 kB]\n",
      "Get:74 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsrt1.4-gnutls amd64 1.4.4-4 [309 kB]\n",
      "Get:75 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libssh-gcrypt-4 amd64 0.9.6-2ubuntu0.22.04.1 [222 kB]\n",
      "Get:76 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libnorm1 amd64 1.5.9+dfsg-2 [221 kB]\n",
      "Get:77 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpgm-5.3-0 amd64 5.3.128~dfsg-2 [161 kB]\n",
      "Get:78 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzmq5 amd64 4.3.4-2 [256 kB]\n",
      "Get:79 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavformat58 amd64 7:4.4.2-0ubuntu0.22.04.1 [1103 kB]\n",
      "Get:80 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbs2b0 amd64 3.1.0+dfsg-2.2build1 [10.2 kB]\n",
      "Get:81 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libflite1 amd64 2.2-3 [13.7 MB]\n",
      "Get:82 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libserd-0-0 amd64 0.30.10-2 [40.8 kB]\n",
      "Get:83 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsord-0-0 amd64 0.16.8-2 [21.2 kB]\n",
      "Get:84 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsratom-0-0 amd64 0.6.8-1 [17.0 kB]\n",
      "Get:85 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblilv-0-0 amd64 0.24.12-2 [42.8 kB]\n",
      "Get:86 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmysofa1 amd64 1.2.1~dfsg0-1 [1157 kB]\n",
      "Get:87 http://archive.ubuntu.com/ubuntu jammy/main amd64 libasyncns0 amd64 0.8-6build2 [12.8 kB]\n",
      "Get:88 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libx11-xcb1 amd64 2:1.7.5-1ubuntu0.2 [7800 B]\n",
      "Get:89 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpulse0 amd64 1:15.99.1+dfsg1-1ubuntu2.1 [297 kB]\n",
      "Get:90 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsphinxbase3 amd64 0.8+5prealpha+1-13build1 [126 kB]\n",
      "Get:91 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpocketsphinx3 amd64 0.8.0+real5prealpha+1-14ubuntu1 [132 kB]\n",
      "Get:92 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libpostproc55 amd64 7:4.4.2-0ubuntu0.22.04.1 [60.1 kB]\n",
      "Get:93 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsamplerate0 amd64 0.2.2-1build1 [1359 kB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:94 http://archive.ubuntu.com/ubuntu jammy/universe amd64 librubberband2 amd64 2.0.0-2 [90.0 kB]\n",
      "Get:95 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libswscale5 amd64 7:4.4.2-0ubuntu0.22.04.1 [180 kB]\n",
      "Get:96 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libvidstab1.1 amd64 1.1.0-2 [35.0 kB]\n",
      "Get:97 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzimg2 amd64 3.0.3+ds1-1 [241 kB]\n",
      "Get:98 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavfilter7 amd64 7:4.4.2-0ubuntu0.22.04.1 [1496 kB]\n",
      "Get:99 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcaca0 amd64 0.99.beta19-2.2ubuntu4 [224 kB]\n",
      "Get:100 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcdio19 amd64 2.1.0-3build1 [63.3 kB]\n",
      "Get:101 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcdio-cdda2 amd64 10.2+2.0.0-1build3 [16.7 kB]\n",
      "Get:102 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcdio-paranoia2 amd64 10.2+2.0.0-1build3 [15.9 kB]\n",
      "Get:103 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libdc1394-25 amd64 2.2.6-4 [88.8 kB]\n",
      "Get:104 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd0 amd64 1.4.0-1 [73.6 kB]\n",
      "Get:105 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglapi-mesa amd64 22.2.5-0ubuntu0.1~22.04.3 [35.5 kB]\n",
      "Get:106 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-dri2-0 amd64 1.14-3ubuntu3 [7206 B]\n",
      "Get:107 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-dri3-0 amd64 1.14-3ubuntu3 [6968 B]\n",
      "Get:108 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-glx0 amd64 1.14-3ubuntu3 [25.9 kB]\n",
      "Get:109 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-present0 amd64 1.14-3ubuntu3 [5734 B]\n",
      "Get:110 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-sync1 amd64 1.14-3ubuntu3 [9416 B]\n",
      "Get:111 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xfixes0 amd64 1.14-3ubuntu3 [9996 B]\n",
      "Get:112 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxshmfence1 amd64 1.3-1build4 [5394 B]\n",
      "Get:113 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86vm1 amd64 1:1.1.4-1build3 [10.4 kB]\n",
      "Get:114 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-amdgpu1 amd64 2.4.113-2~ubuntu0.22.04.1 [19.9 kB]\n",
      "Get:115 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-nouveau2 amd64 2.4.113-2~ubuntu0.22.04.1 [17.5 kB]\n",
      "Get:116 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-radeon1 amd64 2.4.113-2~ubuntu0.22.04.1 [21.6 kB]\n",
      "Get:117 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libllvm15 amd64 1:15.0.7-0ubuntu0.22.04.2 [25.4 MB]\n",
      "Get:118 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsensors-config all 1:3.6.0-7ubuntu1 [5274 B]\n",
      "Get:119 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsensors5 amd64 1:3.6.0-7ubuntu1 [26.3 kB]\n",
      "Get:120 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dri amd64 22.2.5-0ubuntu0.1~22.04.3 [7511 kB]\n",
      "Get:121 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglx-mesa0 amd64 22.2.5-0ubuntu0.1~22.04.3 [158 kB]\n",
      "Get:122 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx0 amd64 1.4.0-1 [41.0 kB]\n",
      "Get:123 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl1 amd64 1.4.0-1 [110 kB]\n",
      "Get:124 http://archive.ubuntu.com/ubuntu jammy/main amd64 libiec61883-0 amd64 1.2.0-4build3 [25.9 kB]\n",
      "Get:125 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjack-jackd2-0 amd64 1.9.20~dfsg-1 [293 kB]\n",
      "Get:126 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenal-data all 1:1.19.1-2build3 [164 kB]\n",
      "Get:127 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsndio7.0 amd64 1.8.1-1.1 [29.3 kB]\n",
      "Get:128 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenal1 amd64 1:1.19.1-2build3 [535 kB]\n",
      "Get:129 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-client0 amd64 1.20.0-1ubuntu0.1 [25.9 kB]\n",
      "Get:130 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdecor-0-0 amd64 0.1.0-3build1 [15.1 kB]\n",
      "Get:131 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-server0 amd64 1.20.0-1ubuntu0.1 [34.3 kB]\n",
      "Get:132 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgbm1 amd64 22.2.5-0ubuntu0.1~22.04.3 [33.1 kB]\n",
      "Get:133 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-cursor0 amd64 1.20.0-1ubuntu0.1 [10.7 kB]\n",
      "Get:134 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-egl1 amd64 1.20.0-1ubuntu0.1 [5582 B]\n",
      "Get:135 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcursor1 amd64 1:1.2.0-2build4 [20.9 kB]\n",
      "Get:136 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxi6 amd64 2:1.8-1build1 [32.6 kB]\n",
      "Get:137 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxinerama1 amd64 2:1.1.4-3 [7382 B]\n",
      "Get:138 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon0 amd64 1.4.0-1 [125 kB]\n",
      "Get:139 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxrandr2 amd64 2:1.5.2-1build1 [20.4 kB]\n",
      "Get:140 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-common all 1:7.7+23ubuntu2 [23.4 kB]\n",
      "Get:141 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxss1 amd64 1:1.2.3-1build2 [8476 B]\n",
      "Get:142 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsdl2-2.0-0 amd64 2.0.20+dfsg-2ubuntu1.22.04.1 [582 kB]\n",
      "Get:143 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-shape0 amd64 1.14-3ubuntu3 [6158 B]\n",
      "Get:144 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxv1 amd64 2:1.0.11-1build2 [11.2 kB]\n",
      "Get:145 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libavdevice58 amd64 7:4.4.2-0ubuntu0.22.04.1 [87.5 kB]\n",
      "Get:146 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 ffmpeg amd64 7:4.4.2-0ubuntu0.22.04.1 [1696 kB]\n",
      "Get:147 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libigdgmm12 amd64 22.1.2+ds1-1 [139 kB]\n",
      "Get:148 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 intel-media-va-driver amd64 22.3.1+dfsg1-1ubuntu2 [2283 kB]\n",
      "Get:149 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaacs0 amd64 0.11.1-1 [64.1 kB]\n",
      "Get:150 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbdplus0 amd64 0.2.0-1 [52.2 kB]\n",
      "Get:151 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdecor-0-plugin-1-cairo amd64 0.1.0-3build1 [20.4 kB]\n",
      "Get:152 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpciaccess0 amd64 0.16-3 [19.1 kB]\n",
      "Get:153 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-intel1 amd64 2.4.113-2~ubuntu0.22.04.1 [66.7 kB]\n",
      "Get:154 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgdk-pixbuf2.0-bin amd64 2.42.8+dfsg-1ubuntu0.2 [14.2 kB]\n",
      "Get:155 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl1-amber-dri amd64 21.3.7-0ubuntu1 [4433 kB]\n",
      "Get:156 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice6 amd64 2:1.0.10-1build2 [42.6 kB]\n",
      "Get:157 http://archive.ubuntu.com/ubuntu jammy/main amd64 librsvg2-common amd64 2.52.5+dfsg-3 [17.7 kB]\n",
      "Get:158 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm6 amd64 2:1.2.3-1build2 [16.7 kB]\n",
      "Get:159 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 mesa-va-drivers amd64 22.2.5-0ubuntu0.1~22.04.3 [3409 kB]\n",
      "Get:160 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vdpau-drivers amd64 22.2.5-0ubuntu0.1~22.04.3 [3313 kB]\n",
      "Get:161 http://archive.ubuntu.com/ubuntu jammy/universe amd64 i965-va-driver amd64 2.4.1+dfsg1-1 [302 kB]\n",
      "Get:162 http://archive.ubuntu.com/ubuntu jammy/universe amd64 va-driver-all amd64 2.14.0-1 [3984 B]\n",
      "Get:163 http://archive.ubuntu.com/ubuntu jammy/main amd64 vdpau-driver-all amd64 1.4-3build2 [4510 B]\n",
      "Get:164 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pocketsphinx-en-us all 0.8.0+real5prealpha+1-14ubuntu1 [27.6 MB]\n",
      "Fetched 146 MB in 30s (4927 kB/s)                                              \n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 164.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Extracting templates from packages: 100%\n",
      "Selecting previously unselected package libelf1:amd64.\n",
      "(Reading database ... 28925 files and directories currently installed.)\n",
      "Preparing to unpack .../000-libelf1_0.186-1build1_amd64.deb ...\n",
      "Unpacking libelf1:amd64 (0.186-1build1) ...\n",
      "Selecting previously unselected package libfribidi0:amd64.\n",
      "Preparing to unpack .../001-libfribidi0_1.0.8-2ubuntu3.1_amd64.deb ...\n",
      "Unpacking libfribidi0:amd64 (1.0.8-2ubuntu3.1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libicu70:amd64.\n",
      "Preparing to unpack .../002-libicu70_70.1-2_amd64.deb ...\n",
      "Unpacking libicu70:amd64 (70.1-2) ...\n",
      "Selecting previously unselected package libslang2:amd64.\n",
      "Preparing to unpack .../003-libslang2_2.3.2-5build4_amd64.deb ...\n",
      "Unpacking libslang2:amd64 (2.3.2-5build4) ...\n",
      "Selecting previously unselected package libxml2:amd64.\n",
      "Preparing to unpack .../004-libxml2_2.9.13+dfsg-1ubuntu0.3_amd64.deb ...\n",
      "Unpacking libxml2:amd64 (2.9.13+dfsg-1ubuntu0.3) ...\n",
      "Selecting previously unselected package shared-mime-info.\n",
      "Preparing to unpack .../005-shared-mime-info_2.1-2_amd64.deb ...\n",
      "Unpacking shared-mime-info (2.1-2) ...\n",
      "Selecting previously unselected package xkb-data.\n",
      "Preparing to unpack .../006-xkb-data_2.33-1_all.deb ...\n",
      "Unpacking xkb-data (2.33-1) ...\n",
      "Selecting previously unselected package libdrm-common.\n",
      "Preparing to unpack .../007-libdrm-common_2.4.113-2~ubuntu0.22.04.1_all.deb ...\n",
      "Unpacking libdrm-common (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libdrm2:amd64.\n",
      "Preparing to unpack .../008-libdrm2_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libusb-1.0-0:amd64.\n",
      "Preparing to unpack .../009-libusb-1.0-0_2%3a1.0.25-1ubuntu2_amd64.deb ...\n",
      "Unpacking libusb-1.0-0:amd64 (2:1.0.25-1ubuntu2) ...\n",
      "Selecting previously unselected package libaom3:amd64.\n",
      "Preparing to unpack .../010-libaom3_3.3.0-1_amd64.deb ...\n",
      "Unpacking libaom3:amd64 (3.3.0-1) ...\n",
      "Selecting previously unselected package libva2:amd64.\n",
      "Preparing to unpack .../011-libva2_2.14.0-1_amd64.deb ...\n",
      "Unpacking libva2:amd64 (2.14.0-1) ...\n",
      "Selecting previously unselected package libmfx1:amd64.\n",
      "Preparing to unpack .../012-libmfx1_22.3.0-1_amd64.deb ...\n",
      "Unpacking libmfx1:amd64 (22.3.0-1) ...\n",
      "Selecting previously unselected package libva-drm2:amd64.\n",
      "Preparing to unpack .../013-libva-drm2_2.14.0-1_amd64.deb ...\n",
      "Unpacking libva-drm2:amd64 (2.14.0-1) ...\n",
      "Selecting previously unselected package libxfixes3:amd64.\n",
      "Preparing to unpack .../014-libxfixes3_1%3a6.0.0-1_amd64.deb ...\n",
      "Unpacking libxfixes3:amd64 (1:6.0.0-1) ...\n",
      "Selecting previously unselected package libva-x11-2:amd64.\n",
      "Preparing to unpack .../015-libva-x11-2_2.14.0-1_amd64.deb ...\n",
      "Unpacking libva-x11-2:amd64 (2.14.0-1) ...\n",
      "Selecting previously unselected package libvdpau1:amd64.\n",
      "Preparing to unpack .../016-libvdpau1_1.4-3build2_amd64.deb ...\n",
      "Unpacking libvdpau1:amd64 (1.4-3build2) ...\n",
      "Selecting previously unselected package ocl-icd-libopencl1:amd64.\n",
      "Preparing to unpack .../017-ocl-icd-libopencl1_2.2.14-3_amd64.deb ...\n",
      "Unpacking ocl-icd-libopencl1:amd64 (2.2.14-3) ...\n",
      "Selecting previously unselected package libavutil56:amd64.\n",
      "Preparing to unpack .../018-libavutil56_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libavutil56:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libfreetype6:amd64.\n",
      "Preparing to unpack .../019-libfreetype6_2.11.1+dfsg-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libfreetype6:amd64 (2.11.1+dfsg-1ubuntu0.2) ...\n",
      "Selecting previously unselected package fonts-dejavu-core.\n",
      "Preparing to unpack .../020-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
      "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
      "Selecting previously unselected package fontconfig-config.\n",
      "Preparing to unpack .../021-fontconfig-config_2.13.1-4.2ubuntu5_all.deb ...\n",
      "Unpacking fontconfig-config (2.13.1-4.2ubuntu5) ...\n",
      "Selecting previously unselected package libfontconfig1:amd64.\n",
      "Preparing to unpack .../022-libfontconfig1_2.13.1-4.2ubuntu5_amd64.deb ...\n",
      "Unpacking libfontconfig1:amd64 (2.13.1-4.2ubuntu5) ...\n",
      "Selecting previously unselected package libpixman-1-0:amd64.\n",
      "Preparing to unpack .../023-libpixman-1-0_0.40.0-1ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libpixman-1-0:amd64 (0.40.0-1ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libxcb-render0:amd64.\n",
      "Preparing to unpack .../024-libxcb-render0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-render0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-shm0:amd64.\n",
      "Preparing to unpack .../025-libxcb-shm0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-shm0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxrender1:amd64.\n",
      "Preparing to unpack .../026-libxrender1_1%3a0.9.10-1build4_amd64.deb ...\n",
      "Unpacking libxrender1:amd64 (1:0.9.10-1build4) ...\n",
      "Selecting previously unselected package libcairo2:amd64.\n",
      "Preparing to unpack .../027-libcairo2_1.16.0-5ubuntu2_amd64.deb ...\n",
      "Unpacking libcairo2:amd64 (1.16.0-5ubuntu2) ...\n",
      "Selecting previously unselected package libcodec2-1.0:amd64.\n",
      "Preparing to unpack .../028-libcodec2-1.0_1.0.1-3_amd64.deb ...\n",
      "Unpacking libcodec2-1.0:amd64 (1.0.1-3) ...\n",
      "Selecting previously unselected package libdav1d5:amd64.\n",
      "Preparing to unpack .../029-libdav1d5_0.9.2-1_amd64.deb ...\n",
      "Unpacking libdav1d5:amd64 (0.9.2-1) ...\n",
      "Selecting previously unselected package libmp3lame0:amd64.\n",
      "Preparing to unpack .../030-libmp3lame0_3.100-3build2_amd64.deb ...\n",
      "Unpacking libmp3lame0:amd64 (3.100-3build2) ...\n",
      "Selecting previously unselected package libopenjp2-7:amd64.\n",
      "Preparing to unpack .../031-libopenjp2-7_2.4.0-6_amd64.deb ...\n",
      "Unpacking libopenjp2-7:amd64 (2.4.0-6) ...\n",
      "Selecting previously unselected package libcairo-gobject2:amd64.\n",
      "Preparing to unpack .../032-libcairo-gobject2_1.16.0-5ubuntu2_amd64.deb ...\n",
      "Unpacking libcairo-gobject2:amd64 (1.16.0-5ubuntu2) ...\n",
      "Selecting previously unselected package libgdk-pixbuf2.0-common.\n",
      "Preparing to unpack .../033-libgdk-pixbuf2.0-common_2.42.8+dfsg-1ubuntu0.2_all.deb ...\n",
      "Unpacking libgdk-pixbuf2.0-common (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libdeflate0:amd64.\n",
      "Preparing to unpack .../034-libdeflate0_1.10-2_amd64.deb ...\n",
      "Unpacking libdeflate0:amd64 (1.10-2) ...\n",
      "Selecting previously unselected package libjbig0:amd64.\n",
      "Preparing to unpack .../035-libjbig0_2.1-3.1ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libjbig0:amd64 (2.1-3.1ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libwebp7:amd64.\n",
      "Preparing to unpack .../036-libwebp7_1.2.2-2ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libwebp7:amd64 (1.2.2-2ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libtiff5:amd64.\n",
      "Preparing to unpack .../037-libtiff5_4.3.0-6ubuntu0.4_amd64.deb ...\n",
      "Unpacking libtiff5:amd64 (4.3.0-6ubuntu0.4) ...\n",
      "Selecting previously unselected package libgdk-pixbuf-2.0-0:amd64.\n",
      "Preparing to unpack .../038-libgdk-pixbuf-2.0-0_2.42.8+dfsg-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Selecting previously unselected package fontconfig.\n",
      "Preparing to unpack .../039-fontconfig_2.13.1-4.2ubuntu5_amd64.deb ...\n",
      "Unpacking fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Selecting previously unselected package libgraphite2-3:amd64.\n",
      "Preparing to unpack .../040-libgraphite2-3_1.3.14-1build2_amd64.deb ...\n",
      "Unpacking libgraphite2-3:amd64 (1.3.14-1build2) ...\n",
      "Selecting previously unselected package libharfbuzz0b:amd64.\n",
      "Preparing to unpack .../041-libharfbuzz0b_2.7.4-1ubuntu3.1_amd64.deb ...\n",
      "Unpacking libharfbuzz0b:amd64 (2.7.4-1ubuntu3.1) ...\n",
      "Selecting previously unselected package libthai-data.\n",
      "Preparing to unpack .../042-libthai-data_0.1.29-1build1_all.deb ...\n",
      "Unpacking libthai-data (0.1.29-1build1) ...\n",
      "Selecting previously unselected package libdatrie1:amd64.\n",
      "Preparing to unpack .../043-libdatrie1_0.2.13-2_amd64.deb ...\n",
      "Unpacking libdatrie1:amd64 (0.2.13-2) ...\n",
      "Selecting previously unselected package libthai0:amd64.\n",
      "Preparing to unpack .../044-libthai0_0.1.29-1build1_amd64.deb ...\n",
      "Unpacking libthai0:amd64 (0.1.29-1build1) ...\n",
      "Selecting previously unselected package libpango-1.0-0:amd64.\n",
      "Preparing to unpack .../045-libpango-1.0-0_1.50.6+ds-2ubuntu1_amd64.deb ...\n",
      "Unpacking libpango-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Selecting previously unselected package libpangoft2-1.0-0:amd64.\n",
      "Preparing to unpack .../046-libpangoft2-1.0-0_1.50.6+ds-2ubuntu1_amd64.deb ...\n",
      "Unpacking libpangoft2-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Selecting previously unselected package libpangocairo-1.0-0:amd64.\n",
      "Preparing to unpack .../047-libpangocairo-1.0-0_1.50.6+ds-2ubuntu1_amd64.deb ...\n",
      "Unpacking libpangocairo-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Selecting previously unselected package librsvg2-2:amd64.\n",
      "Preparing to unpack .../048-librsvg2-2_2.52.5+dfsg-3_amd64.deb ...\n",
      "Unpacking librsvg2-2:amd64 (2.52.5+dfsg-3) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libshine3:amd64.\n",
      "Preparing to unpack .../049-libshine3_3.1.1-2_amd64.deb ...\n",
      "Unpacking libshine3:amd64 (3.1.1-2) ...\n",
      "Selecting previously unselected package libspeex1:amd64.\n",
      "Preparing to unpack .../050-libspeex1_1.2~rc1.2-1.1ubuntu3_amd64.deb ...\n",
      "Unpacking libspeex1:amd64 (1.2~rc1.2-1.1ubuntu3) ...\n",
      "Selecting previously unselected package libsoxr0:amd64.\n",
      "Preparing to unpack .../051-libsoxr0_0.1.3-4build2_amd64.deb ...\n",
      "Unpacking libsoxr0:amd64 (0.1.3-4build2) ...\n",
      "Selecting previously unselected package libswresample3:amd64.\n",
      "Preparing to unpack .../052-libswresample3_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libswresample3:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libtheora0:amd64.\n",
      "Preparing to unpack .../053-libtheora0_1.1.1+dfsg.1-15ubuntu4_amd64.deb ...\n",
      "Unpacking libtheora0:amd64 (1.1.1+dfsg.1-15ubuntu4) ...\n",
      "Selecting previously unselected package libtwolame0:amd64.\n",
      "Preparing to unpack .../054-libtwolame0_0.4.0-2build2_amd64.deb ...\n",
      "Unpacking libtwolame0:amd64 (0.4.0-2build2) ...\n",
      "Selecting previously unselected package libvpx7:amd64.\n",
      "Preparing to unpack .../055-libvpx7_1.11.0-2ubuntu2_amd64.deb ...\n",
      "Unpacking libvpx7:amd64 (1.11.0-2ubuntu2) ...\n",
      "Selecting previously unselected package libwebpmux3:amd64.\n",
      "Preparing to unpack .../056-libwebpmux3_1.2.2-2ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libwebpmux3:amd64 (1.2.2-2ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libx264-163:amd64.\n",
      "Preparing to unpack .../057-libx264-163_2%3a0.163.3060+git5db6aa6-2build1_amd64.deb ...\n",
      "Unpacking libx264-163:amd64 (2:0.163.3060+git5db6aa6-2build1) ...\n",
      "Selecting previously unselected package libx265-199:amd64.\n",
      "Preparing to unpack .../058-libx265-199_3.5-2_amd64.deb ...\n",
      "Unpacking libx265-199:amd64 (3.5-2) ...\n",
      "Selecting previously unselected package libxvidcore4:amd64.\n",
      "Preparing to unpack .../059-libxvidcore4_2%3a1.3.7-1_amd64.deb ...\n",
      "Unpacking libxvidcore4:amd64 (2:1.3.7-1) ...\n",
      "Selecting previously unselected package libzvbi-common.\n",
      "Preparing to unpack .../060-libzvbi-common_0.2.35-19_all.deb ...\n",
      "Unpacking libzvbi-common (0.2.35-19) ...\n",
      "Selecting previously unselected package libzvbi0:amd64.\n",
      "Preparing to unpack .../061-libzvbi0_0.2.35-19_amd64.deb ...\n",
      "Unpacking libzvbi0:amd64 (0.2.35-19) ...\n",
      "Selecting previously unselected package libavcodec58:amd64.\n",
      "Preparing to unpack .../062-libavcodec58_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libavcodec58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libraw1394-11:amd64.\n",
      "Preparing to unpack .../063-libraw1394-11_2.1.2-2build2_amd64.deb ...\n",
      "Unpacking libraw1394-11:amd64 (2.1.2-2build2) ...\n",
      "Selecting previously unselected package libavc1394-0:amd64.\n",
      "Preparing to unpack .../064-libavc1394-0_0.5.4-5build2_amd64.deb ...\n",
      "Unpacking libavc1394-0:amd64 (0.5.4-5build2) ...\n",
      "Selecting previously unselected package libass9:amd64.\n",
      "Preparing to unpack .../065-libass9_1%3a0.15.2-1_amd64.deb ...\n",
      "Unpacking libass9:amd64 (1:0.15.2-1) ...\n",
      "Selecting previously unselected package libudfread0:amd64.\n",
      "Preparing to unpack .../066-libudfread0_1.1.2-1_amd64.deb ...\n",
      "Unpacking libudfread0:amd64 (1.1.2-1) ...\n",
      "Selecting previously unselected package libbluray2:amd64.\n",
      "Preparing to unpack .../067-libbluray2_1%3a1.3.1-1_amd64.deb ...\n",
      "Unpacking libbluray2:amd64 (1:1.3.1-1) ...\n",
      "Selecting previously unselected package libchromaprint1:amd64.\n",
      "Preparing to unpack .../068-libchromaprint1_1.5.1-2_amd64.deb ...\n",
      "Unpacking libchromaprint1:amd64 (1.5.1-2) ...\n",
      "Selecting previously unselected package libgme0:amd64.\n",
      "Preparing to unpack .../069-libgme0_0.6.3-2_amd64.deb ...\n",
      "Unpacking libgme0:amd64 (0.6.3-2) ...\n",
      "Selecting previously unselected package libmpg123-0:amd64.\n",
      "Preparing to unpack .../070-libmpg123-0_1.29.3-1build1_amd64.deb ...\n",
      "Unpacking libmpg123-0:amd64 (1.29.3-1build1) ...\n",
      "Selecting previously unselected package libopenmpt0:amd64.\n",
      "Preparing to unpack .../071-libopenmpt0_0.6.1-1_amd64.deb ...\n",
      "Unpacking libopenmpt0:amd64 (0.6.1-1) ...\n",
      "Selecting previously unselected package librabbitmq4:amd64.\n",
      "Preparing to unpack .../072-librabbitmq4_0.10.0-1ubuntu2_amd64.deb ...\n",
      "Unpacking librabbitmq4:amd64 (0.10.0-1ubuntu2) ...\n",
      "Selecting previously unselected package libsrt1.4-gnutls:amd64.\n",
      "Preparing to unpack .../073-libsrt1.4-gnutls_1.4.4-4_amd64.deb ...\n",
      "Unpacking libsrt1.4-gnutls:amd64 (1.4.4-4) ...\n",
      "Selecting previously unselected package libssh-gcrypt-4:amd64.\n",
      "Preparing to unpack .../074-libssh-gcrypt-4_0.9.6-2ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libssh-gcrypt-4:amd64 (0.9.6-2ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libnorm1:amd64.\n",
      "Preparing to unpack .../075-libnorm1_1.5.9+dfsg-2_amd64.deb ...\n",
      "Unpacking libnorm1:amd64 (1.5.9+dfsg-2) ...\n",
      "Selecting previously unselected package libpgm-5.3-0:amd64.\n",
      "Preparing to unpack .../076-libpgm-5.3-0_5.3.128~dfsg-2_amd64.deb ...\n",
      "Unpacking libpgm-5.3-0:amd64 (5.3.128~dfsg-2) ...\n",
      "Selecting previously unselected package libzmq5:amd64.\n",
      "Preparing to unpack .../077-libzmq5_4.3.4-2_amd64.deb ...\n",
      "Unpacking libzmq5:amd64 (4.3.4-2) ...\n",
      "Selecting previously unselected package libavformat58:amd64.\n",
      "Preparing to unpack .../078-libavformat58_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libavformat58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libbs2b0:amd64.\n",
      "Preparing to unpack .../079-libbs2b0_3.1.0+dfsg-2.2build1_amd64.deb ...\n",
      "Unpacking libbs2b0:amd64 (3.1.0+dfsg-2.2build1) ...\n",
      "Selecting previously unselected package libflite1:amd64.\n",
      "Preparing to unpack .../080-libflite1_2.2-3_amd64.deb ...\n",
      "Unpacking libflite1:amd64 (2.2-3) ...\n",
      "Selecting previously unselected package libserd-0-0:amd64.\n",
      "Preparing to unpack .../081-libserd-0-0_0.30.10-2_amd64.deb ...\n",
      "Unpacking libserd-0-0:amd64 (0.30.10-2) ...\n",
      "Selecting previously unselected package libsord-0-0:amd64.\n",
      "Preparing to unpack .../082-libsord-0-0_0.16.8-2_amd64.deb ...\n",
      "Unpacking libsord-0-0:amd64 (0.16.8-2) ...\n",
      "Selecting previously unselected package libsratom-0-0:amd64.\n",
      "Preparing to unpack .../083-libsratom-0-0_0.6.8-1_amd64.deb ...\n",
      "Unpacking libsratom-0-0:amd64 (0.6.8-1) ...\n",
      "Selecting previously unselected package liblilv-0-0:amd64.\n",
      "Preparing to unpack .../084-liblilv-0-0_0.24.12-2_amd64.deb ...\n",
      "Unpacking liblilv-0-0:amd64 (0.24.12-2) ...\n",
      "Selecting previously unselected package libmysofa1:amd64.\n",
      "Preparing to unpack .../085-libmysofa1_1.2.1~dfsg0-1_amd64.deb ...\n",
      "Unpacking libmysofa1:amd64 (1.2.1~dfsg0-1) ...\n",
      "Selecting previously unselected package libasyncns0:amd64.\n",
      "Preparing to unpack .../086-libasyncns0_0.8-6build2_amd64.deb ...\n",
      "Unpacking libasyncns0:amd64 (0.8-6build2) ...\n",
      "Selecting previously unselected package libx11-xcb1:amd64.\n",
      "Preparing to unpack .../087-libx11-xcb1_2%3a1.7.5-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libx11-xcb1:amd64 (2:1.7.5-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libpulse0:amd64.\n",
      "Preparing to unpack .../088-libpulse0_1%3a15.99.1+dfsg1-1ubuntu2.1_amd64.deb ...\n",
      "Unpacking libpulse0:amd64 (1:15.99.1+dfsg1-1ubuntu2.1) ...\n",
      "Selecting previously unselected package libsphinxbase3:amd64.\n",
      "Preparing to unpack .../089-libsphinxbase3_0.8+5prealpha+1-13build1_amd64.deb ...\n",
      "Unpacking libsphinxbase3:amd64 (0.8+5prealpha+1-13build1) ...\n",
      "Selecting previously unselected package libpocketsphinx3:amd64.\n",
      "Preparing to unpack .../090-libpocketsphinx3_0.8.0+real5prealpha+1-14ubuntu1_amd64.deb ...\n",
      "Unpacking libpocketsphinx3:amd64 (0.8.0+real5prealpha+1-14ubuntu1) ...\n",
      "Selecting previously unselected package libpostproc55:amd64.\n",
      "Preparing to unpack .../091-libpostproc55_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libpostproc55:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libsamplerate0:amd64.\n",
      "Preparing to unpack .../092-libsamplerate0_0.2.2-1build1_amd64.deb ...\n",
      "Unpacking libsamplerate0:amd64 (0.2.2-1build1) ...\n",
      "Selecting previously unselected package librubberband2:amd64.\n",
      "Preparing to unpack .../093-librubberband2_2.0.0-2_amd64.deb ...\n",
      "Unpacking librubberband2:amd64 (2.0.0-2) ...\n",
      "Selecting previously unselected package libswscale5:amd64.\n",
      "Preparing to unpack .../094-libswscale5_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libswscale5:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting previously unselected package libvidstab1.1:amd64.\n",
      "Preparing to unpack .../095-libvidstab1.1_1.1.0-2_amd64.deb ...\n",
      "Unpacking libvidstab1.1:amd64 (1.1.0-2) ...\n",
      "Selecting previously unselected package libzimg2:amd64.\n",
      "Preparing to unpack .../096-libzimg2_3.0.3+ds1-1_amd64.deb ...\n",
      "Unpacking libzimg2:amd64 (3.0.3+ds1-1) ...\n",
      "Selecting previously unselected package libavfilter7:amd64.\n",
      "Preparing to unpack .../097-libavfilter7_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libavfilter7:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libcaca0:amd64.\n",
      "Preparing to unpack .../098-libcaca0_0.99.beta19-2.2ubuntu4_amd64.deb ...\n",
      "Unpacking libcaca0:amd64 (0.99.beta19-2.2ubuntu4) ...\n",
      "Selecting previously unselected package libcdio19:amd64.\n",
      "Preparing to unpack .../099-libcdio19_2.1.0-3build1_amd64.deb ...\n",
      "Unpacking libcdio19:amd64 (2.1.0-3build1) ...\n",
      "Selecting previously unselected package libcdio-cdda2:amd64.\n",
      "Preparing to unpack .../100-libcdio-cdda2_10.2+2.0.0-1build3_amd64.deb ...\n",
      "Unpacking libcdio-cdda2:amd64 (10.2+2.0.0-1build3) ...\n",
      "Selecting previously unselected package libcdio-paranoia2:amd64.\n",
      "Preparing to unpack .../101-libcdio-paranoia2_10.2+2.0.0-1build3_amd64.deb ...\n",
      "Unpacking libcdio-paranoia2:amd64 (10.2+2.0.0-1build3) ...\n",
      "Selecting previously unselected package libdc1394-25:amd64.\n",
      "Preparing to unpack .../102-libdc1394-25_2.2.6-4_amd64.deb ...\n",
      "Unpacking libdc1394-25:amd64 (2.2.6-4) ...\n",
      "Selecting previously unselected package libglvnd0:amd64.\n",
      "Preparing to unpack .../103-libglvnd0_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglvnd0:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libglapi-mesa:amd64.\n",
      "Preparing to unpack .../104-libglapi-mesa_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking libglapi-mesa:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package libxcb-dri2-0:amd64.\n",
      "Preparing to unpack .../105-libxcb-dri2-0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-dri2-0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-dri3-0:amd64.\n",
      "Preparing to unpack .../106-libxcb-dri3-0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-dri3-0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-glx0:amd64.\n",
      "Preparing to unpack .../107-libxcb-glx0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-glx0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-present0:amd64.\n",
      "Preparing to unpack .../108-libxcb-present0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-present0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-sync1:amd64.\n",
      "Preparing to unpack .../109-libxcb-sync1_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-sync1:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-xfixes0:amd64.\n",
      "Preparing to unpack .../110-libxcb-xfixes0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-xfixes0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxshmfence1:amd64.\n",
      "Preparing to unpack .../111-libxshmfence1_1.3-1build4_amd64.deb ...\n",
      "Unpacking libxshmfence1:amd64 (1.3-1build4) ...\n",
      "Selecting previously unselected package libxxf86vm1:amd64.\n",
      "Preparing to unpack .../112-libxxf86vm1_1%3a1.1.4-1build3_amd64.deb ...\n",
      "Unpacking libxxf86vm1:amd64 (1:1.1.4-1build3) ...\n",
      "Selecting previously unselected package libdrm-amdgpu1:amd64.\n",
      "Preparing to unpack .../113-libdrm-amdgpu1_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-amdgpu1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libdrm-nouveau2:amd64.\n",
      "Preparing to unpack .../114-libdrm-nouveau2_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-nouveau2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libdrm-radeon1:amd64.\n",
      "Preparing to unpack .../115-libdrm-radeon1_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-radeon1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libllvm15:amd64.\n",
      "Preparing to unpack .../116-libllvm15_1%3a15.0.7-0ubuntu0.22.04.2_amd64.deb ...\n",
      "Unpacking libllvm15:amd64 (1:15.0.7-0ubuntu0.22.04.2) ...\n",
      "Selecting previously unselected package libsensors-config.\n",
      "Preparing to unpack .../117-libsensors-config_1%3a3.6.0-7ubuntu1_all.deb ...\n",
      "Unpacking libsensors-config (1:3.6.0-7ubuntu1) ...\n",
      "Selecting previously unselected package libsensors5:amd64.\n",
      "Preparing to unpack .../118-libsensors5_1%3a3.6.0-7ubuntu1_amd64.deb ...\n",
      "Unpacking libsensors5:amd64 (1:3.6.0-7ubuntu1) ...\n",
      "Selecting previously unselected package libgl1-mesa-dri:amd64.\n",
      "Preparing to unpack .../119-libgl1-mesa-dri_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking libgl1-mesa-dri:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package libglx-mesa0:amd64.\n",
      "Preparing to unpack .../120-libglx-mesa0_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking libglx-mesa0:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package libglx0:amd64.\n",
      "Preparing to unpack .../121-libglx0_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglx0:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgl1:amd64.\n",
      "Preparing to unpack .../122-libgl1_1.4.0-1_amd64.deb ...\n",
      "Unpacking libgl1:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libiec61883-0:amd64.\n",
      "Preparing to unpack .../123-libiec61883-0_1.2.0-4build3_amd64.deb ...\n",
      "Unpacking libiec61883-0:amd64 (1.2.0-4build3) ...\n",
      "Selecting previously unselected package libjack-jackd2-0:amd64.\n",
      "Preparing to unpack .../124-libjack-jackd2-0_1.9.20~dfsg-1_amd64.deb ...\n",
      "Unpacking libjack-jackd2-0:amd64 (1.9.20~dfsg-1) ...\n",
      "Selecting previously unselected package libopenal-data.\n",
      "Preparing to unpack .../125-libopenal-data_1%3a1.19.1-2build3_all.deb ...\n",
      "Unpacking libopenal-data (1:1.19.1-2build3) ...\n",
      "Selecting previously unselected package libsndio7.0:amd64.\n",
      "Preparing to unpack .../126-libsndio7.0_1.8.1-1.1_amd64.deb ...\n",
      "Unpacking libsndio7.0:amd64 (1.8.1-1.1) ...\n",
      "Selecting previously unselected package libopenal1:amd64.\n",
      "Preparing to unpack .../127-libopenal1_1%3a1.19.1-2build3_amd64.deb ...\n",
      "Unpacking libopenal1:amd64 (1:1.19.1-2build3) ...\n",
      "Selecting previously unselected package libwayland-client0:amd64.\n",
      "Preparing to unpack .../128-libwayland-client0_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libwayland-client0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libdecor-0-0:amd64.\n",
      "Preparing to unpack .../129-libdecor-0-0_0.1.0-3build1_amd64.deb ...\n",
      "Unpacking libdecor-0-0:amd64 (0.1.0-3build1) ...\n",
      "Selecting previously unselected package libwayland-server0:amd64.\n",
      "Preparing to unpack .../130-libwayland-server0_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libwayland-server0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libgbm1:amd64.\n",
      "Preparing to unpack .../131-libgbm1_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking libgbm1:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package libwayland-cursor0:amd64.\n",
      "Preparing to unpack .../132-libwayland-cursor0_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libwayland-cursor0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libwayland-egl1:amd64.\n",
      "Preparing to unpack .../133-libwayland-egl1_1.20.0-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libwayland-egl1:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libxcursor1:amd64.\n",
      "Preparing to unpack .../134-libxcursor1_1%3a1.2.0-2build4_amd64.deb ...\n",
      "Unpacking libxcursor1:amd64 (1:1.2.0-2build4) ...\n",
      "Selecting previously unselected package libxi6:amd64.\n",
      "Preparing to unpack .../135-libxi6_2%3a1.8-1build1_amd64.deb ...\n",
      "Unpacking libxi6:amd64 (2:1.8-1build1) ...\n",
      "Selecting previously unselected package libxinerama1:amd64.\n",
      "Preparing to unpack .../136-libxinerama1_2%3a1.1.4-3_amd64.deb ...\n",
      "Unpacking libxinerama1:amd64 (2:1.1.4-3) ...\n",
      "Selecting previously unselected package libxkbcommon0:amd64.\n",
      "Preparing to unpack .../137-libxkbcommon0_1.4.0-1_amd64.deb ...\n",
      "Unpacking libxkbcommon0:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libxrandr2:amd64.\n",
      "Preparing to unpack .../138-libxrandr2_2%3a1.5.2-1build1_amd64.deb ...\n",
      "Unpacking libxrandr2:amd64 (2:1.5.2-1build1) ...\n",
      "Selecting previously unselected package x11-common.\n",
      "Preparing to unpack .../139-x11-common_1%3a7.7+23ubuntu2_all.deb ...\n",
      "Unpacking x11-common (1:7.7+23ubuntu2) ...\n",
      "Selecting previously unselected package libxss1:amd64.\n",
      "Preparing to unpack .../140-libxss1_1%3a1.2.3-1build2_amd64.deb ...\n",
      "Unpacking libxss1:amd64 (1:1.2.3-1build2) ...\n",
      "Selecting previously unselected package libsdl2-2.0-0:amd64.\n",
      "Preparing to unpack .../141-libsdl2-2.0-0_2.0.20+dfsg-2ubuntu1.22.04.1_amd64.deb ...\n",
      "Unpacking libsdl2-2.0-0:amd64 (2.0.20+dfsg-2ubuntu1.22.04.1) ...\n",
      "Selecting previously unselected package libxcb-shape0:amd64.\n",
      "Preparing to unpack .../142-libxcb-shape0_1.14-3ubuntu3_amd64.deb ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking libxcb-shape0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxv1:amd64.\n",
      "Preparing to unpack .../143-libxv1_2%3a1.0.11-1build2_amd64.deb ...\n",
      "Unpacking libxv1:amd64 (2:1.0.11-1build2) ...\n",
      "Selecting previously unselected package libavdevice58:amd64.\n",
      "Preparing to unpack .../144-libavdevice58_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libavdevice58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package ffmpeg.\n",
      "Preparing to unpack .../145-ffmpeg_7%3a4.4.2-0ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking ffmpeg (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libigdgmm12:amd64.\n",
      "Preparing to unpack .../146-libigdgmm12_22.1.2+ds1-1_amd64.deb ...\n",
      "Unpacking libigdgmm12:amd64 (22.1.2+ds1-1) ...\n",
      "Selecting previously unselected package intel-media-va-driver:amd64.\n",
      "Preparing to unpack .../147-intel-media-va-driver_22.3.1+dfsg1-1ubuntu2_amd64.deb ...\n",
      "Unpacking intel-media-va-driver:amd64 (22.3.1+dfsg1-1ubuntu2) ...\n",
      "Selecting previously unselected package libaacs0:amd64.\n",
      "Preparing to unpack .../148-libaacs0_0.11.1-1_amd64.deb ...\n",
      "Unpacking libaacs0:amd64 (0.11.1-1) ...\n",
      "Selecting previously unselected package libbdplus0:amd64.\n",
      "Preparing to unpack .../149-libbdplus0_0.2.0-1_amd64.deb ...\n",
      "Unpacking libbdplus0:amd64 (0.2.0-1) ...\n",
      "Selecting previously unselected package libdecor-0-plugin-1-cairo:amd64.\n",
      "Preparing to unpack .../150-libdecor-0-plugin-1-cairo_0.1.0-3build1_amd64.deb ...\n",
      "Unpacking libdecor-0-plugin-1-cairo:amd64 (0.1.0-3build1) ...\n",
      "Selecting previously unselected package libpciaccess0:amd64.\n",
      "Preparing to unpack .../151-libpciaccess0_0.16-3_amd64.deb ...\n",
      "Unpacking libpciaccess0:amd64 (0.16-3) ...\n",
      "Selecting previously unselected package libdrm-intel1:amd64.\n",
      "Preparing to unpack .../152-libdrm-intel1_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-intel1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libgdk-pixbuf2.0-bin.\n",
      "Preparing to unpack .../153-libgdk-pixbuf2.0-bin_2.42.8+dfsg-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libgdk-pixbuf2.0-bin (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libgl1-amber-dri:amd64.\n",
      "Preparing to unpack .../154-libgl1-amber-dri_21.3.7-0ubuntu1_amd64.deb ...\n",
      "Unpacking libgl1-amber-dri:amd64 (21.3.7-0ubuntu1) ...\n",
      "Selecting previously unselected package libice6:amd64.\n",
      "Preparing to unpack .../155-libice6_2%3a1.0.10-1build2_amd64.deb ...\n",
      "Unpacking libice6:amd64 (2:1.0.10-1build2) ...\n",
      "Selecting previously unselected package librsvg2-common:amd64.\n",
      "Preparing to unpack .../156-librsvg2-common_2.52.5+dfsg-3_amd64.deb ...\n",
      "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3) ...\n",
      "Selecting previously unselected package libsm6:amd64.\n",
      "Preparing to unpack .../157-libsm6_2%3a1.2.3-1build2_amd64.deb ...\n",
      "Unpacking libsm6:amd64 (2:1.2.3-1build2) ...\n",
      "Selecting previously unselected package mesa-va-drivers:amd64.\n",
      "Preparing to unpack .../158-mesa-va-drivers_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking mesa-va-drivers:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package mesa-vdpau-drivers:amd64.\n",
      "Preparing to unpack .../159-mesa-vdpau-drivers_22.2.5-0ubuntu0.1~22.04.3_amd64.deb ...\n",
      "Unpacking mesa-vdpau-drivers:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Selecting previously unselected package i965-va-driver:amd64.\n",
      "Preparing to unpack .../160-i965-va-driver_2.4.1+dfsg1-1_amd64.deb ...\n",
      "Unpacking i965-va-driver:amd64 (2.4.1+dfsg1-1) ...\n",
      "Selecting previously unselected package va-driver-all:amd64.\n",
      "Preparing to unpack .../161-va-driver-all_2.14.0-1_amd64.deb ...\n",
      "Unpacking va-driver-all:amd64 (2.14.0-1) ...\n",
      "Selecting previously unselected package vdpau-driver-all:amd64.\n",
      "Preparing to unpack .../162-vdpau-driver-all_1.4-3build2_amd64.deb ...\n",
      "Unpacking vdpau-driver-all:amd64 (1.4-3build2) ...\n",
      "Selecting previously unselected package pocketsphinx-en-us.\n",
      "Preparing to unpack .../163-pocketsphinx-en-us_0.8.0+real5prealpha+1-14ubuntu1_all.deb ...\n",
      "Unpacking pocketsphinx-en-us (0.8.0+real5prealpha+1-14ubuntu1) ...\n",
      "Setting up libgme0:amd64 (0.6.3-2) ...\n",
      "Setting up libssh-gcrypt-4:amd64 (0.9.6-2ubuntu0.22.04.1) ...\n",
      "Setting up libgraphite2-3:amd64 (1.3.14-1build2) ...\n",
      "Setting up libsrt1.4-gnutls:amd64 (1.4.4-4) ...\n",
      "Setting up libxcb-dri3-0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libpixman-1-0:amd64 (0.40.0-1ubuntu0.22.04.1) ...\n",
      "Setting up libudfread0:amd64 (1.1.2-1) ...\n",
      "Setting up libwayland-server0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Setting up libaom3:amd64 (3.3.0-1) ...\n",
      "Setting up libx11-xcb1:amd64 (2:1.7.5-1ubuntu0.2) ...\n",
      "Setting up libpciaccess0:amd64 (0.16-3) ...\n",
      "Setting up librabbitmq4:amd64 (0.10.0-1ubuntu2) ...\n",
      "Setting up libraw1394-11:amd64 (2.1.2-2build2) ...\n",
      "Setting up libcodec2-1.0:amd64 (1.0.1-3) ...\n",
      "Setting up libmpg123-0:amd64 (1.29.3-1build1) ...\n",
      "Setting up libxcb-xfixes0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libspeex1:amd64 (1.2~rc1.2-1.1ubuntu3) ...\n",
      "Setting up libshine3:amd64 (3.1.1-2) ...\n",
      "Setting up libxi6:amd64 (2:1.8-1build1) ...\n",
      "Setting up libtwolame0:amd64 (0.4.0-2build2) ...\n",
      "Setting up libxrender1:amd64 (1:0.9.10-1build4) ...\n",
      "Setting up libdatrie1:amd64 (0.2.13-2) ...\n",
      "Setting up libxcb-render0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libsoxr0:amd64 (0.1.3-4build2) ...\n",
      "Setting up libglvnd0:amd64 (1.4.0-1) ...\n",
      "Setting up libpgm-5.3-0:amd64 (5.3.128~dfsg-2) ...\n",
      "Setting up libxcb-glx0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libgdk-pixbuf2.0-common (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Setting up libnorm1:amd64 (1.5.9+dfsg-2) ...\n",
      "Setting up libmysofa1:amd64 (1.2.1~dfsg0-1) ...\n",
      "Setting up libxcb-shape0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up x11-common (1:7.7+23ubuntu2) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
      "debconf: falling back to frontend: Readline\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up libsensors-config (1:3.6.0-7ubuntu1) ...\n",
      "Setting up libdeflate0:amd64 (1.10-2) ...\n",
      "Setting up xkb-data (2.33-1) ...\n",
      "Setting up libxcb-shm0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libigdgmm12:amd64 (22.1.2+ds1-1) ...\n",
      "Setting up libcdio19:amd64 (2.1.0-3build1) ...\n",
      "Setting up libxvidcore4:amd64 (2:1.3.7-1) ...\n",
      "Setting up libjbig0:amd64 (2.1-3.1ubuntu0.22.04.1) ...\n",
      "Setting up libxxf86vm1:amd64 (1:1.1.4-1build3) ...\n",
      "Setting up libxcb-present0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libslang2:amd64 (2.3.2-5build4) ...\n",
      "Setting up libva2:amd64 (2.14.0-1) ...\n",
      "Setting up libfreetype6:amd64 (2.11.1+dfsg-1ubuntu0.2) ...\n",
      "Setting up libxfixes3:amd64 (1:6.0.0-1) ...\n",
      "Setting up libxcb-sync1:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libx264-163:amd64 (2:0.163.3060+git5db6aa6-2build1) ...\n",
      "Setting up libfribidi0:amd64 (1.0.8-2ubuntu3.1) ...\n",
      "Setting up libxinerama1:amd64 (2:1.1.4-3) ...\n",
      "Setting up intel-media-va-driver:amd64 (22.3.1+dfsg1-1ubuntu2) ...\n",
      "Setting up libxv1:amd64 (2:1.0.11-1build2) ...\n",
      "Setting up libxrandr2:amd64 (2:1.5.2-1build1) ...\n",
      "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
      "Setting up libsensors5:amd64 (1:3.6.0-7ubuntu1) ...\n",
      "Setting up libaacs0:amd64 (0.11.1-1) ...\n",
      "Setting up pocketsphinx-en-us (0.8.0+real5prealpha+1-14ubuntu1) ...\n",
      "Setting up libglapi-mesa:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libx265-199:amd64 (3.5-2) ...\n",
      "Setting up libwebp7:amd64 (1.2.2-2ubuntu0.22.04.1) ...\n",
      "Setting up libsndio7.0:amd64 (1.8.1-1.1) ...\n",
      "Setting up libxcb-dri2-0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libbdplus0:amd64 (0.2.0-1) ...\n",
      "Setting up libvidstab1.1:amd64 (1.1.0-2) ...\n",
      "Setting up libflite1:amd64 (2.2-3) ...\n",
      "Setting up ocl-icd-libopencl1:amd64 (2.2.14-3) ...\n",
      "Setting up libasyncns0:amd64 (0.8-6build2) ...\n",
      "Setting up libxshmfence1:amd64 (1.3-1build4) ...\n",
      "Setting up libvdpau1:amd64 (1.4-3build2) ...\n",
      "Setting up libbs2b0:amd64 (3.1.0+dfsg-2.2build1) ...\n",
      "Setting up libzimg2:amd64 (3.0.3+ds1-1) ...\n",
      "Setting up libopenjp2-7:amd64 (2.4.0-6) ...\n",
      "Setting up libharfbuzz0b:amd64 (2.7.4-1ubuntu3.1) ...\n",
      "Setting up libopenal-data (1:1.19.1-2build3) ...\n",
      "Setting up libthai-data (0.1.29-1build1) ...\n",
      "Setting up libvpx7:amd64 (1.11.0-2ubuntu2) ...\n",
      "Setting up libtiff5:amd64 (4.3.0-6ubuntu0.4) ...\n",
      "Setting up libwayland-egl1:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Setting up libxss1:amd64 (1:1.2.3-1build2) ...\n",
      "Setting up libusb-1.0-0:amd64 (2:1.0.25-1ubuntu2) ...\n",
      "Setting up libdav1d5:amd64 (0.9.2-1) ...\n",
      "Setting up libmfx1:amd64 (22.3.0-1) ...\n",
      "Setting up libsamplerate0:amd64 (0.2.2-1build1) ...\n",
      "Setting up libwebpmux3:amd64 (1.2.2-2ubuntu0.22.04.1) ...\n",
      "Setting up libdrm-common (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libelf1:amd64 (0.186-1build1) ...\n",
      "Setting up libopenmpt0:amd64 (0.6.1-1) ...\n",
      "Setting up libzvbi-common (0.2.35-19) ...\n",
      "Setting up libmp3lame0:amd64 (3.100-3build2) ...\n",
      "Setting up libicu70:amd64 (70.1-2) ...\n",
      "Setting up libiec61883-0:amd64 (1.2.0-4build3) ...\n",
      "Setting up libserd-0-0:amd64 (0.30.10-2) ...\n",
      "Setting up libxkbcommon0:amd64 (1.4.0-1) ...\n",
      "Setting up libwayland-client0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Setting up libavc1394-0:amd64 (0.5.4-5build2) ...\n",
      "Setting up libzvbi0:amd64 (0.2.35-19) ...\n",
      "Setting up libice6:amd64 (2:1.0.10-1build2) ...\n",
      "Setting up libzmq5:amd64 (4.3.4-2) ...\n",
      "Setting up libcaca0:amd64 (0.99.beta19-2.2ubuntu4) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up libpulse0:amd64 (1:15.99.1+dfsg1-1ubuntu2.1) ...\n",
      "Setting up libcdio-cdda2:amd64 (10.2+2.0.0-1build3) ...\n",
      "Setting up fontconfig-config (2.13.1-4.2ubuntu5) ...\n",
      "Setting up libcdio-paranoia2:amd64 (10.2+2.0.0-1build3) ...\n",
      "Setting up libxcursor1:amd64 (1:1.2.0-2build4) ...\n",
      "Setting up libopenal1:amd64 (1:1.19.1-2build3) ...\n",
      "Setting up libthai0:amd64 (0.1.29-1build1) ...\n",
      "Setting up libdc1394-25:amd64 (2.2.6-4) ...\n",
      "Setting up librubberband2:amd64 (2.0.0-2) ...\n",
      "Setting up libjack-jackd2-0:amd64 (1.9.20~dfsg-1) ...\n",
      "Setting up libdrm2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libva-drm2:amd64 (2.14.0-1) ...\n",
      "Setting up libsord-0-0:amd64 (0.16.8-2) ...\n",
      "Setting up libwayland-cursor0:amd64 (1.20.0-1ubuntu0.1) ...\n",
      "Setting up libsratom-0-0:amd64 (0.6.8-1) ...\n",
      "Setting up libdecor-0-0:amd64 (0.1.0-3build1) ...\n",
      "Setting up libfontconfig1:amd64 (2.13.1-4.2ubuntu5) ...\n",
      "Setting up libva-x11-2:amd64 (2.14.0-1) ...\n",
      "Setting up libsm6:amd64 (2:1.2.3-1build2) ...\n",
      "Setting up liblilv-0-0:amd64 (0.24.12-2) ...\n",
      "Setting up libxml2:amd64 (2.9.13+dfsg-1ubuntu0.3) ...\n",
      "Setting up libdrm-amdgpu1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up fontconfig (2.13.1-4.2ubuntu5) ...\n",
      "Regenerating fonts cache... done.\n",
      "Setting up libdrm-nouveau2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libgbm1:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libsphinxbase3:amd64 (0.8+5prealpha+1-13build1) ...\n",
      "Setting up libdrm-radeon1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libpango-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Setting up libdrm-intel1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libcairo2:amd64 (1.16.0-5ubuntu2) ...\n",
      "Setting up libavutil56:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libpocketsphinx3:amd64 (0.8.0+real5prealpha+1-14ubuntu1) ...\n",
      "Setting up libass9:amd64 (1:0.15.2-1) ...\n",
      "Setting up shared-mime-info (2.1-2) ...\n",
      "Setting up libpostproc55:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libllvm15:amd64 (1:15.0.7-0ubuntu0.22.04.2) ...\n",
      "Setting up libtheora0:amd64 (1.1.1+dfsg.1-15ubuntu4) ...\n",
      "Setting up libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Setting up libswscale5:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libcairo-gobject2:amd64 (1.16.0-5ubuntu2) ...\n",
      "Setting up mesa-va-drivers:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libpangoft2-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Setting up libbluray2:amd64 (1:1.3.1-1) ...\n",
      "Setting up libsdl2-2.0-0:amd64 (2.0.20+dfsg-2ubuntu1.22.04.1) ...\n",
      "Setting up i965-va-driver:amd64 (2.4.1+dfsg1-1) ...\n",
      "Setting up libpangocairo-1.0-0:amd64 (1.50.6+ds-2ubuntu1) ...\n",
      "Setting up libgl1-amber-dri:amd64 (21.3.7-0ubuntu1) ...\n",
      "Setting up mesa-vdpau-drivers:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libgl1-mesa-dri:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libswresample3:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up librsvg2-2:amd64 (2.52.5+dfsg-3) ...\n",
      "Setting up va-driver-all:amd64 (2.14.0-1) ...\n",
      "Setting up libdecor-0-plugin-1-cairo:amd64 (0.1.0-3build1) ...\n",
      "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3) ...\n",
      "Setting up vdpau-driver-all:amd64 (1.4-3build2) ...\n",
      "Setting up libgdk-pixbuf2.0-bin (2.42.8+dfsg-1ubuntu0.2) ...\n",
      "Setting up libavcodec58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libchromaprint1:amd64 (1.5.1-2) ...\n",
      "Setting up libglx-mesa0:amd64 (22.2.5-0ubuntu0.1~22.04.3) ...\n",
      "Setting up libglx0:amd64 (1.4.0-1) ...\n",
      "Setting up libavformat58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libgl1:amd64 (1.4.0-1) ...\n",
      "Setting up libavfilter7:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up libavdevice58:amd64 (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Setting up ffmpeg (7:4.4.2-0ubuntu0.22.04.1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.2) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt-get install ffmpeg libsm6 libxext6 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73946bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA-Adapter from ckpts/7B.pth\n",
      "model args: ModelArgs(dim=4096, n_layers=32, n_heads=32, vocab_size=-1, multiple_of=256, norm_eps=1e-06, max_batch_size=1, max_seq_len=512, w_bias=True, w_lora=True, lora_rank=16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLaMA_adapter(\n",
       "  (image_bind): ImageBindModel(\n",
       "    (modality_preprocessors): ModuleDict(\n",
       "      (vision): RGBDTPreprocessor(\n",
       "        (cls_token): tensor((1, 1, 1280), requires_grad=False)\n",
       "        \n",
       "        (rgbt_stem): PatchEmbedGeneric(\n",
       "          (proj): Sequential(\n",
       "            (0): PadIm2Video()\n",
       "            (1): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "          (pos_embed): tensor((1, 257, 1280), requires_grad=False)\n",
       "          \n",
       "        )\n",
       "      )\n",
       "      (text): TextPreprocessor(\n",
       "        (pos_embed): tensor((1, 77, 1024), requires_grad=False)\n",
       "        (mask): tensor((77, 77), requires_grad=False)\n",
       "        \n",
       "        (token_embedding): Embedding(49408, 1024)\n",
       "      )\n",
       "      (audio): AudioPreprocessor(\n",
       "        (cls_token): tensor((1, 1, 768), requires_grad=False)\n",
       "        \n",
       "        (rgbt_stem): PatchEmbedGeneric(\n",
       "          (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10), bias=False)\n",
       "          (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "          (pos_embed): tensor((1, 229, 768), requires_grad=False)\n",
       "          \n",
       "        )\n",
       "      )\n",
       "      (depth): RGBDTPreprocessor(\n",
       "        (cls_token): tensor((1, 1, 384), requires_grad=False)\n",
       "        \n",
       "        (depth_stem): PatchEmbedGeneric(\n",
       "          (proj): Conv2d(1, 384, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "          (norm_layer): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "          (pos_embed): tensor((1, 197, 384), requires_grad=False)\n",
       "          \n",
       "        )\n",
       "      )\n",
       "      (thermal): ThermalPreprocessor(\n",
       "        (cls_token): tensor((1, 1, 768), requires_grad=False)\n",
       "        \n",
       "        (rgbt_stem): PatchEmbedGeneric(\n",
       "          (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "          (norm_layer): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_embedding_helper): SpatioTemporalPosEmbeddingHelper(\n",
       "          (pos_embed): tensor((1, 197, 768), requires_grad=False)\n",
       "          \n",
       "        )\n",
       "      )\n",
       "      (imu): IMUPreprocessor(\n",
       "        (pos_embed): tensor((1, 251, 512), requires_grad=False)\n",
       "        (cls_token): tensor((1, 1, 512), requires_grad=False)\n",
       "        \n",
       "        (imu_stem): PatchEmbedGeneric(\n",
       "          (proj): Linear(in_features=48, out_features=512, bias=False)\n",
       "          (norm_layer): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (modality_trunks): ModuleDict(\n",
       "      (vision): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (12): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (13): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (14): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (15): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (16): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (17): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (18): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (19): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (20): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (21): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (22): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (23): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (24): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (25): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (26): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (27): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (28): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (29): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (30): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (31): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "      (text): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (12): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (13): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (14): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (15): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (16): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (17): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (18): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (19): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (20): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (21): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (22): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (23): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "      (audio): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.009)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.018)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.027)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.036)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.045)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.055)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.064)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.073)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.082)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.091)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "      (depth): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "      (thermal): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (6): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (7): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (8): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (9): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (10): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (11): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "      (imu): SimpleTransformer(\n",
       "        (pre_transformer_layer): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): EinOpsRearrange()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (1): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.140)\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (2): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.280)\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (3): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.420)\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (4): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.560)\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (5): BlockWithMasking(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.700)\n",
       "            (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (post_transformer_layer): EinOpsRearrange()\n",
       "      )\n",
       "    )\n",
       "    (modality_heads): ModuleDict(\n",
       "      (vision): Sequential(\n",
       "        (0): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): SelectElement()\n",
       "        (2): Linear(in_features=1280, out_features=1024, bias=False)\n",
       "      )\n",
       "      (text): SelectEOSAndProject(\n",
       "        (proj): Sequential(\n",
       "          (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (audio): Sequential(\n",
       "        (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): SelectElement()\n",
       "        (2): Linear(in_features=768, out_features=1024, bias=False)\n",
       "      )\n",
       "      (depth): Sequential(\n",
       "        (0): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): SelectElement()\n",
       "        (2): Linear(in_features=384, out_features=1024, bias=False)\n",
       "      )\n",
       "      (thermal): Sequential(\n",
       "        (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): SelectElement()\n",
       "        (2): Linear(in_features=768, out_features=1024, bias=False)\n",
       "      )\n",
       "      (imu): Sequential(\n",
       "        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): SelectElement()\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      )\n",
       "      (point): Sequential(\n",
       "        (0): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): Linear(in_features=512, out_features=1024, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (modality_postprocessors): ModuleDict(\n",
       "      (vision): Normalize()\n",
       "      (text): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n",
       "      )\n",
       "      (audio): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=20.0,learnable=False, max_logit_scale=100)\n",
       "      )\n",
       "      (depth): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
       "      )\n",
       "      (thermal): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=10.0,learnable=False, max_logit_scale=100)\n",
       "      )\n",
       "      (imu): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=5.0,learnable=False, max_logit_scale=100)\n",
       "      )\n",
       "      (point): Sequential(\n",
       "        (0): Normalize()\n",
       "        (1): LearnableLogitScaling(logit_scale_init=1.0,learnable=False, max_logit_scale=100)\n",
       "      )\n",
       "    )\n",
       "    (point_trunk): PointTransformerBind(\n",
       "      (point_encoder): PointTransformer(\n",
       "        (group_divider): Group()\n",
       "        (encoder): Encoder(\n",
       "          (first_conv): Sequential(\n",
       "            (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (second_conv): Sequential(\n",
       "            (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace=True)\n",
       "            (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (reduce_dim): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (pos_embed): Sequential(\n",
       "          (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=128, out_features=384, bias=True)\n",
       "        )\n",
       "        (blocks): TransformerEncoder(\n",
       "          (blocks): ModuleList(\n",
       "            (0): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.018)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.027)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.036)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.045)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.055)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.064)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.073)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.082)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.091)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): Block(\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (image_bind_proj): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (image_bind_norm_1): RMSNorm()\n",
       "  (image_bind_f1_1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (image_bind_f2_1): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "  (image_bind_f3_1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (image_bind_norm_2): RMSNorm()\n",
       "  (image_bind_f1_2): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (image_bind_f2_2): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "  (image_bind_f3_2): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (image_bind_norm_3): RMSNorm()\n",
       "  (image_bind_f1_3): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (image_bind_f2_3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "  (image_bind_f3_3): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "  (llama): Transformer(\n",
       "    (tok_embeddings): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (6): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (7): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (8): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (9): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (10): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (11): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (12): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (13): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (14): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (15): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (16): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (17): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (18): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (19): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (20): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (21): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (22): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (23): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (24): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (25): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (26): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (27): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (28): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (29): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (30): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "      (31): TransformerBlock(\n",
       "        (attention): Attention(\n",
       "          (wq): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_wq_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wq_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wk_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wk_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wv_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wv_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_wo_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_wo_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "        )\n",
       "        (feed_forward): FeedForward(\n",
       "          (w1): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (w2): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (w3): Linear(in_features=4096, out_features=11008, bias=True)\n",
       "          (lora_w1_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w1_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "          (lora_w2_l1): Linear(in_features=11008, out_features=16, bias=False)\n",
       "          (lora_w2_l2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "          (lora_w3_l1): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (lora_w3_l2): Linear(in_features=16, out_features=11008, bias=False)\n",
       "        )\n",
       "        (attention_norm): RMSNorm()\n",
       "        (ffn_norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "    (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       "  (prefix_query): Embedding(32, 4096)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ImageBind.data as data\n",
    "import llama\n",
    "\n",
    "\n",
    "llama_dir = \"/home/u8915687/lab/big-superb/Macaw-LLM2/weights/llama/\"\n",
    "\n",
    "# checkpoint will be automatically downloaded\n",
    "model = llama.load(\"7B\", llama_dir, knn=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62b8462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ImageBind.data as data\n",
    "\n",
    "audio = data.load_and_transform_audio_data(['/home/u8915687/lab/big-superb/Macaw-LLM2/data/audio_sample/LJ001-0001.wav'], device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be93b9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 128, 204])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50f4517c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What does the audio mean?\n",
      "\n",
      "### Response:\n",
      "The audio is a narration of the text, which provides a spoken explanation of the content on the page.\n"
     ]
    }
   ],
   "source": [
    "inputs = {}\n",
    "# image = data.load_and_transform_vision_data([\"/home/u8915687/lab/big-superb/Macaw-LLM2/data/image_sample/COCO_train2014_000000344896.jpg\"], device='cuda')\n",
    "# image = data.load_and_transform_vision_data([\"/home/u8915687/lab/big-superb/Macaw-LLM2/data/image_sample/COCO_train2014_000000492606.jpg\"], device=\"cuda\")\n",
    "# image = data.load_and_transform_vision_data([\"/home/u8915687/lab/big-superb/Macaw-LLM2/data/image_sample/COCO_train2014_000000407061.jpg\"], device=\"cuda\")\n",
    "# inputs['Image'] = [image, 1]\n",
    "\n",
    "audio = data.load_and_transform_audio_data(['/home/u8915687/lab/big-superb/Macaw-LLM2/data/audio_sample/LJ001-0001.wav'], device='cuda')\n",
    "inputs['Audio'] = [audio, 1]\n",
    "# print(audio.size())\n",
    "\n",
    "\n",
    "prompt = llama.format_prompt(\"What does the audio mean?\")\n",
    "# prompt = llama.format_prompt(\"What animal is present in the audio?\")\n",
    "# prompt = llama.format_prompt(\"What is the image about?\")\n",
    "print(prompt)\n",
    "results = model.generate(\n",
    "    inputs,\n",
    "    [prompt],\n",
    "    max_gen_len=256,\n",
    "    temperature=0,\n",
    "    top_p=0\n",
    ")\n",
    "result = results[0].strip()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c690283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c280ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a0bd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/u8915687/miniconda3/envs/imagebind_LLM/lib/python3.9/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ImageBind.data as data\n",
    "import torch\n",
    "from datasets import load_dataset, Audio, Dataset\n",
    "dataset = load_dataset(\"SpeechBigBench/SpeechTextMatching_LibriSpeechTestOther\")[\"test\"]\n",
    "dataset = Dataset.from_dict(dataset[:100])\n",
    "\n",
    "audios = []\n",
    "for i in range(len(dataset)):\n",
    "    audios.append(\n",
    "        ( torch.tensor([dataset[i][\"audio\"][\"array\"]]), dataset[i][\"audio\"][\"sampling_rate\"])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bd38ffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_audio = data.my_load_and_transform_audio_data(audios=audios, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b77b6276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ImageBind.data as data\n",
    "import torch\n",
    "from datasets import load_dataset, Audio, Dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"SpeechBigBench/SpeechTextMatching_LJSpeech\",\n",
    "    cache_dir=\"/home/u8915687/.cache/huggingface/datasets\", \n",
    "    split=\"test\"\n",
    ").shuffle(seed=42)\n",
    "dataset = Dataset.from_dict(dataset[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e9d88e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine if the speech and text are matched. The answer could be yes or no.\n",
      "\n",
      "### Text: This was presently superseded by a fresh catch \n",
      "\n",
      "### Response:\n",
      "====================================================================================================\n",
      "The text and speech are not matched. The text says \"This was presently superseded by a fresh catch,\" while the speech says \"This is a picture of a pizza.\"\n",
      "====================================================================================================\n",
      "no\n",
      "####################################################################################################\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Decide if the speech maintains the organization of the written content. The answer could be yes or no.\n",
      "\n",
      "### Text: was shipped from Los Angeles on March twenty nineteen sixtythree and he left for New Orleans on April twentyfour nineteen sixtythree\n",
      "\n",
      "### Response:\n",
      "====================================================================================================\n",
      "The speech does not maintain the organization of the written content. The text is presented in a disorganized manner, with the dates of departure and arrival not following the same format.\n",
      "====================================================================================================\n",
      "yes\n",
      "####################################################################################################\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Does the speech align with the intended message of the text? The answer could be yes or no.\n",
      "\n",
      "### Text: there have been attempts on the lives of one out of every three\n",
      "\n",
      "### Response:\n",
      "====================================================================================================\n",
      "The speech seems to align with the intended message of the text, which is to inform the audience about the prevalence of attempted murders in the United States.\n",
      "====================================================================================================\n",
      "no\n",
      "####################################################################################################\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Assess if the speech and text correspond to each other. The answer could be yes or no.\n",
      "\n",
      "### Text: Catherine Wilson the poisoner was reserved and reticent to the last expressing no contrition but also no fear \n",
      "\n",
      "### Response:\n",
      "====================================================================================================\n",
      "Yes, the text and speech correspond to each other. The text describes Catherine Wilson as reserved and reticent, while the speech shows that she expresses no contrition and no fear.\n",
      "====================================================================================================\n",
      "yes\n",
      "####################################################################################################\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Judge if the speech and the text express the same ideas. The answer could be yes or no.\n",
      "\n",
      "### Text: They had had serious work to get at the diamonds It was necessary to force one heavy door from its hinges and to cut through the thick panels of another\n",
      "\n",
      "### Response:\n",
      "====================================================================================================\n",
      "The text and the speech express the same ideas.\n",
      "====================================================================================================\n",
      "yes\n",
      "####################################################################################################\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine whether the given speech and text pair are a match or not. The answer could be yes or no.\n",
      "\n",
      "### Text: At times the numbers congregated together were very great as many as fifty and sixty even more were crowded indiscriminately into the pressyard\n",
      "\n",
      "### Response:\n",
      "====================================================================================================\n",
      "No, the given speech and text pair are not a match. The text is about the numbers of animals in a pressyard, while the speech is about the numbers of people in a crowd.\n",
      "====================================================================================================\n",
      "yes\n",
      "####################################################################################################\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Verify if the speech and text are congruent. The answer could be yes or no.\n",
      "\n",
      "### Text: who understand those modern conditions\n",
      "\n",
      "### Response:\n",
      "====================================================================================================\n",
      "No, the text and speech do not match. The text says \"who understand those modern conditions,\" while the speech says \"who understands the modern conditions?\"\n",
      "====================================================================================================\n",
      "yes\n",
      "####################################################################################################\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine if the speech matches the text or not. The answer could be yes or no.\n",
      "\n",
      "### Text: he with unusual quickness of apprehension discovered and promptly turned to account an inexcusably lax system of management\n",
      "\n",
      "### Response:\n",
      "====================================================================================================\n",
      "No, the speech does not match the text. The text is a description of a children's book, while the speech is a description of a person's reaction to the book.\n",
      "====================================================================================================\n",
      "yes\n",
      "####################################################################################################\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Decide if the speech and text match each other precisely. The answer could be yes or no.\n",
      "\n",
      "### Text: Forrester the detective who had pursued and captured Burgess at Boston tracked Ker to France and following him there eventually captured him\n",
      "\n",
      "### Response:\n",
      "====================================================================================================\n",
      "No, the text and speech do not match each other precisely. The text states that Forrester the detective pursued and captured Burgess at the Boston track, while the speech reads \"Forrester the detective who had pursued and captured Burgess at Boston tracked Ker to France and following him there\n",
      "====================================================================================================\n",
      "yes\n",
      "####################################################################################################\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine if the speech and the  convey the same meaning or not. The answer could be yes or no.\n",
      "\n",
      "### Text: In the crowd all of them persons who had no other avocation or mode of livelihood but thieving Mr Buxton counted eleven children\n",
      "\n",
      "### Response:\n",
      "====================================================================================================\n",
      "The text and the speech convey the same meaning. The text describes the scene, while the speech provides a description of the scene. Both convey the same information about the people in the crowd and the children.\n",
      "====================================================================================================\n",
      "no\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cuda\")\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(dataset)):\n",
    "    inputs = {}\n",
    "    \n",
    "    waveform, sr = torch.tensor([dataset[i][\"audio\"][\"array\"]]), dataset[i][\"audio\"][\"sampling_rate\"]\n",
    "    audio = data.my_load_and_transform_audio_data(audios=[(waveform, sr)], device=\"cuda\")\n",
    "    \n",
    "    instruction = dataset[i][\"instruction\"]\n",
    "    text = dataset[i].get(\"text\")\n",
    "    instruction += f\"\\n\\n### Text: {text}\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs['Audio'] = [audio, 1]\n",
    "\n",
    "    prompt = llama.format_prompt(instruction)\n",
    "    results = model.generate(\n",
    "        inputs,\n",
    "        [prompt],\n",
    "        max_gen_len=64,\n",
    "        temperature=0,\n",
    "        top_p=0\n",
    "    )\n",
    "    \n",
    "    print(prompt)\n",
    "    print(\"=\"*100)\n",
    "    result = results[0].strip()\n",
    "    print(result)\n",
    "    print(\"=\"*100)\n",
    "    label = dataset[i][\"label\"]\n",
    "    text = dataset[i].get(\"text\")\n",
    "    transcription = dataset[i].get(\"transcription\")\n",
    "    print(label)\n",
    "    print(\"#\"*100)\n",
    "    \n",
    "    predictions.append({\n",
    "        \"llm_prompt\": prompt,\n",
    "        \"llm_response\": result,\n",
    "        \"instruction\": dataset[i].get(\"instruction\"),\n",
    "        \"label\": label,\n",
    "        \"text\": text,\n",
    "        \"transcription\": transcription\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58533f7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Instrcution:\t Determine if the speech and text are matched. The answer could be yes or no.\n",
      "=> Text:\t\t This was presently superseded by a fresh catch \n",
      "=> (Speech):\t and such projects as they approve will be next submitted to the President who under the Act is required to make final allocations\n",
      "==> Response:\t the text and speech are not matched. the text says \"this was presently superseded by a fresh catch,\" while the speech says \"this is a picture of a pizza.\"\n",
      "====================================================================================================\n",
      "=> Instrcution:\t Decide if the speech maintains the organization of the written content. The answer could be yes or no.\n",
      "=> Text:\t\t was shipped from Los Angeles on March twenty nineteen sixtythree and he left for New Orleans on April twentyfour nineteen sixtythree\n",
      "=> (Speech):\t was shipped from Los Angeles on March twenty nineteen sixtythree and he left for New Orleans on April twentyfour nineteen sixtythree\n",
      "==> Response:\t the speech does not maintain the organization of the written content. the text is presented in a disorganized manner, with the dates of departure and arrival not following the same format.\n",
      "====================================================================================================\n",
      "=> Instrcution:\t Does the speech align with the intended message of the text? The answer could be yes or no.\n",
      "=> Text:\t\t there have been attempts on the lives of one out of every three\n",
      "=> (Speech):\t with those who made the selection of the first inspectors and the two gentlemen appointed were probably the most fitted in England to be so employed\n",
      "==> Response:\t the speech seems to align with the intended message of the text, which is to inform the audience about the prevalence of attempted murders in the united states.\n",
      "====================================================================================================\n",
      "=> Instrcution:\t Assess if the speech and text correspond to each other. The answer could be yes or no.\n",
      "=> Text:\t\t Catherine Wilson the poisoner was reserved and reticent to the last expressing no contrition but also no fear \n",
      "=> (Speech):\t Catherine Wilson the poisoner was reserved and reticent to the last expressing no contrition but also no fear \n",
      "==> Response:\t yes, the text and speech correspond to each other. the text describes catherine wilson as reserved and reticent, while the speech shows that she expresses no contrition and no fear.\n",
      "====================================================================================================\n",
      "=> Instrcution:\t Judge if the speech and the text express the same ideas. The answer could be yes or no.\n",
      "=> Text:\t\t They had had serious work to get at the diamonds It was necessary to force one heavy door from its hinges and to cut through the thick panels of another\n",
      "=> (Speech):\t They had had serious work to get at the diamonds It was necessary to force one heavy door from its hinges and to cut through the thick panels of another\n",
      "==> Response:\t the text and the speech express the same ideas.\n",
      "====================================================================================================\n",
      "=> Instrcution:\t Determine whether the given speech and text pair are a match or not. The answer could be yes or no.\n",
      "=> Text:\t\t At times the numbers congregated together were very great as many as fifty and sixty even more were crowded indiscriminately into the pressyard\n",
      "=> (Speech):\t At times the numbers congregated together were very great as many as fifty and sixty even more were crowded indiscriminately into the pressyard\n",
      "==> Response:\t no, the given speech and text pair are not a match. the text is about the numbers of animals in a pressyard, while the speech is about the numbers of people in a crowd.\n",
      "====================================================================================================\n",
      "=> Instrcution:\t Verify if the speech and text are congruent. The answer could be yes or no.\n",
      "=> Text:\t\t who understand those modern conditions\n",
      "=> (Speech):\t who understand those modern conditions\n",
      "==> Response:\t no, the text and speech do not match. the text says \"who understand those modern conditions,\" while the speech says \"who understands the modern conditions?\"\n",
      "====================================================================================================\n",
      "=> Instrcution:\t Determine if the speech matches the text or not. The answer could be yes or no.\n",
      "=> Text:\t\t he with unusual quickness of apprehension discovered and promptly turned to account an inexcusably lax system of management\n",
      "=> (Speech):\t he with unusual quickness of apprehension discovered and promptly turned to account an inexcusably lax system of management\n",
      "==> Response:\t no, the speech does not match the text. the text is a description of a children's book, while the speech is a description of a person's reaction to the book.\n",
      "====================================================================================================\n",
      "=> Instrcution:\t Decide if the speech and text match each other precisely. The answer could be yes or no.\n",
      "=> Text:\t\t Forrester the detective who had pursued and captured Burgess at Boston tracked Ker to France and following him there eventually captured him\n",
      "=> (Speech):\t Forrester the detective who had pursued and captured Burgess at Boston tracked Ker to France and following him there eventually captured him\n",
      "==> Response:\t no, the text and speech do not match each other precisely. the text states that forrester the detective pursued and captured burgess at the boston track, while the speech reads \"forrester the detective who had pursued and captured burgess at boston tracked ker to france and following him there\n",
      "====================================================================================================\n",
      "=> Instrcution:\t Determine if the speech and the  convey the same meaning or not. The answer could be yes or no.\n",
      "=> Text:\t\t In the crowd all of them persons who had no other avocation or mode of livelihood but thieving Mr Buxton counted eleven children\n",
      "=> (Speech):\t Observing the serious problems presented by the head wound and inadequate respiration Dr Carrico directed his attention to improving the Presidents breathing\n",
      "==> Response:\t the text and the speech convey the same meaning. the text describes the scene, while the speech provides a description of the scene. both convey the same information about the people in the crowd and the children.\n",
      "====================================================================================================\n",
      "2\n",
      "7\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "count_no = 0\n",
    "count_yes = 0\n",
    "for i, pred in enumerate(predictions):\n",
    "    resp = pred[\"llm_response\"].lower()\n",
    "    label = pred[\"label\"].lower()\n",
    "    print(f\"=> Instrcution:\\t\",pred[\"instruction\"])\n",
    "    print(\"=> Text:\\t\\t\", pred['text'])\n",
    "    print(\"=> (Speech):\\t\", pred['transcription'])\n",
    "    print(\"==> Response:\\t\", resp)\n",
    "#     print(\"Label:\\t\\t\", f\"{label}\")\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    if label in resp:\n",
    "        count +=1\n",
    "    \n",
    "    if label == \"yes\":\n",
    "        count_yes += 1\n",
    "    if label == \"no\":\n",
    "        count_no += 1\n",
    "print(count)\n",
    "print(count_yes)\n",
    "print(count_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50fa53d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7191f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a79a88b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "57\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "count_no = 0\n",
    "count_yes = 0\n",
    "for i, pred in enumerate(predictions):\n",
    "    resp = \"yes\" if random.random() > 0.5 else \"no\"\n",
    "    label = pred[\"label\"]\n",
    "    \n",
    "#     if label in resp:\n",
    "#         count +=1\n",
    "    \n",
    "#     if label == \"yes\":\n",
    "#         count_yes += 1\n",
    "#     if label == \"no\":\n",
    "#         count_no += 1\n",
    "print(count)\n",
    "print(count_yes)\n",
    "print(count_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea6c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2442664",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b126f6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'audio', 'instruction', 'label'],\n",
       "    num_rows: 3014\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.load_from_disk(\"/work/u8915687/big-superb/BigSuperbPrivate/SpoofDetection_Asvspoof2017_train5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae404b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagebind_LLM",
   "language": "python",
   "name": "imagebind_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
