{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1286f5",
   "metadata": {},
   "source": [
    "處理健祐哥切好的 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4a65fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u2619111/.local/lib/python3.9/site-packages/whisper/timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "import llama\n",
    "from llama import Tokenizer\n",
    "import torchaudio\n",
    "from torchvision import transforms\n",
    "from pytorchvideo.data.clip_sampling import ConstantClipsPerVideoSampler\n",
    "import copy\n",
    "import logging\n",
    "import whisper\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "tokenizer = Tokenizer(model_path=\"/home/u2619111/hank/lab/big-superb/LLaMA-Adapter/imagebind_LLM/ckpts/llama/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8e0a763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hank\n",
    "import torch\n",
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import llama.utils\n",
    "from llama import Tokenizer\n",
    "import copy\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import random\n",
    "from pytorchvideo.data.clip_sampling import ConstantClipsPerVideoSampler\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "class BigSuperbDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, data_path2=None, max_length=128, used_data_split=\"train\", audio_input_type=\"imagebind\", allowed_datasets=[]):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.datas = []\n",
    "        self.used_datasets = []\n",
    "        self.used_data_split = used_data_split\n",
    "        self.audio_input_type = audio_input_type\n",
    "        self.allowed_datasets = allowed_datasets\n",
    "\n",
    "        for task_path in data_path.iterdir():\n",
    "            if not self.check_allowed_datasets(task_path):\n",
    "                continue\n",
    "            \n",
    "            self.used_datasets.append(task_path.stem)\n",
    "            \n",
    "            for data_split in task_path.iterdir():\n",
    "                if data_split.stem != self.used_data_split:\n",
    "                    continue\n",
    "                \n",
    "                json_data = json.load((data_split/\"metadata.json\").open())\n",
    "                for filename, d in json_data.items():\n",
    "                    \n",
    "                    d[\"file\"] = f\"{data_split}/{filename}\"\n",
    "\n",
    "                    if d.get(\"file2\"):\n",
    "                        if \".\" not in d[\"file2\"]:\n",
    "                            d[\"file2\"] = d[\"file2\"] + \".wav\"\n",
    "                        \n",
    "                        if (data_split/f\"{task_path.stem}_{self.used_data_split}_{d['file2']}\").exists():\n",
    "                            d[\"file2\"] = f\"{data_split}/{task_path.stem}_{self.used_data_split}_{d['file2']}\"\n",
    "                        elif (task_path/\"missing_files\"/d[\"file2\"]).exists():\n",
    "                            d[\"file2\"] = str(task_path/\"missing_files\"/d[\"file2\"])\n",
    "                        else:\n",
    "                            assert False, f\"{task_path}\"+ d[\"file2\"]\n",
    "                            \n",
    "                    self.datas.append(d)\n",
    "        # exclude\n",
    "        if data_path2 is not None:\n",
    "            for task_path in data_path2.iterdir():\n",
    "                if not self.check_allowed_datasets(task_path):\n",
    "                    continue\n",
    "                \n",
    "                self.used_datasets.append(task_path.stem)\n",
    "                for data_split in task_path.iterdir():\n",
    "                    if data_split.stem != self.used_data_split:\n",
    "                        continue\n",
    "                    \n",
    "                    json_data = json.load((data_split/\"metadata.json\").open())\n",
    "                    for file_name, d in json_data.items():\n",
    "                        d[\"file\"] = str(data_split/file_name)\n",
    "                                \n",
    "                        self.datas.append(d)\n",
    "\n",
    "\n",
    "        # Audio loader\n",
    "        self.clip_sampler = ConstantClipsPerVideoSampler(\n",
    "            clip_duration=2, clips_per_video=3\n",
    "        )\n",
    "        print(\"Used datasets\", len(self.used_datasets), self.used_datasets)\n",
    "        print(len(self.datas))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.datas[idx]\n",
    "        new_data = {}\n",
    "        \n",
    "        # Text \n",
    "        if self.used_data_split == \"train\":\n",
    "            instruction = data[\"instruction\"].lower()\n",
    "            text = data[\"text\"].lower() if data.get(\"text\") else None\n",
    "            input1 = llama.format_prompt(instruction, text)\n",
    "            input2 = input1 + data[\"label\"]\n",
    "\n",
    "            input1 = torch.tensor(\n",
    "                self.tokenizer.encode(input1, bos=True, eos=False), dtype=torch.long\n",
    "            )\n",
    "            input2 = torch.tensor(self.tokenizer.encode(input2, bos=True, eos=True), dtype=torch.long)\n",
    "            padding = self.max_length - input2.size(0)\n",
    "            if padding > 0:\n",
    "                input2 = torch.cat((input2, torch.zeros(padding, dtype=torch.long) - 1))\n",
    "            else:\n",
    "                input2 = input2[:self.max_length]\n",
    "\n",
    "            labels = copy.deepcopy(input2)\n",
    "            labels[:input1.size(0)] = -1\n",
    "\n",
    "            input2_mask = input2.ge(0)\n",
    "            label_mask = labels.ge(0)\n",
    "            input2[~input2_mask] = 0\n",
    "            labels[~label_mask] = 0\n",
    "\n",
    "            input2_mask = input2_mask.float()\n",
    "            label_mask = label_mask.float()\n",
    "\n",
    "            new_data[\"instruction\"] = data[\"instruction\"]\n",
    "            new_data[\"input_ids\"] = input2\n",
    "            new_data[\"labels\"] = labels\n",
    "            new_data[\"input_mask\"] = input2_mask\n",
    "        elif self.used_data_split == \"test\":\n",
    "            instruction = data[\"instruction\"].lower()\n",
    "            text = data[\"text\"].lower() if data.get(\"text\") else None\n",
    "            input1 = llama.format_prompt(instruction, text)\n",
    "            \n",
    "            new_data[\"prompt\"] = input1\n",
    "            new_data[\"instruction\"] = data[\"instruction\"]\n",
    "            new_data[\"label\"] = data[\"label\"]\n",
    "\n",
    "        if self.audio_input_type == \"imagebind\":\n",
    "            if data.get(\"file2\"):\n",
    "                audio = self._load_and_transform_audio([data[\"file\"], data[\"file2\"]])\n",
    "            else:\n",
    "                audio = self._load_and_transform_audio([data[\"file\"]])\n",
    "                \n",
    "            \n",
    "        elif self.audio_input_type == \"whisper\":\n",
    "            if data.get(\"file2\"):\n",
    "                audio = self._load_whisper_audio([data[\"file\"], data[\"file2\"]])\n",
    "            else:\n",
    "                audio = self._load_whisper_audio([data[\"file\"]])\n",
    "        new_data[\"audio\"] = audio\n",
    "        return new_data\n",
    "    \n",
    "    def _load_whisper_audio(self, audio_paths):\n",
    "        wavforms = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform = torch.tensor(whisper.load_audio(audio_path))\n",
    "            if waveform.size(0) == 0:\n",
    "                waveform = torch.zeros([16000*3])\n",
    "                print(audio_path)\n",
    "            \n",
    "            wavforms.append(\n",
    "                waveform\n",
    "            )\n",
    "        audio = torch.cat(wavforms, dim=0)\n",
    "        audio = whisper.pad_or_trim(audio)\n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "        \n",
    "        return mel\n",
    "\n",
    "    \n",
    "    def _load_and_transform_audio(self, \n",
    "            audio_paths,\n",
    "            num_mel_bins=128,\n",
    "            target_length=204,\n",
    "            sample_rate=16000,\n",
    "            clip_duration=2,\n",
    "            clips_per_video=3,\n",
    "            mean=-4.268,\n",
    "            std=9.138\n",
    "        ):\n",
    "\n",
    "        waveforms = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "            if waveform.size(1) == 0:\n",
    "                waveform = torch.zeros([1, 16000*3])\n",
    "                sr = 16000\n",
    "                # logging.warning(f\"An audio is set to zero, {audio_path}\")\n",
    "                print(audio_path)\n",
    "                \n",
    "            if sample_rate != sr:\n",
    "                waveform = torchaudio.functional.resample(\n",
    "                    waveform, orig_freq=sr, new_freq=sample_rate\n",
    "                )\n",
    "\n",
    "            waveforms.append(waveform)\n",
    "        waveform = torch.cat(waveforms, dim=1)\n",
    "            \n",
    "        all_clips_timepoints = self._get_clip_timepoints(\n",
    "            self.clip_sampler, waveform.size(1) / sample_rate\n",
    "        )\n",
    "        all_clips = []\n",
    "        for clip_timepoints in all_clips_timepoints:\n",
    "            waveform_clip = waveform[\n",
    "                :,\n",
    "                int(clip_timepoints[0] * sample_rate) : int(\n",
    "                    clip_timepoints[1] * sample_rate\n",
    "                ),\n",
    "            ]\n",
    "            waveform_melspec = self._def_waveform2melspec(\n",
    "                waveform_clip, sample_rate, num_mel_bins, target_length\n",
    "            )\n",
    "            all_clips.append(waveform_melspec)\n",
    "\n",
    "        normalize = transforms.Normalize(mean=mean, std=std)\n",
    "        all_clips = [normalize(ac) for ac in all_clips]\n",
    "\n",
    "        all_clips = torch.stack(all_clips, dim=0)\n",
    "\n",
    "        return all_clips\n",
    "        \n",
    "    def _get_clip_timepoints(self, clip_sampler, duration):\n",
    "        # Read out all clips in this video\n",
    "        all_clips_timepoints = []\n",
    "        is_last_clip = False\n",
    "        end = 0.0\n",
    "        while not is_last_clip:\n",
    "            start, end, _, _, is_last_clip = clip_sampler(end, duration, annotation=None)\n",
    "            all_clips_timepoints.append((start, end))\n",
    "        return all_clips_timepoints\n",
    "\n",
    "    def _def_waveform2melspec(self, waveform, sample_rate, num_mel_bins, target_length):\n",
    "        # Based on https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/dataloader.py#L102\n",
    "        waveform -= waveform.mean()\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(\n",
    "            waveform,\n",
    "            htk_compat=True,\n",
    "            sample_frequency=sample_rate,\n",
    "            use_energy=False,\n",
    "            window_type=\"hanning\",\n",
    "            num_mel_bins=num_mel_bins,\n",
    "            dither=0.0,\n",
    "            frame_length=25,\n",
    "            frame_shift=10,\n",
    "        )\n",
    "        # Convert to [mel_bins, num_frames] shape\n",
    "        fbank = fbank.transpose(0, 1)\n",
    "        # Pad to target_length\n",
    "        n_frames = fbank.size(1)\n",
    "        p = target_length - n_frames\n",
    "        \n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            fbank = torch.nn.functional.pad(fbank, (0, p), mode=\"constant\", value=0)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[:, 0:target_length]\n",
    "        # Convert to [1, mel_bins, num_frames] shape, essentially like a 1\n",
    "        # channel image\n",
    "        fbank = fbank.unsqueeze(0)\n",
    "        return fbank\n",
    "\n",
    "    def check_allowed_datasets(self, task_path):\n",
    "        for d in self.allowed_datasets:\n",
    "            if task_path.stem.lower() in d.lower():\n",
    "                return True\n",
    "        # print(f\"find {task_path.stem} not in allowed list.\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56bb44a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used datasets 49 ['NoiseSNRLevelPredictiongaussian_VCTKMusan', 'SpoofDetection_ASVspoof2017', 'SpokenTermDetection_LibriSpeechTestOther', 'EnvironmentalSoundClassification_NaturalSoundscapesAndWaterSoundsESC50', 'Intent_Classification_FluentSpeechCommands_Object', 'Intent_Classification_FluentSpeechCommands_Action', 'DialogueActClassification_DailyTalk', 'DialogueEmotionClassification_DailyTalk', 'ReverberationDetectionmediumroom_VCTKRirsNoises', 'NoiseSNRLevelPredictionspeech_VCTKMusan', 'EnvironmentalSoundClassification_AnimalsESC50', 'EnvironmentalSoundClassification_ExteriorAndUrbanNoisesESC50', 'EmotionRecognition_MultimodalEmotionlinesDataset', 'ReverberationDetectionlargeroom_LJSpeechRirsNoises', 'SpeechDetection_LJSpeech', 'BirdSoundDetection_Warblrb10k', 'Intent_Classification_FluentSpeechCommands_Location', 'SpoofDetection_ASVspoof2015', 'SpokenTermDetection_LJSpeech', 'ReverberationDetectionmediumroom_LJSpeechRirsNoises', 'AccentClassification_AccentdbExtended', 'SpeechDetection_LibriSpeechTestOther', 'HowFarAreYou_3DSpeaker', 'ChordClassification_AcousticGuitarAndPiano', 'SarcasmDetection_Mustard', 'SpeechTextMatching_LibriSpeechTestOther', 'NoiseDetectiongaussian_VCTKMusan', 'ReverberationDetectionlargeroom_VCTKRirsNoises', 'EnhancementDetection_LibrittsTestCleanWham', 'ReverberationDetectionsmallroom_LJSpeechRirsNoises', 'SpeakerCounting_LibriTTSTestClean', 'EnvironmentalSoundClassification_InteriorAndDomesticSoundsESC50', 'NoiseDetectionmusic_VCTKMusan', 'NoiseDetectiongaussian_LJSpeechMusan', 'StressDetection_MIRSD', 'SpeechTextMatching_LibriSpeechTestClean', 'SpeechDetection_LibriSpeechTestClean', 'ReverberationDetectionsmallroom_VCTKRirsNoises', 'NoiseSNRLevelPredictionnoise_VCTKMusan', 'EnvironmentalSoundClassification_HumanAndNonSpeechSoundsESC50', 'NoiseDetectionnoise_LJSpeechMusan', 'NoiseDetectionmusic_LJSpeechMusan', 'SpokenTermDetection_LibriSpeechTestClean', 'SpeechTextMatching_LJSpeech', 'NoiseSNRLevelPredictionmusic_VCTKMusan', 'NoiseDetectionspeech_LJSpeechMusan', 'SpeechCommandRecognition_GoogleSpeechCommandsV1', 'NoiseDetectionnoise_VCTKMusan', 'NoiseDetectionspeech_VCTKMusan']\n",
      "724515\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(\"/home/u2619111/hank/Dataset/big-superb-test-data-renamed\")\n",
    "allowed_datasets = open(\"data/test_dataset.txt\").read().split(\"\\n\")\n",
    "train_dataset = BigSuperbDataset(data_path, tokenizer, audio_input_type=\"whisper\", used_data_split=\"test\", allowed_datasets=allowed_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d154c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 65/484 [00:09<00:59,  7.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(train_dataset), \u001b[39m1500\u001b[39m)):\n\u001b[0;32m----> 2\u001b[0m     train_dataset[i]\n",
      "Cell \u001b[0;32mIn[38], line 144\u001b[0m, in \u001b[0;36mBigSuperbDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    142\u001b[0m         audio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_whisper_audio([data[\u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m], data[\u001b[39m\"\u001b[39m\u001b[39mfile2\u001b[39m\u001b[39m\"\u001b[39m]])\n\u001b[1;32m    143\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         audio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_whisper_audio([data[\u001b[39m\"\u001b[39;49m\u001b[39mfile\u001b[39;49m\u001b[39m\"\u001b[39;49m]])\n\u001b[1;32m    145\u001b[0m new_data[\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m audio\n\u001b[1;32m    146\u001b[0m \u001b[39mreturn\u001b[39;00m new_data\n",
      "Cell \u001b[0;32mIn[38], line 151\u001b[0m, in \u001b[0;36mBigSuperbDataset._load_whisper_audio\u001b[0;34m(self, audio_paths)\u001b[0m\n\u001b[1;32m    149\u001b[0m wavforms \u001b[39m=\u001b[39m []\n\u001b[1;32m    150\u001b[0m \u001b[39mfor\u001b[39;00m audio_path \u001b[39min\u001b[39;00m audio_paths:\n\u001b[0;32m--> 151\u001b[0m     waveform \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(whisper\u001b[39m.\u001b[39;49mload_audio(audio_path))\n\u001b[1;32m    152\u001b[0m     \u001b[39mif\u001b[39;00m waveform\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    153\u001b[0m         waveform \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros([\u001b[39m16000\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/whisper/audio.py:46\u001b[0m, in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mOpen an audio file and read as mono waveform, resampling as necessary\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mA NumPy array containing the audio waveform, in float32 dtype.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[39m# This launches a subprocess to decode audio while down-mixing and resampling as necessary.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[39m# Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     out, _ \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 46\u001b[0m         ffmpeg\u001b[39m.\u001b[39;49minput(file, threads\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     47\u001b[0m         \u001b[39m.\u001b[39;49moutput(\u001b[39m\"\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39ms16le\u001b[39;49m\u001b[39m\"\u001b[39;49m, acodec\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpcm_s16le\u001b[39;49m\u001b[39m\"\u001b[39;49m, ac\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, ar\u001b[39m=\u001b[39;49msr)\n\u001b[1;32m     48\u001b[0m         \u001b[39m.\u001b[39;49mrun(cmd\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mffmpeg\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m-nostdin\u001b[39;49m\u001b[39m\"\u001b[39;49m], capture_stdout\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, capture_stderr\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m ffmpeg\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     51\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to load audio: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mdecode()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ffmpeg/_run.py:313\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(stream_spec, cmd, capture_stdout, capture_stderr, input, quiet, overwrite_output)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39m@output_operator\u001b[39m()\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\n\u001b[1;32m    291\u001b[0m     stream_spec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     overwrite_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    298\u001b[0m ):\n\u001b[1;32m    299\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Invoke ffmpeg for the supplied node graph.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m    Returns: (out, err) tuple containing captured stdout and stderr data.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m     process \u001b[39m=\u001b[39m run_async(\n\u001b[1;32m    314\u001b[0m         stream_spec,\n\u001b[1;32m    315\u001b[0m         cmd,\n\u001b[1;32m    316\u001b[0m         pipe_stdin\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    317\u001b[0m         pipe_stdout\u001b[39m=\u001b[39;49mcapture_stdout,\n\u001b[1;32m    318\u001b[0m         pipe_stderr\u001b[39m=\u001b[39;49mcapture_stderr,\n\u001b[1;32m    319\u001b[0m         quiet\u001b[39m=\u001b[39;49mquiet,\n\u001b[1;32m    320\u001b[0m         overwrite_output\u001b[39m=\u001b[39;49moverwrite_output,\n\u001b[1;32m    321\u001b[0m     )\n\u001b[1;32m    322\u001b[0m     out, err \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mcommunicate(\u001b[39minput\u001b[39m)\n\u001b[1;32m    323\u001b[0m     retcode \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mpoll()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ffmpeg/_run.py:284\u001b[0m, in \u001b[0;36mrun_async\u001b[0;34m(stream_spec, cmd, pipe_stdin, pipe_stdout, pipe_stderr, quiet, overwrite_output)\u001b[0m\n\u001b[1;32m    282\u001b[0m stdout_stream \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPIPE \u001b[39mif\u001b[39;00m pipe_stdout \u001b[39mor\u001b[39;00m quiet \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    283\u001b[0m stderr_stream \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPIPE \u001b[39mif\u001b[39;00m pipe_stderr \u001b[39mor\u001b[39;00m quiet \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m subprocess\u001b[39m.\u001b[39;49mPopen(\n\u001b[1;32m    285\u001b[0m     args, stdin\u001b[39m=\u001b[39;49mstdin_stream, stdout\u001b[39m=\u001b[39;49mstdout_stream, stderr\u001b[39m=\u001b[39;49mstderr_stream\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/imagebind_LLM/lib/python3.9/subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m    948\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    949\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 951\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    952\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    953\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    954\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    955\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    956\u001b[0m                         errread, errwrite,\n\u001b[1;32m    957\u001b[0m                         restore_signals,\n\u001b[1;32m    958\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    959\u001b[0m                         start_new_session)\n\u001b[1;32m    960\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m~/anaconda3/envs/imagebind_LLM/lib/python3.9/subprocess.py:1770\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1768\u001b[0m     fds_to_keep \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(pass_fds)\n\u001b[1;32m   1769\u001b[0m     fds_to_keep\u001b[39m.\u001b[39madd(errpipe_write)\n\u001b[0;32m-> 1770\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid \u001b[39m=\u001b[39m _posixsubprocess\u001b[39m.\u001b[39;49mfork_exec(\n\u001b[1;32m   1771\u001b[0m             args, executable_list,\n\u001b[1;32m   1772\u001b[0m             close_fds, \u001b[39mtuple\u001b[39;49m(\u001b[39msorted\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39mint\u001b[39;49m, fds_to_keep))),\n\u001b[1;32m   1773\u001b[0m             cwd, env_list,\n\u001b[1;32m   1774\u001b[0m             p2cread, p2cwrite, c2pread, c2pwrite,\n\u001b[1;32m   1775\u001b[0m             errread, errwrite,\n\u001b[1;32m   1776\u001b[0m             errpipe_read, errpipe_write,\n\u001b[1;32m   1777\u001b[0m             restore_signals, start_new_session,\n\u001b[1;32m   1778\u001b[0m             gid, gids, uid, umask,\n\u001b[1;32m   1779\u001b[0m             preexec_fn)\n\u001b[1;32m   1780\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_child_created \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39m# be sure the FD is closed no matter what\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, len(train_dataset), 1500)):\n",
    "    train_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd885240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\ndetermine the noise-to-signal ratio for the utterance contaminated with gaussian noise. the answer could be zero, five, ten, fifteen, or clean.\\n\\n### Response:', 'instruction': 'Determine the noise-to-signal ratio for the utterance contaminated with Gaussian noise. The answer could be zero, five, ten, fifteen, or clean.', 'label': 'clean', 'audio': tensor([[ 0.3177,  0.3964,  0.4824,  ..., -0.9105, -0.9105, -0.9105],\n",
      "        [ 0.0837,  0.2006,  0.2708,  ..., -0.9105, -0.9105, -0.9105],\n",
      "        [-0.1182, -0.0250,  0.0349,  ..., -0.9105, -0.9105, -0.9105],\n",
      "        ...,\n",
      "        [-0.8325, -0.9105, -0.9105,  ..., -0.9105, -0.9105, -0.9105],\n",
      "        [-0.8958, -0.9105, -0.9105,  ..., -0.9105, -0.9105, -0.9105],\n",
      "        [-0.8445, -0.9105, -0.9105,  ..., -0.9105, -0.9105, -0.9105]])}, {'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nevaluate the snr of the utterance with gaussian noise. the answer could be zero, five, ten, fifteen, or clean.\\n\\n### Response:', 'instruction': 'Evaluate the SNR of the utterance with Gaussian noise. The answer could be zero, five, ten, fifteen, or clean.', 'label': 'ten', 'audio': tensor([[ 0.4257,  0.3220, -0.1822,  ..., -0.8105, -0.8105, -0.8105],\n",
      "        [ 0.4192,  0.3633, -0.0908,  ..., -0.8105, -0.8105, -0.8105],\n",
      "        [ 0.2283,  0.2742,  0.2252,  ..., -0.8105, -0.8105, -0.8105],\n",
      "        ...,\n",
      "        [ 0.1568,  0.2605,  0.2821,  ..., -0.8105, -0.8105, -0.8105],\n",
      "        [ 0.3429,  0.2807,  0.2484,  ..., -0.8105, -0.8105, -0.8105],\n",
      "        [ 0.2922,  0.2473,  0.1834,  ..., -0.8105, -0.8105, -0.8105]])}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'audio': tensor([[[ 0.3177,  0.3964,  0.4824,  ..., -0.9105, -0.9105, -0.9105],\n",
       "          [ 0.0837,  0.2006,  0.2708,  ..., -0.9105, -0.9105, -0.9105],\n",
       "          [-0.1182, -0.0250,  0.0349,  ..., -0.9105, -0.9105, -0.9105],\n",
       "          ...,\n",
       "          [-0.8325, -0.9105, -0.9105,  ..., -0.9105, -0.9105, -0.9105],\n",
       "          [-0.8958, -0.9105, -0.9105,  ..., -0.9105, -0.9105, -0.9105],\n",
       "          [-0.8445, -0.9105, -0.9105,  ..., -0.9105, -0.9105, -0.9105]],\n",
       " \n",
       "         [[ 0.4257,  0.3220, -0.1822,  ..., -0.8105, -0.8105, -0.8105],\n",
       "          [ 0.4192,  0.3633, -0.0908,  ..., -0.8105, -0.8105, -0.8105],\n",
       "          [ 0.2283,  0.2742,  0.2252,  ..., -0.8105, -0.8105, -0.8105],\n",
       "          ...,\n",
       "          [ 0.1568,  0.2605,  0.2821,  ..., -0.8105, -0.8105, -0.8105],\n",
       "          [ 0.3429,  0.2807,  0.2484,  ..., -0.8105, -0.8105, -0.8105],\n",
       "          [ 0.2922,  0.2473,  0.1834,  ..., -0.8105, -0.8105, -0.8105]]]),\n",
       " 'prompts': ['Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\ndetermine the noise-to-signal ratio for the utterance contaminated with gaussian noise. the answer could be zero, five, ten, fifteen, or clean.\\n\\n### Response:',\n",
       "  'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nevaluate the snr of the utterance with gaussian noise. the answer could be zero, five, ten, fifteen, or clean.\\n\\n### Response:'],\n",
       " 'instructions': ['Determine the noise-to-signal ratio for the utterance contaminated with Gaussian noise. The answer could be zero, five, ten, fifteen, or clean.',\n",
       "  'Evaluate the SNR of the utterance with Gaussian noise. The answer could be zero, five, ten, fifteen, or clean.'],\n",
       " 'labels': ['clean', 'ten']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(b):\n",
    "    print(b)\n",
    "    batch = {}\n",
    "\n",
    "    batch[\"audio\"] = torch.stack([d[\"audio\"] for d in b])\n",
    "    batch[\"prompts\"] = [d[\"prompt\"] for d in b]\n",
    "    batch[\"instructions\"] = [d[\"instruction\"] for d in b]\n",
    "    batch[\"labels\"] = [d[\"label\"] for d in b]\n",
    "    return batch\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40a8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e9ca36",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29fc0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "from ImageBind.data import my_load_and_transform_audio_data\n",
    "\n",
    "org_data_path = Path(\"/work/u8915687/big-superb/train_datasets\")\n",
    "all_datasets = ['BigSuperbPrivate/SpeechDetection_LibrispeechTrainClean100']\n",
    "\n",
    "tokenizer = Tokenizer(model_path=\"/home/u8915687/lab/big-superb/Macaw-LLM2/weights/llama_7B/tokenizer.model\")\n",
    "def prepare_dataset(b):\n",
    "    max_length = 128\n",
    "    \n",
    "    batch = {}\n",
    "    instruction = b[\"instruction\"].lower()\n",
    "    text = b[\"text\"].lower() if b.get(\"text\") else None\n",
    "    input1 = llama.format_prompt(instruction, text)\n",
    "    input2 = input1 + b[\"label\"]\n",
    "    \n",
    "    input1 = torch.tensor(\n",
    "        tokenizer.encode(input1, bos=True, eos=False), dtype=torch.long\n",
    "    )\n",
    "    input2 = torch.tensor(tokenizer.encode(input2, bos=True, eos=True), dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    padding = max_length - input2.size(0)\n",
    "    if padding > 0:\n",
    "        input2 = torch.cat((input2, torch.zeros(padding, dtype=torch.long) - 1))\n",
    "    else:\n",
    "        input2 = input2[:max_length]\n",
    "    \n",
    "    labels = copy.deepcopy(input2)\n",
    "    labels[:input1.size(0)] = -1\n",
    "    \n",
    "    input2_mask = input2.ge(0)\n",
    "    label_mask = labels.ge(0)\n",
    "    input2[~input2_mask] = 0\n",
    "    labels[~label_mask] = 0\n",
    "    \n",
    "    input2_mask = input2_mask.float()\n",
    "    label_mask = label_mask.float()\n",
    "    \n",
    "    batch[\"instruction\"] = b[\"instruction\"]\n",
    "    batch[\"input_ids\"] = input2\n",
    "    batch[\"labels\"] = labels\n",
    "    batch[\"input_mask\"] = input2_mask\n",
    "    batch[\"audio\"] = my_load_and_transform_audio_data(\n",
    "        torch.tensor(b[\"audio\"][\"array\"]).unsqueeze(0)\n",
    "    )[0]\n",
    "    return batch\n",
    "\n",
    "for dataset_name in all_datasets:\n",
    "    ori_dataset =  load_from_disk(org_data_path/dataset_name)    \n",
    "\n",
    "    td = ori_dataset[\"train\"].shuffle(42)[:50]\n",
    "    td = Dataset.from_dict(td)\n",
    "\n",
    "    # td = td.map(prepare_dataset)\n",
    "    # td = td.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec32a088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_name': 'SpeechDetection_LibrispeechTrainClean100_0',\n",
       " 'name': '289-121652-0038.flac',\n",
       " 'instruction': 'Does the recording contain speech from human? The answer could be yes or no.',\n",
       " 'input_ids': tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 13221,\n",
       "           278, 16867,  1712, 12032,   515,  5199, 29973,   278,  1234,  1033,\n",
       "           367,  4874,   470,   694, 29889,    13,    13,  2277, 29937, 13291,\n",
       "         29901,  3582,     2,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'labels': tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0, 3582,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'input_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]),\n",
       " 'audio': tensor([[[[-1.0245, -0.8402, -0.7290,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-1.2776, -1.2310, -1.1678,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.9941, -0.8542, -0.7909,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           ...,\n",
       "           [-0.3938, -0.4086, -0.5283,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.4391, -0.4342, -0.5335,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.4806, -0.5034, -0.5158,  ...,  0.4671,  0.4671,  0.4671]]],\n",
       " \n",
       " \n",
       "         [[[-0.7067, -0.6565, -0.6842,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-1.1143, -1.1518, -0.8906,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.7375, -0.7750, -0.5138,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           ...,\n",
       "           [-0.3882, -0.3532, -0.3948,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.3247, -0.3022, -0.3114,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.2978, -0.2267, -0.2737,  ...,  0.4671,  0.4671,  0.4671]]],\n",
       " \n",
       " \n",
       "         [[[-0.2674, -0.2020, -0.1675,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.7535, -0.5812, -0.6304,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.3766, -0.2044, -0.2536,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           ...,\n",
       "           [-0.3294, -0.3546, -0.3744,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.2043, -0.3943, -0.3750,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.3051, -0.3877, -0.4372,  ...,  0.4671,  0.4671,  0.4671]]]])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[65000-1986]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba3c209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0e92c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(b):\n",
    "    batch = {}\n",
    "    \n",
    "    audios = []\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    for data in b:\n",
    "        audio = my_load_and_transform_audio_data(\n",
    "                        torch.tensor(data[\"audio\"][\"array\"], dtype=torch.float32\n",
    "                    ).unsqueeze(0))[0]\n",
    "        audios.append(audio)\n",
    "        text = data.get(\"text\").lower() if data.get(\"text\") else None\n",
    "        instruction = data[\"instruction\"].lower()\n",
    "        \n",
    "        prompts.append(llama.format_prompt(instruction, text))\n",
    "        labels.append(data[\"label\"])\n",
    "    \n",
    "    batch[\"audio\"] = torch.stack(audios)\n",
    "    batch[\"prompts\"] = prompts\n",
    "    batch[\"instructions\"] = [d[\"instruction\"] for d in b]\n",
    "    batch[\"labels\"] = [d[\"label\"] for d in b]\n",
    "    return batch\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        td,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3cebdc87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc356e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ad13f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagebind_LLM",
   "language": "python",
   "name": "imagebind_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
