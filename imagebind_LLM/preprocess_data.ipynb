{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6023a2-de2e-4ea9-bc7d-9d8155017609",
   "metadata": {},
   "source": [
    "處理健祐哥切好的 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d074c887-c1c6-42fb-97d2-d777ab9173ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "import llama\n",
    "from llama import Tokenizer\n",
    "import torchaudio\n",
    "from torchvision import transforms\n",
    "from pytorchvideo.data.clip_sampling import ConstantClipsPerVideoSampler\n",
    "import copy\n",
    "import logging\n",
    "import whisper\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "data_path = Path(\"/work/u8915687/big-superb/big-superb-train-data\")\n",
    "tokenizer = Tokenizer(model_path=\"/home/u8915687/lab/big-superb/Macaw-LLM2/weights/llama_7B/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ed0290f-9bfd-458d-b08e-e9ba80682e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import llama.utils\n",
    "from llama import Tokenizer\n",
    "import copy\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import random\n",
    "from pytorchvideo.data.clip_sampling import ConstantClipsPerVideoSampler\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "try:\n",
    "    from torchvision.transforms import InterpolationMode\n",
    "    BICUBIC = InterpolationMode.BICUBIC\n",
    "except ImportError:\n",
    "    BICUBIC = Image.BICUBIC\n",
    "\n",
    "class BigSuperbDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, data_path2=None, max_length=128, used_data_split=\"train\", audio_input_type=\"imagebind\"):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.datas = []\n",
    "        self.used_datasets = []\n",
    "        self.used_data_split = used_data_split\n",
    "        self.audio_input_type = audio_input_type\n",
    "\n",
    "        for task_path in data_path.iterdir():\n",
    "            if self._filter_dataset(task_path):\n",
    "                continue\n",
    "            \n",
    "            self.used_datasets.append(task_path.stem)\n",
    "            \n",
    "            for data_split in task_path.iterdir():\n",
    "                if data_split.stem != self.used_data_split:\n",
    "                    continue\n",
    "                \n",
    "                json_data = json.load((data_split/\"metadata.json\").open())\n",
    "                for d in json_data.values():\n",
    "                    d[\"file\"] = str(data_split/d[\"file\"])\n",
    "                    if d.get(\"file2\"):\n",
    "                        if \".\" not in d[\"file2\"]:\n",
    "                            d[\"file2\"] = d[\"file2\"] + \".wav\"\n",
    "                            \n",
    "                        if (data_split/d[\"file2\"]).exists():\n",
    "                            d[\"file2\"] = str(data_split/d[\"file2\"])\n",
    "                        elif (task_path/\"missing_files\"/d[\"file2\"]).exists():\n",
    "                            d[\"file2\"] = str(task_path/\"missing_files\"/d[\"file2\"])\n",
    "                        else:\n",
    "                            assert False, d[\"file2\"]\n",
    "                            \n",
    "                    self.datas.append(d)\n",
    "        # exclude\n",
    "        if data_path2 is not None:\n",
    "            for task_path in data_path2.iterdir():\n",
    "                if self._filter_dataset(task_path):\n",
    "                    continue\n",
    "                \n",
    "                self.used_datasets.append(task_path.stem)\n",
    "                for data_split in task_path.iterdir():\n",
    "                    if data_split.stem != self.used_data_split:\n",
    "                        continue\n",
    "                    \n",
    "                    json_data = json.load((data_split/\"metadata.json\").open())\n",
    "                    for file_name, d in json_data.items():\n",
    "                        d[\"file\"] = str(data_split/file_name)\n",
    "                                \n",
    "                        self.datas.append(d)\n",
    "\n",
    "\n",
    "        # Audio loader\n",
    "        self.clip_sampler = ConstantClipsPerVideoSampler(\n",
    "            clip_duration=2, clips_per_video=3\n",
    "        )\n",
    "        print(\"Used datasets\", len(self.used_datasets), self.used_datasets)\n",
    "        print(len(self.datas))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.datas[idx]\n",
    "        new_data = {}\n",
    "\n",
    "        instruction = data[\"instruction\"].lower()\n",
    "        text = data[\"text\"].lower() if data.get(\"text\") else None\n",
    "        input1 = llama.format_prompt(instruction, text)\n",
    "        input2 = input1 + data[\"label\"]\n",
    "        \n",
    "        input1 = torch.tensor(\n",
    "            self.tokenizer.encode(input1, bos=True, eos=False), dtype=torch.long\n",
    "        )\n",
    "        input2 = torch.tensor(self.tokenizer.encode(input2, bos=True, eos=True), dtype=torch.long)\n",
    "        padding = self.max_length - input2.size(0)\n",
    "        if padding > 0:\n",
    "            input2 = torch.cat((input2, torch.zeros(padding, dtype=torch.long) - 1))\n",
    "        else:\n",
    "            input2 = input2[:self.max_length]\n",
    "        \n",
    "        labels = copy.deepcopy(input2)\n",
    "        labels[:input1.size(0)] = -1\n",
    "        \n",
    "        input2_mask = input2.ge(0)\n",
    "        label_mask = labels.ge(0)\n",
    "        input2[~input2_mask] = 0\n",
    "        labels[~label_mask] = 0\n",
    "        \n",
    "        input2_mask = input2_mask.float()\n",
    "        label_mask = label_mask.float()\n",
    "        \n",
    "        new_data[\"instruction\"] = data[\"instruction\"]\n",
    "        new_data[\"input_ids\"] = input2\n",
    "        new_data[\"labels\"] = labels\n",
    "        new_data[\"input_mask\"] = input2_mask\n",
    "\n",
    "        if self.audio_input_type == \"imagebind\":\n",
    "            if data.get(\"file2\"):\n",
    "                audio = self._load_and_transform_audio([data[\"file\"], data[\"file2\"]])\n",
    "            else:\n",
    "                audio = self._load_and_transform_audio([data[\"file\"]])\n",
    "                \n",
    "            new_data[\"audio\"] = audio\n",
    "        elif self.audio_input_type == \"whisper\":\n",
    "            if data.get(\"file2\"):\n",
    "                audio = self._load_whisper_audio([data[\"file\"], data[\"file2\"]])\n",
    "            else:\n",
    "                audio = self._load_whisper_audio([data[\"file\"]])\n",
    "        \n",
    "        return new_data\n",
    "    \n",
    "    def _load_whisper_audio(self, audio_paths):\n",
    "        wavforms = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform = torch.tensor(whisper.load_audio(audio_path))\n",
    "            if waveform.size(0) == 0:\n",
    "                waveform = torch.zeros([16000*3])\n",
    "                print(audio_path)\n",
    "            \n",
    "            wavforms.append(\n",
    "                waveform\n",
    "            )\n",
    "        audio = torch.cat(wavforms, dim=0)\n",
    "        audio = whisper.pad_or_trim(audio, 16000*5)\n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "        \n",
    "        return mel\n",
    "\n",
    "    \n",
    "    def _load_and_transform_audio(self, \n",
    "            audio_paths,\n",
    "            num_mel_bins=128,\n",
    "            target_length=204,\n",
    "            sample_rate=16000,\n",
    "            clip_duration=2,\n",
    "            clips_per_video=3,\n",
    "            mean=-4.268,\n",
    "            std=9.138\n",
    "        ):\n",
    "\n",
    "        waveforms = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "            if waveform.size(1) == 0:\n",
    "                waveform = torch.zeros([1, 16000*3])\n",
    "                sr = 16000\n",
    "                # logging.warning(f\"An audio is set to zero, {audio_path}\")\n",
    "                print(audio_path)\n",
    "                \n",
    "            if sample_rate != sr:\n",
    "                waveform = torchaudio.functional.resample(\n",
    "                    waveform, orig_freq=sr, new_freq=sample_rate\n",
    "                )\n",
    "\n",
    "            waveforms.append(waveform)\n",
    "        waveform = torch.cat(waveforms, dim=1)\n",
    "            \n",
    "        all_clips_timepoints = self._get_clip_timepoints(\n",
    "            self.clip_sampler, waveform.size(1) / sample_rate\n",
    "        )\n",
    "        all_clips = []\n",
    "        for clip_timepoints in all_clips_timepoints:\n",
    "            waveform_clip = waveform[\n",
    "                :,\n",
    "                int(clip_timepoints[0] * sample_rate) : int(\n",
    "                    clip_timepoints[1] * sample_rate\n",
    "                ),\n",
    "            ]\n",
    "            waveform_melspec = self._def_waveform2melspec(\n",
    "                waveform_clip, sample_rate, num_mel_bins, target_length\n",
    "            )\n",
    "            all_clips.append(waveform_melspec)\n",
    "\n",
    "        normalize = transforms.Normalize(mean=mean, std=std)\n",
    "        all_clips = [normalize(ac) for ac in all_clips]\n",
    "\n",
    "        all_clips = torch.stack(all_clips, dim=0)\n",
    "\n",
    "        return all_clips\n",
    "        \n",
    "    def _get_clip_timepoints(self, clip_sampler, duration):\n",
    "        # Read out all clips in this video\n",
    "        all_clips_timepoints = []\n",
    "        is_last_clip = False\n",
    "        end = 0.0\n",
    "        while not is_last_clip:\n",
    "            start, end, _, _, is_last_clip = clip_sampler(end, duration, annotation=None)\n",
    "            all_clips_timepoints.append((start, end))\n",
    "        return all_clips_timepoints\n",
    "\n",
    "    def _def_waveform2melspec(self, waveform, sample_rate, num_mel_bins, target_length):\n",
    "        # Based on https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/dataloader.py#L102\n",
    "        waveform -= waveform.mean()\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(\n",
    "            waveform,\n",
    "            htk_compat=True,\n",
    "            sample_frequency=sample_rate,\n",
    "            use_energy=False,\n",
    "            window_type=\"hanning\",\n",
    "            num_mel_bins=num_mel_bins,\n",
    "            dither=0.0,\n",
    "            frame_length=25,\n",
    "            frame_shift=10,\n",
    "        )\n",
    "        # Convert to [mel_bins, num_frames] shape\n",
    "        fbank = fbank.transpose(0, 1)\n",
    "        # Pad to target_length\n",
    "        n_frames = fbank.size(1)\n",
    "        p = target_length - n_frames\n",
    "        # if p is too large (say >20%), flash a warning\n",
    "        # if abs(p) / n_frames > 0.2:\n",
    "            # logging.warning(\n",
    "            #     \"Large gap between audio n_frames(%d) and \"\n",
    "            #     \"target_length (%d). Is the audio_target_length \"\n",
    "            #     \"setting correct?\",\n",
    "            #     n_frames,\n",
    "            #     target_length,\n",
    "            # )\n",
    "        # cut and pad\n",
    "        if p > 0:\n",
    "            fbank = torch.nn.functional.pad(fbank, (0, p), mode=\"constant\", value=0)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[:, 0:target_length]\n",
    "        # Convert to [1, mel_bins, num_frames] shape, essentially like a 1\n",
    "        # channel image\n",
    "        fbank = fbank.unsqueeze(0)\n",
    "        return fbank\n",
    "    \n",
    "    def _filter_dataset(self, task_path):\n",
    "        if task_path.stem.startswith(\"HowFarAreYou\"):\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb5283-1a4a-4e6d-8167-25a6db7f192c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bacf33b-5412-4256-944d-833db57b807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used datasets 22 ['SpoofDetection_Asvspoof2017', 'SpeechTextMatching_LibrispeechTrainClean360', 'DialogueActClassification_DailyTalk', 'DialogueEmotionClassification_DailyTalk', 'SpokenTermDetection_Tedlium2Train', 'NoiseSNRLevelPredictionGaussian_VoxcelebMusan', 'EnhancementDetection_LibrittsTrainClean360Wham', 'SpeakerCounting_LibrittsTrainClean100', 'SpeakerVerification_Aishell1Train', 'SpoofDetection_ASVspoof2015', 'SpeakerVerification_LibrispeechTrainClean100', 'SpeakerVerification_Voxceleb1Train', 'SpokenTermDetection_LibrispeechTrainClean100', 'SpeechDetection_LibrispeechTrainClean100', 'SpeakerVerification_Tedlium2Train', 'NoiseDetectionGaussian_VoxcelebMusan', 'SpeechTextMatching_LibrispeechTrainClean100', 'SpeechDetection_Aishell1Train', 'SpeechTextMatching_Tedlium2Train', 'SpeechDetection_Tedlium2Train', 'ReverberationDetectionSmallRoom_VoxcelebRirsNoises', 'SpeechDetection_Voxceleb1Train']\n",
      "108014\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BigSuperbDataset(data_path, tokenizer, audio_input_type=\"whisper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8fb58-6c90-46df-84c3-bcc93f2820b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                 | 26956/108014 [2:07:15<10:31:25,  2.14it/s]"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(train_dataset))):\n",
    "    train_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f1620-839b-42ea-aeaa-8d11226875dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01fb775d-84b8-4f64-95e6-4f332c944b9e",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b93131a5-2155-47a7-b228-a98d6845e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "from ImageBind.data import my_load_and_transform_audio_data\n",
    "\n",
    "org_data_path = Path(\"/work/u8915687/big-superb/train_datasets\")\n",
    "all_datasets = ['BigSuperbPrivate/SpeechDetection_LibrispeechTrainClean100']\n",
    "\n",
    "tokenizer = Tokenizer(model_path=\"/home/u8915687/lab/big-superb/Macaw-LLM2/weights/llama_7B/tokenizer.model\")\n",
    "def prepare_dataset(b):\n",
    "    max_length = 128\n",
    "    \n",
    "    batch = {}\n",
    "    instruction = b[\"instruction\"].lower()\n",
    "    text = b[\"text\"].lower() if b.get(\"text\") else None\n",
    "    input1 = llama.format_prompt(instruction, text)\n",
    "    input2 = input1 + b[\"label\"]\n",
    "    \n",
    "    input1 = torch.tensor(\n",
    "        tokenizer.encode(input1, bos=True, eos=False), dtype=torch.long\n",
    "    )\n",
    "    input2 = torch.tensor(tokenizer.encode(input2, bos=True, eos=True), dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    padding = max_length - input2.size(0)\n",
    "    if padding > 0:\n",
    "        input2 = torch.cat((input2, torch.zeros(padding, dtype=torch.long) - 1))\n",
    "    else:\n",
    "        input2 = input2[:max_length]\n",
    "    \n",
    "    labels = copy.deepcopy(input2)\n",
    "    labels[:input1.size(0)] = -1\n",
    "    \n",
    "    input2_mask = input2.ge(0)\n",
    "    label_mask = labels.ge(0)\n",
    "    input2[~input2_mask] = 0\n",
    "    labels[~label_mask] = 0\n",
    "    \n",
    "    input2_mask = input2_mask.float()\n",
    "    label_mask = label_mask.float()\n",
    "    \n",
    "    batch[\"instruction\"] = b[\"instruction\"]\n",
    "    batch[\"input_ids\"] = input2\n",
    "    batch[\"labels\"] = labels\n",
    "    batch[\"input_mask\"] = input2_mask\n",
    "    batch[\"audio\"] = my_load_and_transform_audio_data(\n",
    "        torch.tensor(b[\"audio\"][\"array\"]).unsqueeze(0)\n",
    "    )[0]\n",
    "    return batch\n",
    "\n",
    "for dataset_name in all_datasets:\n",
    "    ori_dataset =  load_from_disk(org_data_path/dataset_name)    \n",
    "\n",
    "    td = ori_dataset[\"train\"].shuffle(42)[:50]\n",
    "    td = Dataset.from_dict(td)\n",
    "\n",
    "    # td = td.map(prepare_dataset)\n",
    "    # td = td.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b106ebf-4fd5-421b-83a2-5b59ecb47add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_name': 'SpeechDetection_LibrispeechTrainClean100_0',\n",
       " 'name': '289-121652-0038.flac',\n",
       " 'instruction': 'Does the recording contain speech from human? The answer could be yes or no.',\n",
       " 'input_ids': tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
       "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 13221,\n",
       "           278, 16867,  1712, 12032,   515,  5199, 29973,   278,  1234,  1033,\n",
       "           367,  4874,   470,   694, 29889,    13,    13,  2277, 29937, 13291,\n",
       "         29901,  3582,     2,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'labels': tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0, 3582,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'input_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0.]),\n",
       " 'audio': tensor([[[[-1.0245, -0.8402, -0.7290,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-1.2776, -1.2310, -1.1678,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.9941, -0.8542, -0.7909,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           ...,\n",
       "           [-0.3938, -0.4086, -0.5283,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.4391, -0.4342, -0.5335,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.4806, -0.5034, -0.5158,  ...,  0.4671,  0.4671,  0.4671]]],\n",
       " \n",
       " \n",
       "         [[[-0.7067, -0.6565, -0.6842,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-1.1143, -1.1518, -0.8906,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.7375, -0.7750, -0.5138,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           ...,\n",
       "           [-0.3882, -0.3532, -0.3948,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.3247, -0.3022, -0.3114,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.2978, -0.2267, -0.2737,  ...,  0.4671,  0.4671,  0.4671]]],\n",
       " \n",
       " \n",
       "         [[[-0.2674, -0.2020, -0.1675,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.7535, -0.5812, -0.6304,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.3766, -0.2044, -0.2536,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           ...,\n",
       "           [-0.3294, -0.3546, -0.3744,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.2043, -0.3943, -0.3750,  ...,  0.4671,  0.4671,  0.4671],\n",
       "           [-0.3051, -0.3877, -0.4372,  ...,  0.4671,  0.4671,  0.4671]]]])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[65000-1986]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbe32a2-f13e-4073-97c3-04e20fb8fc11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "95f11645-3e81-4a50-9923-67a1e25481a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(b):\n",
    "    batch = {}\n",
    "    \n",
    "    audios = []\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    for data in b:\n",
    "        audio = my_load_and_transform_audio_data(\n",
    "                        torch.tensor(data[\"audio\"][\"array\"], dtype=torch.float32\n",
    "                    ).unsqueeze(0))[0]\n",
    "        audios.append(audio)\n",
    "        text = data.get(\"text\").lower() if data.get(\"text\") else None\n",
    "        instruction = data[\"instruction\"].lower()\n",
    "        \n",
    "        prompts.append(llama.format_prompt(instruction, text))\n",
    "        labels.append(data[\"label\"])\n",
    "    \n",
    "    batch[\"audio\"] = torch.stack(audios)\n",
    "    batch[\"prompts\"] = prompts\n",
    "    batch[\"instructions\"] = [d[\"instruction\"] for d in b]\n",
    "    batch[\"labels\"] = [d[\"label\"] for d in b]\n",
    "    return batch\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        td,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d8c65fd3-73f5-4dbd-bc67-bd5bd550eb65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438ba08-d7a2-41db-aeca-390cb9edd419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da795614-6db1-4d33-8744-6b3dae3704d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagebind_LLM",
   "language": "python",
   "name": "imagebind_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
